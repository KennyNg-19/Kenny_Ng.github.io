<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>池中之物</title>
  
  <subtitle>By Kenny_Ng</subtitle>
  <link href="/Kenny_Ng.github.io/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-06-02T15:30:01.125Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Kenny Ng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>datawhale-cv05模型集成</title>
    <link href="http://yoursite.com/2020/06/02/datawhale-cv05%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/"/>
    <id>http://yoursite.com/2020/06/02/datawhale-cv05%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/</id>
    <published>2020-06-02T14:35:07.000Z</published>
    <updated>2020-06-02T15:30:01.125Z</updated>
    
    <content type="html"><![CDATA[<p>本章是本次赛题学习的最后一章，将会讲解如何使用集成学习提高预测精度</p><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><p>本章讲解的知识点包括</p><ul><li>学习集成学习<strong>方法</strong>，以及交叉验证情况下的模型集成</li><li>学会使用<strong>深度学习</strong>模型的集成学习和结果后处理思路。</li></ul><h2 id="5-模型集成"><a href="#5-模型集成" class="headerlink" title="5 模型集成"></a>5 模型集成</h2><h3 id="5-1-集成学习方法"><a href="#5-1-集成学习方法" class="headerlink" title="5.1 集成学习方法"></a>5.1 集成学习方法</h3><p>在机器学习中的集成学习可以在一定程度上提高预测精度，常见的集成学习方法有Stacking、Bagging和Boosting，同时这些集成学习方法与具体验证集划分联系紧密。</p><p>由于深度学习模型一般需要较长的训练周期，如果硬件设备不允许建议选取留出法，如果需要追求精度可以使用交叉验证的方法。</p><p>下面假设构建了10折交叉验证，训练得到10个CNN模型。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task05/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.png" alt="集成学习"></p><p>那么在10个CNN模型可以使用如下方式进行集成：</p><ol><li><p>对预测的结果的<u>概率值</u><strong>进行平均</strong>，然后解码为具体字符；</p></li><li><p>or 对预测的字符进行<strong>投票</strong>，得到最终字符</p></li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>训练了<strong>10个CNN</strong>，<strong>训练时间</strong>较长</p><h3 id="5-2-深度学习中的集成学习"><a href="#5-2-深度学习中的集成学习" class="headerlink" title="5.2 深度学习中的集成学习"></a>5.2 深度学习中的集成学习</h3><p>此外在深度学习中本身还有一些集成学习思路的做法，值得借鉴学习：</p><h4 id="5-2-1-Dropout"><a href="#5-2-1-Dropout" class="headerlink" title="5.2.1 Dropout"></a>5.2.1 Dropout</h4><p>Dropout可以作为<font color="#dd0000"><strong>训练</strong></font>深度神经网络的一种技巧。在每个<font color="#dd0000"><strong>训练</strong>批次</font>中，通过<font color="#dd0000"><strong>随机让一部分(按照输入的比例)的节点</strong>停止工作</font>，但同时在<font color="#dd0000"><strong>预测</strong>的过程中</font>让<font color="#dd0000"><strong>所有的</strong>节点</font>都其作用。</p><h5 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h5><p>经常出现在在<strong>先有的CNN网络</strong>中，可以有效的<strong>缓解模型过拟合</strong>的情况，也可以在预测时<strong>增加模型的精度</strong></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><p>加入Dropout后的代码如下: 即nn<strong>.Dropout(0.25)</strong>这一行，(按照输入的比例为0.25)</p><pre class=" language-Python"><code class="language-Python"># 定义模型class SVHN_Model1(nn.Module):    def __init__(self):        super(SVHN_Model1, self).__init__()        # CNN提取特征模块        self.cnn = nn.Sequential(            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),            nn.Dropout(0.25), //            nn.MaxPool2d(2),            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),             nn.Dropout(0.25), //            nn.MaxPool2d(2),        )        #         self.fc1 = nn.Linear(32*3*7, 11)        self.fc2 = nn.Linear(32*3*7, 11)        self.fc3 = nn.Linear(32*3*7, 11)        self.fc4 = nn.Linear(32*3*7, 11)        self.fc5 = nn.Linear(32*3*7, 11)        self.fc6 = nn.Linear(32*3*7, 11)    def forward(self, img):          //...</code></pre><h4 id="5-2-2-Snapshot"><a href="#5-2-2-Snapshot" class="headerlink" title="5.2.2 Snapshot"></a>5.2.2 Snapshot</h4><p>本章的开头5.2已经提到训练了<strong>10个CNN</strong>则可以将多个模型的预测结果<strong>进行平均</strong>。但是加入只训练了<strong><u>一个</u></strong>CNN模型，如何做模型集成呢？</p><p>在论文Snapshot Ensembles中，作者提出使用cyclical learning rate进行训练模型，并<font color="#dd0000">保存<strong>精度比较好的</strong>一些checkopint，最后<strong>将多个checkpoint</strong>进行<strong>模型集成</strong></font>。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task05/Snapshot.png" alt="Snapshot"></p><h5 id="优-缺点"><a href="#优-缺点" class="headerlink" title="优/缺点"></a>优/缺点</h5><p>在Snapshot论文中作者通过使用表明，此种方法</p><ul><li><p>优点：可以在一定程度上<strong>提高模型精度</strong>，</p></li><li><p>缺点：但<strong>需要更长的训练时间</strong>；由于在cyclical learning rate中<strong>学习率</strong>有周期性变大和减少的行为，因此CNN模型很有可能在<strong>跳出局部最优进入<u>另一个局部最优</u></strong>。</p></li></ul><h4 id="5-2-3-TTA测试集数据扩增"><a href="#5-2-3-TTA测试集数据扩增" class="headerlink" title="5.2.3 TTA测试集数据扩增"></a>5.2.3 TTA<font color="#dd0000"><strong><u>测试集</u></strong></font>数据扩增</h4><p><font color="#dd0000"><strong><u>测试集</u></strong></font>数据扩增（Test Time Augmentation，简称TTA）也是常用的集成学习技巧。</p><p>数据扩增<font color="#dd0000">不仅可以在训练时候用，而且可以同样在预测时候进行数据扩增</font>，对同一个样本预测三次，然后对三次结果进行平均。</p><table><thead><tr><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/23.png" alt="IMG"></td><td><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/23_1.png" alt="IMG"></td><td><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/23_2.png" alt="IMG"></td></tr></tbody></table><h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>test_loader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> tta<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>   test_pred_tta <span class="token operator">=</span> None   <span class="token comment" spellcheck="true"># TTA 次数(对测试集做数据扩增)</span>   <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>tta<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">//</span>       test_pred <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>       <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>           <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>input<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>               c0<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> c5 <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>               output <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>c0<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c1<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                  c2<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c3<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                  c4<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c5<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>               test_pred<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>       test_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>test_pred<span class="token punctuation">)</span>       <span class="token keyword">if</span> test_pred_tta <span class="token keyword">is</span> None<span class="token punctuation">:</span>           test_pred_tta <span class="token operator">=</span> test_pred       <span class="token keyword">else</span><span class="token punctuation">:</span>           test_pred_tta <span class="token operator">+=</span> test_pred   <span class="token keyword">return</span> test_pred_tta</code></pre><h2 id="6-结果后的额外处理"><a href="#6-结果后的额外处理" class="headerlink" title="6. 结果后的额外处理"></a>6. 结果后的额外处理</h2><p>在不同的任务中可能会有不同的解决方案，不同思路的模型<strong>不仅可以互相借鉴</strong>，同时也可以<strong>修正</strong>最终的预测结果。</p><p>在本次赛题中，可以从以下几个思路对预测结果进行后处理：</p><ul><li>统计图片中每个位置字符出现的<strong>频率</strong>，使用<strong><u>规则</u>修正结果</strong>；</li><li>单独训练一个<strong>字符长度预测模型</strong>，用来预测图片中<strong>字符个数，并修正结果</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本章是本次赛题学习的最后一章，将会讲解如何使用集成学习提高预测精度&lt;/p&gt;
&lt;h2 id=&quot;学习目标&quot;&gt;&lt;a href=&quot;#学习目标&quot; class=&quot;headerlink&quot; title=&quot;学习目标&quot;&gt;&lt;/a&gt;学习目标&lt;/h2&gt;&lt;p&gt;本章讲解的知识点包括&lt;/p&gt;
&lt;ul&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-04模型训练与验证</title>
    <link href="http://yoursite.com/2020/05/30/datawhale-cv04%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2020/05/30/datawhale-cv04%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/</id>
    <published>2020-05-30T10:56:39.000Z</published>
    <updated>2020-05-30T15:28:12.625Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型训练验证与调优"><a href="#模型训练验证与调优" class="headerlink" title="模型训练验证与调优"></a>模型训练验证与调优</h1><p>在上一章节我们构建了一个简单的CNN进行训练，但这些还远远不够。一个成熟合格的深度学习训练流程至少具备以下功能：</p><ul><li>在训练集上进行训练，并<strong>在验证集上进行验证；</strong></li><li>(模型可以保存最优的权重，并读取权重)</li><li>记录下训练集和验证集的精度，便于调参。</li></ul><h2 id="4-模型训练与验证"><a href="#4-模型训练与验证" class="headerlink" title="4 模型训练与验证"></a>4 模型训练与验证</h2><p>为此本章将从构建验证集、模型训练和验证、模型保存与加载和模型调参几个部分讲解，在部分小节中将会结合Pytorch代码进行讲解。</p><h2 id="4-1-学习目标"><a href="#4-1-学习目标" class="headerlink" title="4.1 学习目标"></a>4.1 学习目标</h2><ul><li>理解验证集的作用，并使用训练集和验证集完成训练</li><li>学会使用Pytorch环境下的模型读取和加载，并了解调参流程</li></ul><h2 id="4-2-why构造验证集"><a href="#4-2-why构造验证集" class="headerlink" title="4.2 why构造验证集"></a>4.2 why构造验证集</h2><p>在机器学习模型（特别是深度学习模型）的训练过程中，模型是<strong>非常容易过拟合的</strong>——模型在不断的训练过程中训练<strong>误差会逐渐降低</strong>，但<font color="#dd0000"><strong>测试误差的走势</strong></font>则<strong>不一定</strong>。</p><p>在训练过程中，模型只能利用训练数据来进行训练，<strong>并不能接触到测试集上的样本</strong>。因此模型如果将训练集学的<font color="#dd0000">过好，模型就会<strong>记住</strong><u>训练样本</u>的<strong>细节</strong>，导致模型<strong>在测试集的泛化效果较差</strong></font>，这种现象称为过拟合（Overfitting）。</p><p>过拟合的大致特征，如图所示：随着模型复杂度和模型训练轮数的增加，CNN模型在训练集上的误差会降低，但在测试集上的误差会逐渐降低，然后逐渐升高。</p><p>[<img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task04/loss.png" alt="IMG">](<a href="https://github.com/datawhalechina/team-learning/blob/master/03" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/03</a> 计算机视觉/计算机视觉实践（街景字符编码识别）/IMG/Task04/loss.png)</p><h3 id="导致过拟合的最常见原因"><a href="#导致过拟合的最常见原因" class="headerlink" title="导致过拟合的最常见原因"></a>导致过拟合的最常见原因</h3><ul><li>最为常见的情况是<strong>模型复杂度（Model Complexity ）太高</strong>，导致模型学习了数据的方方面面——包括<strong>过于细枝末节</strong>的规律。</li></ul><p>解决上述问题最好的解决方法：构建一个与测试集<font color="#dd0000"><strong>尽可能分布一致</strong></font>的验证集，在训练过程中不断验证模型在验证集上的精度，并以此控制模型的训练。</p><h3 id="何为数据的分布？"><a href="#何为数据的分布？" class="headerlink" title="何为数据的分布？"></a>何为数据的分布？</h3><p>这里讨论的<strong>数据的分布</strong>一般指的是<font color="#dd0000"><strong>与标签相关的统计分布</strong></font>：比如在分类任务中“分布”指的是标签的<strong>类别分布</strong>，训练集-验证集-测试集的<strong>类别分布情况应该大体一致</strong>；如果标签是带有时序信息，则验证集和测试集的时间间隔应该保持一致。</p><h3 id="验证集主要的2个作用：验证精度和调整超参数；"><a href="#验证集主要的2个作用：验证精度和调整超参数；" class="headerlink" title="验证集主要的2个作用：验证精度和调整超参数；"></a>验证集主要的2个作用：<font color="#dd0000">验证</font>精度和调整<font color="#dd0000">超参数</font>；</h3><p>在给定赛题后，赛题方会<u>给定训练集和测试集<strong>两部分</strong></u>数据。参赛者需要在训练集上面构建模型，并在测试集上面验证模型的泛化能力。</p><p>在不提供验证集时，参赛选手可以自己<strong>在本地划分出一个验证集出来</strong>，进行本地验证。训练集、验证集和测试集分别有不同的作用：</p><ul><li><h4 id="训练集（Train-Set）：模型用于训练和调整模型参数；"><a href="#训练集（Train-Set）：模型用于训练和调整模型参数；" class="headerlink" title="训练集（Train Set）：模型用于训练和调整模型参数；"></a>训练集（Train Set）：模型用于训练和调整模型参数；</h4></li><li><h4 id="验证集（Validation-Set）：用来①验证模型训练后的精度和②调整模型超参数；"><a href="#验证集（Validation-Set）：用来①验证模型训练后的精度和②调整模型超参数；" class="headerlink" title="验证集（Validation Set）：用来①验证模型训练后的精度和②调整模型超参数；"></a>验证集（Validation Set）：用来①<font color="#dd0000">验证</font>模型<u>训练后</u>的精度和②调整模型<font color="#dd0000">超参数</font>；</h4></li><li><h4 id="测试集（Test-Set）：验证模型的泛化能力。"><a href="#测试集（Test-Set）：验证模型的泛化能力。" class="headerlink" title="测试集（Test Set）：验证模型的泛化能力。"></a>测试集（Test Set）：验证模型的泛化能力。</h4></li></ul><p>因为训练集和验证集是分开的，所以模型在验证集上面的精度<strong>在一定程度上可以反映模型的泛化能力</strong>（前提就是，和测试集分布的一致性）——所以划分时，需要<strong>注意验证集的分布应该与测试集尽量保持一致</strong>。</p><h2 id="4-3-如何划分-生成验证集"><a href="#4-3-如何划分-生成验证集" class="headerlink" title="4.3 如何划分 生成验证集"></a>4.3 如何划分 生成验证集</h2><p>既然验证集这么重要，那么如何划分本地验证集呢：<font color="#dd0000"><strong>从训练集中拆分</strong></font>一部分得到验证集。</p><p>具体的有如下几种方式：</p><h4 id="1-留出法（Hold-Out）——最简单直接的"><a href="#1-留出法（Hold-Out）——最简单直接的" class="headerlink" title="1. 留出法（Hold-Out）——最简单直接的"></a>1. 留出法（Hold-Out）——最简单直接的</h4><p>直接将训练集划分成两部分，新的训练集和验证集。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfapeqhp0gj30r00ee0t0.jpg" alt=""></p><ul><li><p>这种划分方式的优点是最为直接简单</p></li><li><p>缺点是只得到了一份验证集，有可能导致模型在验证集上过拟合。</p></li><li><p>场景: <strong>数据量比较大</strong>的情况—随机划分，划分得均匀程度的概率更大</p></li></ul><h4 id="2-交叉验证法（Cross-Validation，CV）"><a href="#2-交叉验证法（Cross-Validation，CV）" class="headerlink" title="2. 交叉验证法（Cross Validation，CV）"></a>2. 交叉验证法（Cross Validation，CV）</h4><p>将训练集划分成K份，将其中的K-1份作为训练集，剩余的1份作为验证集，循环K训练。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfapfbvkqkj30qg0huaaj.jpg" alt=""></p><p>这种划分方式是<strong>所有的训练集<font color="#dd0000">都做过一次验证集</font></strong>，最终模型验证精度是K份平均得到。</p><ul><li>优点: 验证集<font color="#dd0000"><strong>精度比较可靠（所有训练数据都做过验证集）</strong></font>，训练K次可以得到K个有多样性差异的模型；</li><li>缺点: 需要训练K次，加大了训练的量（epoch）</li><li>场景：不适合<strong>数据量很大</strong>的情况——那样划分的频率过高</li></ul><h4 id="3-自助采样法（BootStrap）-略"><a href="#3-自助采样法（BootStrap）-略" class="headerlink" title="3. 自助采样法（BootStrap）(略)"></a>3. 自助采样法（BootStrap）(略)</h4><p>通过<strong>有放回</strong>的<strong>采样方式</strong>得到新的训练集和验证集，每次的训练集和验证集都是有区别的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfapje5ph0j30q40ew74p.jpg" alt=""></p><ul><li>场景：数据量较小</li></ul><h4 id="实际使用时的选择"><a href="#实际使用时的选择" class="headerlink" title="实际使用时的选择"></a>实际使用时的选择</h4><p>这些划分方法是<strong>从数据划分方式的角度</strong>来讲的，在现有的数据比赛中<strong>一般采用</strong>的划分方法是<strong><font color="#dd0000">留出法和交叉验证法</font></strong></p><h2 id="5-模型调优流程"><a href="#5-模型调优流程" class="headerlink" title="5. 模型调优流程"></a>5. 模型调优流程</h2><p>深度学习原理少但实践性非常强，基本上很多的模型的验证只能通过训练来完成。同时深度学习有<strong>众多的网络结构和超参数</strong>，因此需要反复尝试需要较多的训练时间。而如何有效的训练深度学习模型逐渐成为了一门学问。</p><h3 id="5-1-训练技巧"><a href="#5-1-训练技巧" class="headerlink" title="5.1 训练技巧"></a>5.1 训练技巧</h3><p>深度学习有众多的训练技巧，本节挑选了常见的一些技巧来讲解，并针对本次赛题进行具体分析。</p><p>与传统的机器学习模型不同，深度学习模型的精度与模型的<strong>复杂度、数据量、正则化、数据扩增等因素</strong>直接相关。</p><p>所以当深度学习模型处于不同的阶段（欠拟合、过拟合和完美拟合）的情况下，大家可以知道可以什么角度来继续优化模型。</p><p>在参加本次比赛的过程中，我建议大家以如下逻辑完成：</p><ul><li>1.初步构建简单的CNN模型，不用特别复杂，跑通训练、验证和预测的流程；</li><li>2.此时简单CNN模型的损失会比较大，尝试<strong>增加模型复杂度</strong>，并观察验证集精度；</li><li>3.在增加模型复杂度的<strong>同时增加数据扩增方法</strong>，<font color="#dd0000">直至<strong>验证集精度不变</strong></font>。</li></ul><p>对应如下图</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task04/%E8%B0%83%E5%8F%82%E6%B5%81%E7%A8%8B.png" alt="IMG"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;模型训练验证与调优&quot;&gt;&lt;a href=&quot;#模型训练验证与调优&quot; class=&quot;headerlink&quot; title=&quot;模型训练验证与调优&quot;&gt;&lt;/a&gt;模型训练验证与调优&lt;/h1&gt;&lt;p&gt;在上一章节我们构建了一个简单的CNN进行训练，但这些还远远不够。一个成熟合格的深度学
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Hexo指令的使用和经历的坑</title>
    <link href="http://yoursite.com/2020/05/26/hexo%E6%8C%87%E4%BB%A4%E4%BD%BF%E7%94%A8%E7%BB%8F%E5%8E%86/"/>
    <id>http://yoursite.com/2020/05/26/hexo%E6%8C%87%E4%BB%A4%E4%BD%BF%E7%94%A8%E7%BB%8F%E5%8E%86/</id>
    <published>2020-05-26T04:52:21.000Z</published>
    <updated>2020-05-25T17:04:51.031Z</updated>
    
    <content type="html"><![CDATA[<p>（为了防止自己忘记，而万一有下一次，所以当日记记下）</p><p><a href="https://hexo.io/zh-cn/docs/commands.html" target="_blank" rel="noopener">官网中文解说</a></p><h2 id="亲历-用Hexo命令解决搭建博客的坑"><a href="#亲历-用Hexo命令解决搭建博客的坑" class="headerlink" title="亲历: 用Hexo命令解决搭建博客的坑"></a>亲历: 用Hexo命令解决搭建博客的<font color="#dd0000">坑</font></h2><h3 id="1-hexo-clean"><a href="#1-hexo-clean" class="headerlink" title="1. hexo clean"></a>1. hexo clean</h3><p>清除缓存文件 (<code>db.json</code>) 和<strong>已生成的静态文件 (<code>public</code>)</strong>——就是hexo d<strong>推送到GitHub并显示到前端的<font color="#dd0000">网页静态源码</font></strong>。</p><p><strong>Case</strong>: 在某些情况（尤其是<strong>编辑过主题后</strong>）， 如果发现您<strong>对站点的<font color="#dd0000">更改无论如何也不生效</font>——因为<code>hexo g</code>并<u>不会</u>将全本地的<font color="#dd0000">生成的静态文件</font>和<font color="#dd0000">现有的<code>public</code></font>路径的文件<font color="#dd0000">一一对比，而是部分对比</font>，来更新后者</strong>——此时，可能需要运行该命令<strong>重新生成网页源码</strong>。</p><h3 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h3><p>to be coninue…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;（为了防止自己忘记，而万一有下一次，所以当日记记下）&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hexo.io/zh-cn/docs/commands.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网中文解说&lt;/a&gt;&lt;/p&gt;
&lt;h2 id
      
    
    </summary>
    
    
    
      <category term="走过的坑" scheme="http://yoursite.com/tags/%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-03字符识别模型</title>
    <link href="http://yoursite.com/2020/05/26/datawhale-cv03%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/05/26/datawhale-cv03%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-05-25T17:24:13.000Z</published>
    <updated>2020-05-26T16:08:30.975Z</updated>
    
    <content type="html"><![CDATA[<p>前面的章节，我们讲解了赛题的背景知识和赛题数据的读取。 本文的任务是: 基于对赛题理解本章将构建一个定长多字符分类模型, 构建一个<strong>CNN类</strong>的定长字符识别模型。</p><h2 id="3-字符识别模型"><a href="#3-字符识别模型" class="headerlink" title="3 字符识别模型"></a>3 字符识别模型</h2><p>本章将会讲解卷积神经网络（Convolutional Neural Network, CNN）的常见层，并从头搭建一个字符识别模型。</p><h3 id="3-1-学习目标"><a href="#3-1-学习目标" class="headerlink" title="3.1 学习目标"></a>3.1 学习目标</h3><ul><li>学习CNN基础和原理</li><li>使用Pytorch框架构建CNN模型，并完成训练</li></ul><h3 id="3-2-CNN介绍"><a href="#3-2-CNN介绍" class="headerlink" title="3.2 CNN介绍"></a>3.2 CNN介绍</h3><p>卷积神经网络（简称CNN）是一类特殊的人工神经网络，是深度学习中重要的一个分支。</p><h4 id="为什么CV方面用CNN？"><a href="#为什么CV方面用CNN？" class="headerlink" title="为什么CV方面用CNN？"></a>为什么CV方面用CNN？</h4><p>CNN在很多领域都表现优异，精度和速度比传统计算学习算法高很多。<strong>特别是在计算机视觉领域</strong>，CNN是解决图像分类、图像检索、物体检测和语义分割的主流模型。</p><h4 id="常见结构组成"><a href="#常见结构组成" class="headerlink" title="常见结构组成"></a>常见结构组成</h4><p>CNN每一层由众多的卷积核组成，每个卷积核对输入的像素进行<strong>卷积操作</strong>，得到下一次的输入。随着网络层的增加卷积核会逐渐扩大感受野，并缩减图像的尺寸。</p><p>CNN是一种<u><strong>层次</strong></u>模型: </p><h5 id="1-Input"><a href="#1-Input" class="headerlink" title="1. Input"></a>1. Input</h5><p>输入的是原始的<strong>像素数据</strong>。</p><h5 id="2-中间处理计算"><a href="#2-中间处理计算" class="headerlink" title="2. 中间处理计算"></a>2. 中间处理计算</h5><p>CNN通过<strong>卷积（convolution）、池化（pooling）、<u>非线性</u>激活函数（non-linear activation function）和全连接层（fully connected layer）</strong>构成。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task03/%E5%8D%B7%E7%A7%AF.png" alt="卷积过程"></p><h5 id="3-Output"><a href="#3-Output" class="headerlink" title="3. Output"></a>3. Output</h5><p>通过多次卷积和池化，CNN的<strong>最后一层将输入的图像像素映射为具体的输出</strong>：如在分类任务中会转换为不同类别的<strong>概率输出</strong>，</p><h5 id="4-学习过程-优化参数-——反向传播"><a href="#4-学习过程-优化参数-——反向传播" class="headerlink" title="4. 学习过程(优化参数)——反向传播"></a>4. 学习过程(优化参数)——反向传播</h5><p>然后计算真实标签与CNN模型的输出的预测结果的<strong>差异</strong>，并通过<strong>反向传播更新每层的参数</strong>；在更新完成<strong>后再次前向传播</strong>，如此反复直到训练完成 。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task03/Le_CNN.png" alt="经典的字符识别模型"></p><h3 id="3-3-模型的优势特点"><a href="#3-3-模型的优势特点" class="headerlink" title="3.3 模型的优势特点"></a>3.3 模型的优势特点</h3><p>传统机器学习相比，CNN, 或者说<font color="#dd0000"><strong>深度学习模型</strong>(各种<strong><u>深度神经网络</u></strong>)</font>，具有一种<strong><font color="#dd0000">端到端（End to End）的优势</font></strong>：模型训练的过程中是<strong><font color="#dd0000">直接</font></strong>从<u>输入</u>图像像素到<u>最终的输出</u>分类结果——<font color="#dd0000">并<strong>不涉及</strong>到具体的<strong>特征提取</strong>和构建模型的过程</font>，也不需要<strong>人工的</strong>参与。</p><h3 id="3-4-实战：Pytorch构建CNN模型"><a href="#3-4-实战：Pytorch构建CNN模型" class="headerlink" title="3.4 实战：Pytorch构建CNN模型"></a>3.4 实战：Pytorch构建CNN模型</h3><p>在Pytorch中构建CNN模型非常简单，<strong>只需要定义</strong>好模型的<strong>参数</strong>和<strong>正向传播函数</strong>即可，<font color="#dd0000">Pytorch会根据正向传播<strong>自动计算反向传播</strong></font>！！</p><h4 id="1-定义模型"><a href="#1-定义模型" class="headerlink" title="1. 定义模型"></a>1. 定义模型</h4><pre class=" language-Python"><code class="language-Python"># 定义模型: 这个CNN模型包括两个卷积层，最后并联6个全连接层进行分类class SVHN_Model1(nn.Module):    # 构造器：1. 只需要定义好模型参数    def __init__(self):        super(SVHN_Model1, self).__init__()        # CNN提取特征模块        self.cnn = nn.Sequential(            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),              nn.MaxPool2d(2),            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),             nn.MaxPool2d(2),        )        #         self.fc1 = nn.Linear(32*3*7, 11)        self.fc2 = nn.Linear(32*3*7, 11)        self.fc3 = nn.Linear(32*3*7, 11)        self.fc4 = nn.Linear(32*3*7, 11)        self.fc5 = nn.Linear(32*3*7, 11)        self.fc6 = nn.Linear(32*3*7, 11)        # 2. 只需要定义好模型正向传播即可，Pytorch会根据正向传播自动计算反向传播。    def forward(self, img):                feat = self.cnn(img)        feat = feat.view(feat.shape[0], -1)        c1 = self.fc1(feat)        c2 = self.fc2(feat)        c3 = self.fc3(feat)        c4 = self.fc4(feat)        c5 = self.fc5(feat)        c6 = self.fc6(feat)        return c1, c2, c3, c4, c5, c6# 构造一个模型对象model = SVHN_Model1()</code></pre><h4 id="2-使用模型"><a href="#2-使用模型" class="headerlink" title="2. 使用模型"></a>2. 使用模型</h4><p>在训练之前，需要定义好</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 损失函数</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 优化器</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.005</span><span class="token punctuation">)</span></code></pre><p>然后就是训练： 过程包括，Pytorch<strong>自动计算</strong>反向传播，让模型真正的实现<font color="#dd0000">“<strong>自我学习</strong>”</font></p><pre class=" language-python"><code class="language-python">loss_plot<span class="token punctuation">,</span> c0_plot <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 迭代10个Epoch</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>        c0<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> c5 <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>c0<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c1<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c2<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c3<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c4<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c5<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        loss <span class="token operator">/=</span> <span class="token number">6</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss_plot<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        c0_plot<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>c0<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1.0</span> <span class="token operator">/</span> c0<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span></code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面的章节，我们讲解了赛题的背景知识和赛题数据的读取。 本文的任务是: 基于对赛题理解本章将构建一个定长多字符分类模型, 构建一个&lt;strong&gt;CNN类&lt;/strong&gt;的定长字符识别模型。&lt;/p&gt;
&lt;h2 id=&quot;3-字符识别模型&quot;&gt;&lt;a href=&quot;#3-字符识别模型
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-02数据读取与数据扩增</title>
    <link href="http://yoursite.com/2020/05/23/datawhale-cv02/"/>
    <id>http://yoursite.com/2020/05/23/datawhale-cv02/</id>
    <published>2020-05-23T04:39:17.000Z</published>
    <updated>2020-05-23T13:52:38.634Z</updated>
    
    <content type="html"><![CDATA[<p>在上一章节，官方提供了三种不同的解决方案。从本章开始 将逐渐的学习使用<strong>【定长字符识别】思路</strong>来构建模型，讲解赛题的解决方案和相应知识点。</p><h2 id="2-数据读取与数据扩增"><a href="#2-数据读取与数据扩增" class="headerlink" title="2 数据读取与数据扩增"></a>2 数据读取与数据扩增</h2><h3 id="2-1-学习目标"><a href="#2-1-学习目标" class="headerlink" title="2.1 学习目标"></a>2.1 学习目标</h3><p>本章主要内容为<strong>图像数据读取、数据扩增方法</strong>和<strong>实战Pytorch读取赛题数据</strong>三个部分组成。</p><ul><li>学习Python和Pytorch中图像读取</li><li>学会扩增方法和实战Pytorch读取赛题数据</li></ul><h3 id="2-2-图像读取"><a href="#2-2-图像读取" class="headerlink" title="2.2 图像读取"></a>2.2 图像读取</h3><p>在识别之前，首先需要完成<strong>对数据的读取操作</strong>。在Python中有很多库可以完成数据读取的操作，比较常见的有<strong>Pillow和OpenCV</strong>。</p><h4 id="2-2-1-Pillow"><a href="#2-2-1-Pillow" class="headerlink" title="2.2.1 Pillow"></a>2.2.1 Pillow</h4><p>Pillow是Python图像<strong>处理函式库(PIL）</strong>的一个分支。Pillow提供了常见的图像读取和处理的操作。</p><h4 id="2-2-2-OpenCV"><a href="#2-2-2-OpenCV" class="headerlink" title="2.2.2 OpenCV"></a>2.2.2 OpenCV</h4><p>OpenCV是一个跨平台的<strong>计算机视觉库</strong>。OpenCV发展的非常早，拥有<strong>众多的计算机视觉、数字图像处理和机器视觉等功能</strong>。OpenCV在功能上<font color="#dd0000"><strong>比Pillow更加强大很多，但学习成本也高很多</strong></font>。</p><h3 id="2-3-数据扩增"><a href="#2-3-数据扩增" class="headerlink" title="2.3 数据扩增"></a>2.3 数据扩增</h3><p>现在回到赛题街道字符识别任务中。在赛题中我们需要对的图像进行字符识别，因此需要我们完成的数据的读取操作，同时也需要完成<strong>数据扩增（Data Augmentation）操作</strong>。</p><h4 id="2-3-1-基本介绍"><a href="#2-3-1-基本介绍" class="headerlink" title="2.3.1 基本介绍"></a>2.3.1 基本介绍</h4><p>在深度学习中数据扩增方法非常重要，数据扩增可以增加训练集的样本，同时也可以有效缓解模型过拟合的情况，也可以给模型带来的更强的泛化能力。</p><p>已知，在深度学习模型的训练过程中，数据扩增是<font color="#dd0000">必不可少的环节</font>。</p><ul><li><h4 id="数据扩增为什么有用？"><a href="#数据扩增为什么有用？" class="headerlink" title="数据扩增为什么有用？"></a><font color="#dd0000">数据扩增为什么有用？</font></h4></li></ul><ol><li><p>现有深度学习的参数非常多，一般的模型可训练的<font color="#dd0000"><strong>参数量基本上都是万到百万级别，而训练集样本的数量很难有这么多</strong></font>。</p></li><li><p>其次数据扩增可以<font color="#dd0000"> <strong>扩展样本空间</strong></font>：假设现在的分类模型需要对汽车进行分类</p><p>如果<strong>不使用任何数据扩增方法</strong>，深度学习模型会<strong>从汽车车头的角度❌</strong>来进行判别，<strong>而不是汽车具体的区别✅</strong>。</p></li></ol><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9Ecar.png" alt="汽车分类"></p><h4 id="2-3-2-常见的数据扩增方法"><a href="#2-3-2-常见的数据扩增方法" class="headerlink" title="2.3.2 常见的数据扩增方法"></a>2.3.2 常见的数据扩增方法</h4><ul><li><h4 id="有哪些数据扩增方法？"><a href="#有哪些数据扩增方法？" class="headerlink" title="有哪些数据扩增方法？"></a>有哪些数据扩增方法？</h4></li></ul><p>数据扩增方法有很多：<font color="#dd0000">从<strong>颜色空间、尺度空间到样本空间</strong>，同时根据不同任务数据扩增都有相应的区别</font>。</p><blockquote><p>对于图像分类，数据扩增一般<font color="#dd0000"><strong>不会改变标签</strong></font>;(即本比赛的<strong>需求</strong>场景)</p><p>对于物体检测，数据扩增会改变物体坐标位置；</p><p>对于图像分割，数据扩增会改变像素标签。</p></blockquote><p><strong>Note</strong>: 在本次赛题中，赛题任务是需要对图像中的字符进行识别，因此对于<strong><font color="#dd0000">字符图片并不能进行翻转操作</font>。比如字符6经过水平翻转就变成了字符9</strong>，<font color="#dd0000"><strong>会改变字符原本的含义</strong></font></p><ul><li><h4 id="具体常用的方法和数据扩增库"><a href="#具体常用的方法和数据扩增库" class="headerlink" title="具体常用的方法和数据扩增库"></a><strong>具体</strong>常用的方法和数据扩增<u>库</u></h4></li></ul><p>在常见的数据扩增方法中，一般会从<strong>图像颜色、尺寸、形态、空间和像素等角度</strong>进行变换。当然不同的数据扩增方法可以自由进行组合，得到更加丰富的数据扩增方法。</p><p>以<strong><a href="https://pytorch.org/docs/stable/torchvision/index.html" target="_blank" rel="noopener">torchvision</a></strong>(pytorch官方提供的数据扩增库，提供了基本的数据数据扩增方法，可以<u>无缝与torch进行集成</u>；但<u>数据扩增方法种类较少，且速度中等</u>)为例，常见的数据扩增方法（API）包括：</p><pre class=" language-python"><code class="language-python"><span class="token operator">-</span> transforms<span class="token punctuation">.</span>CenterCrop 对图片中心进行裁剪<span class="token operator">-</span> transforms<span class="token punctuation">.</span>ColorJitter 对图像颜色的对比度、饱和度和零度进行变换<span class="token operator">-</span> transforms<span class="token punctuation">.</span>FiveCrop 对图像四个角和中心进行裁剪得到五分图像<span class="token operator">-</span> transforms<span class="token punctuation">.</span>Grayscale 对图像进行灰度变换<span class="token operator">-</span> transforms<span class="token punctuation">.</span>Pad 使用固定值进行像素填充<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomAffine 随机仿射变换<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomCrop 随机区域裁剪<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomHorizontalFlip 随机水平翻转<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomRotation 随机旋转<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomVerticalFlip 随机垂直翻转</code></pre><p>除了torchvision，还有速度<strong>更快的第三方扩增库</strong>供选择：</p><ol><li><p><a href="https://github.com/aleju/imgaug" target="_blank" rel="noopener">imgaug</a> 提供了多样的数据扩增方法，且组合起来非常方便，速度较快；</p></li><li><p><a href="https://albumentations.readthedocs.io" target="_blank" rel="noopener">albumentations</a> 提供了多样的数据扩增方法，对图像分类、语义分割、物体检测和关键点检测都支持，速度较快。</p></li></ol><h2 id="2-4-Pytorch读取数据"><a href="#2-4-Pytorch读取数据" class="headerlink" title="2.4 Pytorch读取数据"></a>2.4 Pytorch读取数据</h2><p>由于本次赛题我们使用Pytorch框架讲解具体的解决方案，接下来将是解决赛题的<strong>第一步</strong>使用<strong>Pytorch读取赛题数据</strong>。</p><h4 id="2-4-1-一些定义-写代码前想好大致逻辑"><a href="#2-4-1-一些定义-写代码前想好大致逻辑" class="headerlink" title="2.4.1 一些定义(写代码前想好大致逻辑)"></a>2.4.1 一些定义(写代码前想好大致逻辑)</h4><p>首先，<strong>区分</strong>Dataset和DataLoder这两个<u>数据处理的常用术语</u> 和 <strong>解释</strong>有了Dataset为什么还要有DataLoder？</p><p>其实这两个是两个不同的概念，是为了<strong>实现不同的功能</strong>。</p><ul><li>Dataset：对<font color="#dd0000"><strong>数据集的封装</strong></font>，提供索引方式的对数据样本进行读取</li><li>DataLoder：对<font color="#dd0000"><strong>Dataset进行封装</strong></font>，提供批量读取的迭代读取</li></ul><p>而<font color="#dd0000"><strong>在Pytorch中的数据读取逻辑</strong></font>， 数据①先<strong>通过Dataset进行封装</strong>，②再<u><strong>通过DataLoder进行并行读取</strong></u>。</p><h4 id="2-4-2-代码"><a href="#2-4-2-代码" class="headerlink" title="2.4.2 代码"></a>2.4.2 代码</h4><h5 id="Step①定义对数据集封装的Dataset-详情，见注释"><a href="#Step①定义对数据集封装的Dataset-详情，见注释" class="headerlink" title="Step①定义对数据集封装的Dataset(详情，见注释)"></a>Step①定义对数据集封装的Dataset(详情，见注释)</h5><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 省略各种import </span><span class="token keyword">class</span> <span class="token class-name">SVHNDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># constructor</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img_path<span class="token punctuation">,</span> img_label<span class="token punctuation">,</span> transform<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>img_path <span class="token operator">=</span> img_path        self<span class="token punctuation">.</span>img_label <span class="token operator">=</span> img_label         <span class="token keyword">if</span> transform <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>transform <span class="token operator">=</span> None    <span class="token comment" spellcheck="true"># getter: 因为Dataset是提供【索引方式】的对数据样本进行读取</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        img <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_path<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 原始SVHN中类别10为数字0</span>        lbl <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_label<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int<span class="token punctuation">)</span>        lbl <span class="token operator">=</span> list<span class="token punctuation">(</span>lbl<span class="token punctuation">)</span>  <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">5</span> <span class="token operator">-</span> len<span class="token punctuation">(</span>lbl<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> img<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>lbl<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_path<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取: 图片数据和label的路径</span>train_path <span class="token operator">=</span> glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span><span class="token string">'../input/train/*.png'</span><span class="token punctuation">)</span>train_path<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>train_json <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>open<span class="token punctuation">(</span><span class="token string">'../input/train.json'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>train_label <span class="token operator">=</span> <span class="token punctuation">[</span>train_json<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> train_json<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 定义数据集实例</span>data <span class="token operator">=</span> SVHNDataset<span class="token punctuation">(</span>train_path<span class="token punctuation">,</span> train_label<span class="token punctuation">,</span>          transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>              <span class="token comment" spellcheck="true"># 缩放到固定尺寸</span>              transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token comment" spellcheck="true">########################## 数据扩增 ##########################</span>              <span class="token comment" spellcheck="true"># 随机颜色变换</span>              transforms<span class="token punctuation">.</span>ColorJitter<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 加入随机旋转</span>              transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 将图片转换为pytorch 的tesntor</span>              transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 对图像像素进行归一化</span>                            transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span><span class="token number">0.456</span><span class="token punctuation">,</span><span class="token number">0.406</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span><span class="token number">0.224</span><span class="token punctuation">,</span><span class="token number">0.225</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h5 id="Step②定义对Dataset封装的DataLoader"><a href="#Step②定义对Dataset封装的DataLoader" class="headerlink" title="Step②定义对Dataset封装的DataLoader"></a>Step②定义对Dataset封装的DataLoader</h5><p>加入DataLoder：数据按照<strong>批次(batch_size=10)</strong>获取，每批次调用Dataset读取单个样本进行拼接。</p><p>此时data的格式为：<code>torch.Size([10, 3, 64, 128]), torch.Size([10, 6])</code>。</p><p>前者为图像文件，为batchsize * chanel * height * width次序；后者为字符标签。</p><pre class=" language-python"><code class="language-python">train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>data<span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 封装上面的dataset即可</span>    batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 每批样本个数</span>    shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 是否打乱顺序</span>    num_workers<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 读取的线程个数</span><span class="token punctuation">)</span><span class="token keyword">for</span> data <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 后续操作...</span></code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一章节，官方提供了三种不同的解决方案。从本章开始 将逐渐的学习使用&lt;strong&gt;【定长字符识别】思路&lt;/strong&gt;来构建模型，讲解赛题的解决方案和相应知识点。&lt;/p&gt;
&lt;h2 id=&quot;2-数据读取与数据扩增&quot;&gt;&lt;a href=&quot;#2-数据读取与数据扩增&quot; clas
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-01赛题研究</title>
    <link href="http://yoursite.com/2020/05/19/datawhale-cv01/"/>
    <id>http://yoursite.com/2020/05/19/datawhale-cv01/</id>
    <published>2020-05-18T16:53:16.000Z</published>
    <updated>2020-05-23T10:45:24.082Z</updated>
    
    <content type="html"><![CDATA[<p>这次<strong>基础</strong>赛事, 是Datawhale与天池联合发起的零基础<strong>入门系列</strong>赛事 <a href="https://tianchi.aliyun.com/competition/entrance/531795/introduction" target="_blank" rel="noopener">赛事地址</a></p><h1 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h1><ol><li><strong>总结</strong>基本了解比赛规则</li><li><strong>总结</strong>解题思路</li><li>数据下载和<strong>理解</strong></li></ol><h1 id="1-规则"><a href="#1-规则" class="headerlink" title="1.规则"></a>1.规则</h1><p>本赛题需要选手识别图片中所有的字符。<strong>评测指标</strong>：准确率，Score=编码识别正确的数量/测试集图片数量</p><p>为了降低比赛难度，我们提供了训练集、验证集中所有字符的<strong>位置框</strong>（在<strong>阿里天池</strong>上下载）。</p><p><strong>注意</strong>: 按照比赛规则，所有的参赛选手，<strong>只能使用比赛给定的数据集完成训练(不要使用SVHN原始数据集进行训练</strong>）</p><h4 id="使用的Python模块"><a href="#使用的Python模块" class="headerlink" title="使用的Python模块"></a>使用的Python模块</h4><p>大概介绍一下，这里可能需要用到的主要模块。</p><blockquote><p>numpy ：提供了python对多维数组对象的支持：ndarray，具有矢量运算能力，快速、节省空间。numpy支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。</p><p>torch：神经网络界的 Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算. 所以神经网络的话, 当然是用 Torch 的 tensor 形式数据最好</p><p>torchvision：torchvision包是服务于pytorch深度学习框架的,用来生成图片,视频数据集,和一些流行的模型类和预训练模型。我认为这个是最关键的模块</p><p>OpenCV（import时候是cv2）：一款强大的跨平台的计算机视觉库，使用它能完成我们对于图像和视频处理的很多功能。它以电信号的方式加以捕捉、记录、处理、储存、传送与重现的各种技术。这里主要是用来对图片的处理</p><p>json：这个就是json的读写库，处理json文件的</p></blockquote><h1 id="2-数据理解"><a href="#2-数据理解" class="headerlink" title="2. 数据理解"></a>2. 数据理解</h1><h4 id="数据集初步观察"><a href="#数据集初步观察" class="headerlink" title="数据集初步观察"></a>数据集初步观察</h4><p>分.json的label位置信息，和原图集合</p><p><strong>数据读取</strong>： json文件包含的位置信息，除了便于正式的训练，还可以用于数据观察——直接作用在原图集，<strong>查看已给的位置信息的分割效果</strong></p><p>样例代码: 数据读取，在此我们给出JSON中标签的读取方式</p><pre><code>import jsontrain_json = json.load(open(&#39;../input/train.json&#39;))# 数据标注处理def parse_json(d):   arr = np.array([       d[&#39;top&#39;], d[&#39;height&#39;], d[&#39;left&#39;],  d[&#39;width&#39;], d[&#39;label&#39;]   ])   arr = arr.astype(int)   return arrimg = cv2.imread(&#39;../input/train/000000.png&#39;)arr = parse_json(train_json[&#39;000000.png&#39;])plt.figure(figsize=(10, 10))plt.subplot(1, arr.shape[1]+1, 1)plt.imshow(img)plt.xticks([]); plt.yticks([])for idx in range(arr.shape[1]):   plt.subplot(1, arr.shape[1]+1, idx+2)   plt.imshow(img[arr[0, idx]:arr[0, idx]+arr[1, idx],arr[2, idx]:arr[2, idx]+arr[3, idx]])   plt.title(arr[4, idx])   plt.xticks([]); plt.yticks([])</code></pre><p>输出</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez59nbvuij30z30u0gtm.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez59nbvuij30z30u0gtm.jpg" alt="img"></a></p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez5ab60sij30yw0oaqc0.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez5ab60sij30yw0oaqc0.jpg" alt="img"></a></p><h1 id="3-潜在的疑难杂症"><a href="#3-潜在的疑难杂症" class="headerlink" title="3. 潜在的疑难杂症"></a>3. 潜在的疑难杂症</h1><h4 id="预处理细节"><a href="#预处理细节" class="headerlink" title="预处理细节"></a>预处理细节</h4><p>数据集存在原图片<strong>大小不统一</strong>，这个只需要用pytorch的transforms处理即可</p><h4 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h4><h5 id="a-确定待识别数字在图中的位置"><a href="#a-确定待识别数字在图中的位置" class="headerlink" title="a.确定待识别数字在图中的位置"></a>a.确定待识别数字在图中的位置</h5><p>(使用比赛简化后的数据，则该问题并不存在了)</p><p>在简化数据集之前的难点是：模型要能找到待识别数字的<strong>位置</strong>。但是既然处理后的数据集，<strong>位置信息全都提供了</strong>，那么这个问题就容易很多——<strong>单纯的识别数字信息</strong>。数字的位数问题可以通过简单的算法来解决，就像MNIST数据集一样。</p><p><a href="https://crazy-winds.github.io/images/cv1/0-1.png" target="_blank" rel="noopener"><img src="https://crazy-winds.github.io/images/cv1/0-1.png" alt="图1"></a>图1</p><p>（json格式存储label和位置信息）</p><p><a href="https://crazy-winds.github.io/images/cv1/0-2.png" target="_blank" rel="noopener"><img src="https://crazy-winds.github.io/images/cv1/0-2.png" alt="图2"></a>图2</p><h5 id="b-确定待识别数字的个数"><a href="#b-确定待识别数字的个数" class="headerlink" title="b.确定待识别数字的个数"></a>b.确定待识别数字的个数</h5><p>即每幅图的数字个数可能均不相同，如何统一的解决(找到一种general的方法)</p><p>将在解题思路部分详细展开</p><h1 id="4-大致解题思路"><a href="#4-大致解题思路" class="headerlink" title="4. 大致解题思路"></a>4. 大致解题思路</h1><ol><li>简单入门思路：定长字符识别。将不定长字符转化为定长处理，不足部分用<strong>填充占位符</strong>为代替</li></ol><p>（<strong>适合新手</strong>也<strong>适合此题给的处理后的数据集</strong>：赛题数据集中大部分图像中字符个数为2-4个，最多的字符 个数为<strong>6个</strong>）</p><p><a href="https://crazy-winds.github.io/images/cv1/0-3.png" target="_blank" rel="noopener"><img src="https://crazy-winds.github.io/images/cv1/0-3.png" alt="图3"></a>图3</p><ol><li>专业字符识别思路：按照<strong>不定长字符</strong>处理</li></ol><p>在字符识别研究中，有<strong>特定的方法</strong>来解决此种不定长的字符识别问题：如<strong>典型的有CRNN字符识别模型</strong>。</p><p>因为本次赛题中给定的图像数据都<strong>比较规整，可以视为一个单词或者一个句子</strong> 喂进CRNN模型。</p><ol><li>专业分类思路：检测位置再识别数字</li></ol><p>在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用<strong>物体检测</strong>的思路完成。</p><p>可参考物体检测模型：<strong>SSD或者YOLO</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这次&lt;strong&gt;基础&lt;/strong&gt;赛事, 是Datawhale与天池联合发起的零基础&lt;strong&gt;入门系列&lt;/strong&gt;赛事 &lt;a href=&quot;https://tianchi.aliyun.com/competition/entrance/531795/int
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>NLP-text-process</title>
    <link href="http://yoursite.com/2020/02/07/NLP-text-process/"/>
    <id>http://yoursite.com/2020/02/07/NLP-text-process/</id>
    <published>2020-02-07T07:06:46.000Z</published>
    <updated>2020-06-07T09:41:41.055Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP文本处理"><a href="#NLP文本处理" class="headerlink" title="NLP文本处理"></a>NLP文本处理</h1><h2 id="NLP-general-pipeline"><a href="#NLP-general-pipeline" class="headerlink" title="NLP general pipeline"></a>NLP general pipeline</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjqjhtaemj31kj0u0qte.jpg" alt="pipeline" style="zoom:50%;" /><h2 id="文本处理阶段"><a href="#文本处理阶段" class="headerlink" title="文本处理阶段"></a>文本处理阶段</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjqm2tae6j31go08gai2.jpg" alt="text process" style="zoom:50%;" /><h3 id="常见处理技术"><a href="#常见处理技术" class="headerlink" title="常见处理技术"></a>常见处理技术</h3><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200607151346944.png" alt="image-20200607151346944" style="zoom:50%;" /><hr><h2 id="分词工具的底层算法"><a href="#分词工具的底层算法" class="headerlink" title="分词工具的底层算法"></a>分词工具的底层算法</h2><p>所有分词的工具，都要<strong><u>依赖于词库</u></strong></p><h3 id="1-最大匹配算法-Max-Matching——贪心算法"><a href="#1-最大匹配算法-Max-Matching——贪心算法" class="headerlink" title="1. 最大匹配算法 Max Matching——贪心算法"></a>1. <u>最大</u>匹配算法 Max Matching——<font color="#dd0000"><u>贪心</u></font>算法</h3><h5 id="何为最大？-人为设的-每次截取的最大长度——经验决定，看词典和这种语言中词语常见的最大长度"><a href="#何为最大？-人为设的-每次截取的最大长度——经验决定，看词典和这种语言中词语常见的最大长度" class="headerlink" title="何为最大？(人为设的)每次截取的最大长度——经验决定，看词典和这种语言中词语常见的最大长度"></a>何为<u>最大</u>？(人为设的)每次截取的最大长度——<u>经验</u>决定，看词典和这种语言中<u>词语常见的最大长度</u></h5><ul><li><u>前向</u>最大匹配(forward-max matching)</li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjs4728c6j31gi0t07wh.jpg" alt="前向最大匹配" style="zoom:50%;" /><ul><li><u>后向</u>最大匹配(backward-max matching): 句子的<strong>遍历方向，从右到左</strong>和前向<strong>相反</strong>而已<ul><li>和前向的效果90%大多数情况，结果一样</li><li>不一样的情况：后向可能会导致一些分词的琐碎</li></ul></li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjs9uae0oj31cg0nudwn.jpg" alt="image-20200607160130052" style="zoom:50%;" /><ul><li>双向：结合了前向和后向，双向遍历</li></ul><h4 id="该算法的缺点"><a href="#该算法的缺点" class="headerlink" title="该算法的缺点"></a>该算法的缺点</h4><ul><li><p>贪心只能找局部最优，未必最优 </p></li><li><p>效率，依赖于max_length</p></li><li><p><font color="#dd0000">容易歧义，因为算法不支持考虑<strong><u>语义</u></strong></font>，<strong>不智能</strong>。算法习得的：<strong>纯粹是单词</strong>，没有任何<strong>句法</strong>甚至<strong>语义</strong></p><ul><li><p>比如一个case：在词典中，一个词是另一个词的子集，而最大匹配会<strong>保留那个更长的词</strong>(<strong>匹配到</strong>该更长的词，则提前结束遍历)</p><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200607161233659.png" alt="image-20200607161233659" style="zoom:35%;" /></li></ul></li></ul><hr><p>那么，什么样的算法工具，能考虑到语义？想想<strong>语言模型</strong>(机器翻译也用到了)</p><p>语言模型，可以evaluate一句话<font color="#dd0000"><strong>语义的正确性</strong></font>——基于<strong>已有的词频的统计</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjt7ifbxlj310m0doak1.jpg" alt="人工的词频统计" style="zoom:33%;" /><p>所以，对于分词结果，计算其语义合理性</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjt8qkr98j319209cn88.jpg" style="zoom:33%;" /><h3 id="2-考虑语义—by语言模型-机器翻译也用到了"><a href="#2-考虑语义—by语言模型-机器翻译也用到了" class="headerlink" title="2. 考虑语义—by语言模型(机器翻译也用到了)"></a>2. 考虑语义—by<u>语言模型</u>(<u>机器翻译</u>也用到了)</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjux5rp84j31760u07wh.jpg" alt="加入语言模型" style="zoom:80%;" /><h4 id="可能出现的问题和解决-underflow"><a href="#可能出现的问题和解决-underflow" class="headerlink" title="可能出现的问题和解决: underflow"></a>可能出现的问题和解决: underflow</h4><p>问题：词频统计的概率太小，小数一旦再乘积，导致underflow的溢出</p><p>解决：<strong>数学处理技巧</strong>，给概率加上log<strong>转化为<u>累加</u></strong>，利用log的<strong><u>递增型</u></strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjtgdx5m3j316y0f6ws9.jpg" alt="log转化" style="zoom:33%;" /><h4 id="缺点和解决"><a href="#缺点和解决" class="headerlink" title="缺点和解决"></a>缺点和解决</h4><h5 id="缺点：如果要考虑所有可能，复杂度太高"><a href="#缺点：如果要考虑所有可能，复杂度太高" class="headerlink" title="缺点：如果要考虑所有可能，复杂度太高"></a>缺点：如果要考虑所有可能，复杂度太高</h5><h5 id="解决：维特比算法"><a href="#解决：维特比算法" class="headerlink" title="解决：维特比算法"></a>解决：维特比算法</h5><p>(问题和解决方法，都和我<a href="https://kennyng-19.github.io/Kenny_Ng.github.io/2020/01/29/NLP-intro/" target="_blank" rel="noopener">NLP-intro</a>文中的机器翻译case的<strong>穷举法下</strong>的情况，<strong>是一样的</strong>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;NLP文本处理&quot;&gt;&lt;a href=&quot;#NLP文本处理&quot; class=&quot;headerlink&quot; title=&quot;NLP文本处理&quot;&gt;&lt;/a&gt;NLP文本处理&lt;/h1&gt;&lt;h2 id=&quot;NLP-general-pipeline&quot;&gt;&lt;a href=&quot;#NLP-general-p
      
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP-intro</title>
    <link href="http://yoursite.com/2020/01/29/NLP-intro/"/>
    <id>http://yoursite.com/2020/01/29/NLP-intro/</id>
    <published>2020-01-29T05:30:38.000Z</published>
    <updated>2020-06-07T08:26:54.568Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-NLP-Overview"><a href="#0-NLP-Overview" class="headerlink" title="0. NLP Overview"></a>0. NLP Overview</h1><h2 id="a-NLP研究方向"><a href="#a-NLP研究方向" class="headerlink" title="a. NLP研究方向"></a>a. NLP研究方向</h2><p>2个大方向对应了，语言的<strong>“一进一出”</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf99ljg3ooj30v80hswh7.jpg" alt=""></p><h2 id="b-技术的4个-实现难度递增的-维度"><a href="#b-技术的4个-实现难度递增的-维度" class="headerlink" title="b. 技术的4个(实现难度递增的)维度"></a>b. 技术的4个(实现难度递增的)维度</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gff3scbnfmj31980pynao.jpg" alt="难度递增的四个维度"></p><h3 id="简称-全称-英文"><a href="#简称-全称-英文" class="headerlink" title="简称-全称/英文"></a>简称-全称/英文</h3><ul><li><p><strong>Relation Extraction</strong>(重要): <strong>关系抽取</strong></p></li><li><p>Parsing：句法分析——给定句子，根据<u>词性</u>生成<strong>树状结构</strong> by CYK算法</p></li><li><p>Dependency Parsing： (词的)依存分析 ——生成<strong>graph</strong></p></li></ul><ul><li><p>POS: Part of Speech <strong>词性</strong>分析(词性是很重要的特征)</p></li><li><p>NER：Named Entity Recognition <strong>命名实体</strong>识别——<strong><u>抽取</u></strong>语句中，我<strong>重点关注</strong>的<strong>名词</strong></p></li></ul><h1 id="1-case-机器翻译"><a href="#1-case-机器翻译" class="headerlink" title="1. case-机器翻译"></a>1. case-机器翻译</h1><h2 id="思路a-暴力法"><a href="#思路a-暴力法" class="headerlink" title="思路a: 暴力法"></a>思路a: 暴力法</h2><h3 id="原理：全排列组合分词后的句子，用语言模型从中筛选出语义最合适的"><a href="#原理：全排列组合分词后的句子，用语言模型从中筛选出语义最合适的" class="headerlink" title="原理：全排列组合分词后的句子，用语言模型从中筛选出语义最合适的"></a>原理：<strong>全排列组合</strong>分词后的句子，用语言模型从中筛选出语义最合适的</h3><p>第一步通过<strong>翻译模型TM</strong>进行<strong>分词</strong></p><p>第二步映射词语，因为语法的存在而进行<u>映射后词语的排列组合</u></p><p>第三步通过<strong>语言模型LM</strong>选出排列组合中最合适的语句</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf99soj3vmj317w0kuq8d.jpg" alt="">缺点: 排列组合的复杂度太高</p><p>解决: <strong>维特比Viterbi</strong>算法——<strong>同时考虑TM</strong>分词+<strong>LM筛选</strong>这分开2个步骤</p><h2 id="思路b-维特比Viterbi算法"><a href="#思路b-维特比Viterbi算法" class="headerlink" title="思路b: 维特比Viterbi算法"></a>思路b: <strong>维特比Viterbi</strong>算法</h2><h3 id="原理：贝叶斯TM和LM"><a href="#原理：贝叶斯TM和LM" class="headerlink" title="原理：贝叶斯TM和LM"></a>原理：贝叶斯TM和LM</h3><p><strong>串联TM</strong>分词+<strong>LM筛选</strong>这分开的2个步骤</p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9i9qd3wlj31680f20vt.jpg" alt="流程图"></p><p>各个部分的作用</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9iup3gr2j30x20icwjg.jpg" alt="">好处：降低了复杂度，<strong>避免了原来随机排列组合生成句子</strong>的<strong>NP-hard的指数级</strong>复杂度</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9iao2h3fj309802e749.jpg" alt=""></p><h4 id="1-语言模型LM"><a href="#1-语言模型LM" class="headerlink" title="1. 语言模型LM"></a>1. 语言模型LM</h4><h5 id="作用：生成句子-即输出"><a href="#作用：生成句子-即输出" class="headerlink" title="作用：生成句子(即输出)"></a>作用：生成句子(即输出)</h5><h5 id="分类：按照当前生成的语言，对之前词语的”记忆“程度"><a href="#分类：按照当前生成的语言，对之前词语的”记忆“程度" class="headerlink" title="分类：按照当前生成的语言，对之前词语的”记忆“程度"></a>分类：按照当前生成的语言，对之前词语的”记忆“程度</h5><p>这种<strong>连续的条件概率</strong>——又称，<strong>联合分布</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9i640a7bj316e0aojut.jpg" alt=""></p><p><strong>Unigram</strong> model, <strong>Bingram</strong> model …. n-gram model</p><p>至于每个Prop, 则源于<strong>已有的<u>统计频率</u>结果</strong></p><h5 id="性能标准"><a href="#性能标准" class="headerlink" title="性能标准"></a>性能标准</h5><p>生成之后同时看<strong>句子质量</strong>和<strong>对应概率(score)</strong>，句子<strong>错误少</strong>的<strong>概率应该越大</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9iknu2qsj30yw06idh3.jpg" alt=""></p><h4 id="2-翻译模型TM"><a href="#2-翻译模型TM" class="headerlink" title="2. 翻译模型TM"></a>2. 翻译模型TM</h4><p>…</p><h1 id="2-case-基于检索的智能问答系统"><a href="#2-case-基于检索的智能问答系统" class="headerlink" title="2. case-基于检索的智能问答系统"></a>2. case-基于检索的智能问答系统</h1><h3 id="数据-输入-语料库"><a href="#数据-输入-语料库" class="headerlink" title="数据/输入: 语料库"></a>数据/输入: 语料库</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfinf6trm9j311w0kuke0.jpg" alt="写好的问与答结对"></p><h3 id="NLP-general-pipeline"><a href="#NLP-general-pipeline" class="headerlink" title="NLP general pipeline"></a>NLP general pipeline</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjqjhtaemj31kj0u0qte.jpg" alt="image-20200607150135228"></p><h3 id="QAsys的流程"><a href="#QAsys的流程" class="headerlink" title="QAsys的流程"></a>QAsys的流程</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfir8926c2j31ip0u0e81.jpg" alt="image-20200606183943480"></p><h3 id="解决：如何搜索出最相似问题"><a href="#解决：如何搜索出最相似问题" class="headerlink" title="解决：如何搜索出最相似问题"></a>解决：如何搜索出最相似问题</h3><h4 id="相似度的衡量？"><a href="#相似度的衡量？" class="headerlink" title="相似度的衡量？"></a>相似度的衡量？</h4><ul><li>❌<strong>逐字</strong>比较：正则/<strong>规则</strong>AI——<strong>对匹配要求很高很高</strong>, 所以需要考虑<strong><u>尽可能多</u></strong>的输入，过于繁琐<ul><li>唯一使用场景：<strong><u>没有数据时</u></strong>，语料库很空</li></ul></li><li>✅<strong>概率</strong>比较：字符串的<strong>“相似性”计算</strong></li></ul><h3 id="阶段一-文本处理"><a href="#阶段一-文本处理" class="headerlink" title="阶段一 文本处理"></a>阶段一 <a href="https://kennyng-19.github.io/Kenny_Ng.github.io/2019/08/07/NLP-text-process/" target="_blank" rel="noopener">文本处理</a></h3><p>常用技术</p><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200607151346944.png" alt="image-20200607151346944" style="zoom:50%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;0-NLP-Overview&quot;&gt;&lt;a href=&quot;#0-NLP-Overview&quot; class=&quot;headerlink&quot; title=&quot;0. NLP Overview&quot;&gt;&lt;/a&gt;0. NLP Overview&lt;/h1&gt;&lt;h2 id=&quot;a-NLP研究方向&quot;&gt;&lt;a h
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch一些小细节补充</title>
    <link href="http://yoursite.com/2019/08/26/sum_pytorch_details/"/>
    <id>http://yoursite.com/2019/08/26/sum_pytorch_details/</id>
    <published>2019-08-26T08:32:47.000Z</published>
    <updated>2020-05-26T09:05:12.724Z</updated>
    
    <content type="html"><![CDATA[<h1 id="补充Pytorch中的易被忽视但得注意的细节"><a href="#补充Pytorch中的易被忽视但得注意的细节" class="headerlink" title="补充Pytorch中的易被忽视但得注意的细节"></a>补充Pytorch中的易被忽视但得注意的细节</h1><h4 id="1-定义Dataloader的num-workers参数"><a href="#1-定义Dataloader的num-workers参数" class="headerlink" title="1. 定义Dataloader的num_workers参数"></a>1. 定义Dataloader的num_workers参数</h4><p><strong>Q</strong>: </p><p>在给Dataloader设置worker数量（<code>num_worker</code>）时，到底<strong>设置多少合适</strong>？这个worker到底怎么工作的？<br><strong>如果将<code>num_worker</code>设为0（也是默认值），就没有worker了吗？</strong></p><p><strong>A</strong>: <code>num_workers</code>的经验设置值<strong>是<font color="#dd0000">自己电脑/服务器的CPU核心数</font></strong>，(比如我的Macbook Pro16寸是 6核)如果CPU很强+配的<strong>RAM是充足的</strong>，就可以设置为<strong>略微≥核心数</strong>。</p><ol><li><p>参数的<strong>作用</strong>： 官方注释为：</p><blockquote><p> how many <strong>subprocesses</strong> to use for data loading.</p><p> <code>0</code> means that the data will be loaded <strong>in the main process.</strong> (default: <code>0</code>)</p></blockquote><p>作用：denotes the <strong><font color="#dd0000">number of processes that generate batches in parallel</font></strong> to generate your data <font color="#dd0000"><strong>on multiple cores</strong> in real time</font></p><p>详细解释：每每轮到dataloader加载数据时：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>start_epoch<span class="token punctuation">,</span> end_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span><span class="token punctuation">:</span></code></pre><p>dataloader<strong>一次性创建<code>num_worker</code>个worker</strong>，（也可以说dataloader一次性创建<code>num_worker</code>个工作进程，worker也是普通的工作进程），并用<code>batch_sampler</code>将指定batch分配给指定worker，worker将它负责的batch加载进RAM。</p><p>然后，dataloader从RAM中找本轮迭代要用的batch，如果找到了，就使用。如果没找到，就要<code>num_worker</code>个worker继续加载batch到内存，直到dataloader在RAM中找到目标batch。一般情况下都是能找到的，因为<code>batch_sampler</code>指定batch时当然优先指定本轮要用的batch。</p></li><li><p><strong>不同赋值时各自的意义</strong>：<code>num_worker</code>设置得大，好处是<strong>寻batch速度快</strong>，因为下一轮迭代的batch很可能在上一轮/上上一轮…迭代时已经加载好了。坏处是<strong>内存开销大</strong>，也加重了CPU负担（worker加载数据到RAM的进程是CPU复制的嘛）。</p><p>如果设为0，意味着每一轮迭代时，dataloader<strong>不再有自主加载数据到RAM</strong>这一步骤（因为没有worker了），而是在RAM中找batch，找不到时再加载相应的batch。缺点当然是<strong>速度更慢</strong>。</p></li></ol><h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;补充Pytorch中的易被忽视但得注意的细节&quot;&gt;&lt;a href=&quot;#补充Pytorch中的易被忽视但得注意的细节&quot; class=&quot;headerlink&quot; title=&quot;补充Pytorch中的易被忽视但得注意的细节&quot;&gt;&lt;/a&gt;补充Pytorch中的易被忽视但得注意的
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>(转)算法复杂度-intro(推导分治算法的master theorem)</title>
    <link href="http://yoursite.com/2018/02/06/algo-complexity-intro/"/>
    <id>http://yoursite.com/2018/02/06/algo-complexity-intro/</id>
    <published>2018-02-06T08:10:13.000Z</published>
    <updated>2020-06-06T10:45:18.215Z</updated>
    
    <content type="html"><![CDATA[<p>在实现算法的时候，通常会从两方面考虑算法的复杂度，即时间复杂度和空间复杂度。</p><p>顾名思义，时间复杂度用于度量算法的计算工作量；空间复杂度用于度量算法占用的内存空间。</p><p>由于算法测试结果非常<strong>依赖测试环境，且受数据规模的影响，</strong>因此需要一种粗略的评估方法，即时间、空间复杂度分析方法。</p><h2 id="渐进时间复杂度"><a href="#渐进时间复杂度" class="headerlink" title="渐进时间复杂度"></a>渐进时间复杂度</h2><p>理论上来说，时间复杂度是算法运算所消耗的时间，但是对于一个算法来说，评估运行时间是很难的，因为针对不同大小的输入数据，算法处理所要消耗的时间是不同的，因此通常关注的是时间频度，即算法运行计算操作的次数，记为 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> ，其中n称为问题的规模。同样，因为n是一个变量，n发生变化时，时间频度 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 也在发生变化，我们称时间复杂度的极限情形称为算法的“渐近时间复杂度”，记为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。这种表示方法称作<strong>大 O 时间复杂度表示法</strong>。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示<strong>代码执行时间随数据规模增长的变化趋势</strong>，因为我们没办法用通常的评估方式来评估所有算法的时间复杂度，所以通常使用渐进时间复杂度表示算法的时间复杂度，下文中的时间复杂度均表示渐进时间复杂度。</p><p>我们在这里放一个python算法的例子来解释一下：</p><pre class=" language-python3"><code class="language-python3">def f(n):    a,b=0,0#运行消耗t0时间    for i in range(n):#运行一次平均消耗t1时间        a = a + rand()#运行一次平均消耗t2时间    for j in range(n):#运行一次平均消耗t3时间        b = b + rand()#运行一次平均消耗t4时间</code></pre><p>在这个例子中，我们分别计算 <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 函数的时间复杂度与空间复杂度。根据代码上执行的平均时间假设，计算出来执行 <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 的时间为 <img src="https://www.zhihu.com/equation?tex=T%28n%29%3Dt_0%2B%28t_1%2Bt_2%29%2An%2B%28t_3%2Bt_4%29%2An%3Dt_0%2B%28t_1%2Bt_2%2Bt_3%2Bt_4%29%2An" alt="[公式]"> ；而函数中申请了两个变量a,b，占用内存空间为2。上述 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 是我们对函数f(n)进行的准确时间复杂度的计算。但实际情况中，输入规模n是影响算法执行时间的因素之一。在n固定的情况下，不同的输入序列也会影响其执行时间。当n值非常大的时候， <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 函数中常数项t0以及n的系数 <img src="https://www.zhihu.com/equation?tex=%28t_1%2Bt_2%2Bt_3%2Bt_4%29" alt="[公式]"> 对n的影响也可以忽略不计了，因此这里函数 <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 的时间复杂度我们可以表示为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。我们再来看空间复杂度，跟时间复杂度表示类似，也可以用极限的方式来表示空间复杂度（但是貌似没有渐进空间复杂度的说法），因为这里只声明了2个变量，因此空间复杂度也是常数阶的，因此这里空间复杂度计算为 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]"> </p><p><strong>规律总结：</strong></p><p><strong>所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。</strong></p><p><img src="https://www.zhihu.com/equation?tex=T%28n%29%3DO%28f%28n%29%29" alt="[公式]"></p><p>其中f(n) 表示每行代码执行的次数总和。公式中的 O，表示代码的执行时间 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=+f%28n%29+" alt="[公式]"> 表达式成正比。</p><h3 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h3><p>因为我们计算的是极限状态下的时间复杂度，因此存在两种特性：</p><p>1.按照函数数量级角度来说，相对增长低的项对相对增长高的项产生的影响很小，可忽略不计。</p><p>2.最高项系数对最高项的影响也很小，因此也可以忽略不计。针对第1点，常见的时间复杂度有：常数阶：常数阶： <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]"> , 对数阶： <img src="https://www.zhihu.com/equation?tex=O%28log_2+n%29" alt="[公式]"> , 线性阶： <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> , k次方阶： <img src="https://www.zhihu.com/equation?tex=O%28n%5EK%29" alt="[公式]"> ,指数阶： <img src="https://www.zhihu.com/equation?tex=O%282%5En%29" alt="[公式]"> 。</p><p>根据上述两种特性，总结时间复杂度的计算方法：</p><p>1.<strong>加法法则-总复杂度等于量级最大的那段代码的复杂度：</strong>计算时只取相对增长最高的项，去掉低阶项，并去掉最高项的系数（比如 <img src="https://www.zhihu.com/equation?tex=O%282n%29" alt="[公式]"> 只需要表示为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> ）；</p><p>2.<strong>只关注循环执行次数最多的一段代码</strong>：大 O 这种复杂度表示方法只是表示一种变化趋势。通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了；</p><p>3.<strong>乘法法则-嵌套代码的复杂度等于嵌套内外代码复杂度的乘积：</strong>如果 <img src="https://www.zhihu.com/equation?tex=T_1%28n%29%3DO%28f%28n%29%29%2CT_2%28n%29%3DO%28f%28n%29%29" alt="[公式]"> ,那么 <img src="https://www.zhihu.com/equation?tex=T%28n%29%3DT_1%28n%29%E2%88%97T_2%28n%29%3DO%28f%28n%29%29%E2%88%97O%28g%28n%29%29%3DO%28f%28n%29%E2%88%97g%28n%29%29." alt="[公式]"> 也就是说假设<img src="https://www.zhihu.com/equation?tex=+T_1%28n%29+%3D+O%28n%29%EF%BC%8CT_2%28n%29+%3D+O%28n%5E2%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=T_1%28n%29+%2A+T_2%28n%29+%3D+O%28n%5E3%29" alt="[公式]"></p><p>4.<strong>针对常数阶，取时间复杂度为O(1)</strong>。</p><h2 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h2><p>空间复杂度指的是算法在内存上临时占用的空间，包括程序代码所占用的空间，输入数据占用的空间和变量占用的空间。在递归运算时，由于递归计算是需要使用堆栈的，所以需要考虑堆栈操作占用的内存空间大小。空间复杂度的计算也遵循渐进原则，即参考时间复杂度与空间复杂度计算方法项。</p><h3 id="计算方法-1"><a href="#计算方法-1" class="headerlink" title="计算方法"></a>计算方法</h3><p>1.通常只考虑参数表中为形参分配的存储空间和为函数体中定义的局部变量分配的存储空间（比如变量 <img src="https://www.zhihu.com/equation?tex=a%3D0" alt="[公式]"> 在算法中空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]"> ； <img src="https://www.zhihu.com/equation?tex=list_a%3D%5B0%2C1%2C....%2Cn%5D" alt="[公式]"> 的空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> ； <img src="https://www.zhihu.com/equation?tex=set%28list_a%29" alt="[公式]"> 的空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]">) )。</p><p>2.递归函数情况下，空间复杂度等于一次递归调用分配的临时存储空间的大小乘被调用的次数。</p><p>我们这里以递归方法实现的斐波那契数列为例：</p><pre class=" language-python3"><code class="language-python3">def fib(n):    if n < 3:         return 1    else:        return fib(n-2)+fib(n-1)</code></pre><p>斐波那契数列的序列依次为 <img src="https://www.zhihu.com/equation?tex=1%2C1%2C2%2C3%2C5%2C8%2C13....." alt="[公式]"> 特点是，当数列的长度大于等于3时，数列中任意位置的元素值等于该元素前两位元素之和。</p><p>1.计算时间复杂度： <img src="https://www.zhihu.com/equation?tex=O%282%5En%29" alt="[公式]"> 。计算方法：通过归纳证明的方法，我们尝试计算数列中第8个元素的值的递归调用次数,为了方便观察，我把外层括号替换为了大括号。 <img src="https://www.zhihu.com/equation?tex=fib%288%29%3Dfib%287%29%2Bfib%286%29+%3D%7Bfib%286%29%2Bfib%285%29%7D%2B%7Bfib%285%29%2Bfib%284%29%7D+" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3D%28%7Bfib%285%29%2Bfib%284%29%7D%2B%7Bfib%284%29%2Bfib%283%29%7D%29%2B%28%7Bfib%284%29%2Bfib%283%29%7D%2B%7Bfib%283%29%2Bfib%282%29%7D%29+%3D....." alt="[公式]"> 这里太多我就不一一写出了。不难发现，每次调用递归时，递归调用次数都是以程序中调用递归次数即2的指数形式增长的。第一层递归时，调用了2次 <img src="https://www.zhihu.com/equation?tex=fib%28n%29" alt="[公式]"> 函数；第二层递归时，第一层的2次递归调用分别又要调用2次，即调用了 <img src="https://www.zhihu.com/equation?tex=2%5E2" alt="[公式]"> 次；第三层递归调用了 <img src="https://www.zhihu.com/equation?tex=2%5E3" alt="[公式]"> 次，以此规律，不难算出时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%282%5En%29" alt="[公式]"></p><p>2.计算空间复杂度： <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。计算方法：我们同样使用归纳证明的方法，尝试推导数列中第6个元素值的内存占用情况。调用函数 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> ,此时因为有形参n传递，在栈中为n申请内存资源，我们为了方便，以 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> 表示栈中元素。此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> ;我们根据函数内的递归调用关系，为了计算 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> ,我们需要 <img src="https://www.zhihu.com/equation?tex=fib%285%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%284%29" alt="[公式]"> 的值，此时发生形参传递，栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29" alt="[公式]"> ;为了计算 <img src="https://www.zhihu.com/equation?tex=fib%284%29" alt="[公式]"> ，我们需要 <img src="https://www.zhihu.com/equation?tex=fib%283%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 的值，此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29%2Cfib%283%29%2Cfib%282%29" alt="[公式]"> ，但是由于 <img src="https://www.zhihu.com/equation?tex=fib%282%29%3D1" alt="[公式]"> ，此时 <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 函数计算完成， <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 出栈，此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29%2Cfib%283%29" alt="[公式]"> 。为了计算 <img src="https://www.zhihu.com/equation?tex=fib%283%29" alt="[公式]"> ，需要 <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%281%29" alt="[公式]"> 的值，此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29%2Cfib%283%29%2Cfib%282%29%2Cfib%281%29" alt="[公式]"> 。但是 <img src="https://www.zhihu.com/equation?tex=fib%282%29%3D1%2Cfib%281%29%3D1" alt="[公式]"> ,计算完成， <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%281%29" alt="[公式]"> 出栈。此时得到fib(3)的值为2，fib(3)出栈；由此出栈顺序， <img src="https://www.zhihu.com/equation?tex=fib%284%29%2Cfib%285%29%2Cfib%286%29" alt="[公式]"> 也会随计算完成出栈。不难发现，在此次递归计算的过程中，内存中最多消耗了6个内存资源，由归纳证明法得出 <img src="https://www.zhihu.com/equation?tex=fib%28n%29" alt="[公式]"> 的空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。</p><h2 id="Master-Theorem-主方法"><a href="#Master-Theorem-主方法" class="headerlink" title="Master Theorem 主方法"></a>Master Theorem 主方法</h2><p>Master Theorem是为了计算<strong><font color="#dd0000">含有递归调用</strong>的<strong>分治</strong>算法</font>的时间复杂度的。因为算法中如果含有递归调用算法的情况下<strong>使用归纳证明的方法</strong>，计算时间复杂度是相当困难的，因此需要利用<strong>已有的定理Master Theorem</strong>来帮我们计算复杂情况下的算法的时间复杂度</p><h3 id="定理的定义"><a href="#定理的定义" class="headerlink" title="定理的定义"></a>定理的定义</h3><p><img src="https://pic1.zhimg.com/80/v2-d05e32c23bdbbd70ef3dea46eb47db58_1440w.jpg" alt="img"></p><p>Master Theorem的一般形式是T(n) = a T(n / b) + f(n)， a &gt;= 1, b &gt; 1。递归项f(n)理解为一个高度为log_b n 的a叉树， 这样总时间频次为 (a ^ log_b n) - 1， 右边的f(n)假设为 nc 那么我们对比一下这两项就会发现 T(n)的复杂度主要取决于 log_b a 与 f(n) 的大小。（log_b a表示以b为底，a的对数）因此我总结了使用Master Theorem的三种case的简单判断：</p><p>1.计算 <img src="https://www.zhihu.com/equation?tex=log_b+a" alt="[公式]"> 的值，比较 n^(log_b a)与f(n)的大小。</p><p>2.若n^(log_b a)&gt;f(n),时间复杂度为O(n^(log_b a)) （case 1）</p><p>3.若n^(log_b a)&lt;f(n),时间复杂度为O(f(n)) （case 3）</p><p>4.若n^(log_b a)=f(n),时间复杂度为O(n^(log_b a)*(log n)^k+1) （case 2） (其中k值为f(n)中如果有log对数项时，该对数项的指数为k，例如，如果f(n)=log n ，k=1；f(n)=n,k=0)</p><p>可能公式理解起来有点困难，举几个例子来加深理解：</p><p><img src="https://pic4.zhimg.com/80/v2-40b032e934608df08aabbbd79d1b18cb_1440w.jpg" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><p>常规套路，就是<strong>比较n^(log_b a)与f(n)</strong>的值了，比较出来就可以套用上述公式</p></li><li><p>但是一定要注意公式中的限制条件，如a必须为常数项等。</p></li></ul><p>Reference:</p><p>《数据结构与算法之美》</p><p><a href="http://people.csail.mit.edu/thies/6.046-web/master.pdf" target="_blank" rel="noopener">The Master theorem</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在实现算法的时候，通常会从两方面考虑算法的复杂度，即时间复杂度和空间复杂度。&lt;/p&gt;
&lt;p&gt;顾名思义，时间复杂度用于度量算法的计算工作量；空间复杂度用于度量算法占用的内存空间。&lt;/p&gt;
&lt;p&gt;由于算法测试结果非常&lt;strong&gt;依赖测试环境，且受数据规模的影响，&lt;/stro
      
    
    </summary>
    
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
