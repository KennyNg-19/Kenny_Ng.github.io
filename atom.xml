<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>池中之物</title>
  
  <subtitle>By Kenny Ng</subtitle>
  <link href="/Kenny_Ng.github.io/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-11-26T21:29:14.156Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Kenny Ng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Jupyter NoteBook notes</title>
    <link href="http://yoursite.com/2020/10/09/jupyterBook-notes/"/>
    <id>http://yoursite.com/2020/10/09/jupyterBook-notes/</id>
    <published>2020-10-09T09:04:35.000Z</published>
    <updated>2020-11-26T21:29:14.156Z</updated>
    
    <content type="html"><![CDATA[<p>说个可能很多人不知道的点(反正我是真不知道)</p><blockquote><p>Jupyter 其中 <strong>Ju</strong> 指的是 Julia、<strong>py</strong> 代表 Python 而 <strong>r</strong> 则是 R</p></blockquote><p>该命名说明了 Jupyter希望海纳百川、作为多种<strong>科学计算语言</strong>的共同开发平台的决心</p><h1 id="kernel-一切的发动机"><a href="#kernel-一切的发动机" class="headerlink" title="kernel: 一切的发动机"></a>kernel: 一切的发动机</h1><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjiyi5idf1j30vi08240k.jpg" alt="3 default types" style="zoom:50%;" /><p>Think of the kernel as <strong>another machine</strong> within your computer that understands Python</p><p><strong>默认</strong>是IPython kernel</p><h1 id="Widget-用于制作出动态效果"><a href="#Widget-用于制作出动态效果" class="headerlink" title="Widget: 用于制作出动态效果"></a>Widget: 用于制作出动态效果</h1><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjiyhpsnltj30ui0g8jvl.jpg" style="zoom:30%;" /><blockquote><p>虽然多数笔记本输出都是<strong>静态图形或表格</strong>，</p><p>但资料科学团队时常<strong>有更多的应用场景是需要即时</strong>（real-time）地看到参数调校对模型输出所产生的影响，在这个著力点上，Jupyter 善用了身为网页应用程序的优势，设计出<strong>具备网页前端 Widgets 的表单功能</strong>，让资料科学团队可以<strong>在笔记本中添加互动式介面</strong>，让实验、试误、模拟或学习可以在 Jupyter Notebook 中实现参数化（parameterization）</p></blockquote><h1 id="import-os"><a href="#import-os" class="headerlink" title="import os"></a>import os</h1><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjiysospqfj31fm0cuwh9.jpg" style="zoom:50%;" /><h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说个可能很多人不知道的点(反正我是真不知道)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jupyter 其中 &lt;strong&gt;Ju&lt;/strong&gt; 指的是 Julia、&lt;strong&gt;py&lt;/strong&gt; 代表 Python 而 &lt;strong&gt;r&lt;/strong&gt; 则是
      
    
    </summary>
    
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="积累" scheme="http://yoursite.com/tags/%E7%A7%AF%E7%B4%AF/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai出品NLP的Course1-NLP的Classification &amp; Vector Spaces</title>
    <link href="http://yoursite.com/2020/08/08/NLP-DeepLearning-ai/"/>
    <id>http://yoursite.com/2020/08/08/NLP-DeepLearning-ai/</id>
    <published>2020-08-08T12:45:24.000Z</published>
    <updated>2020-11-26T21:39:54.477Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Course1-NLP-with-Classification-and-Vector-Spaces"><a href="#Course1-NLP-with-Classification-and-Vector-Spaces" class="headerlink" title="Course1: NLP with Classification and Vector Spaces"></a>Course1: NLP with Classification and Vector Spaces</h2><h2 id="1-Task-text-classification"><a href="#1-Task-text-classification" class="headerlink" title="1. Task: text classification"></a>1. Task: text classification</h2><h3 id="法1-Logsitc回归模型"><a href="#法1-Logsitc回归模型" class="headerlink" title="法1: Logsitc回归模型"></a>法1: Logsitc回归模型</h3><h3 id="法2-纯靠词频的Naive-Bayes模型"><a href="#法2-纯靠词频的Naive-Bayes模型" class="headerlink" title="法2: 纯靠词频的Naive Bayes模型"></a>法2: 纯靠词频的Naive Bayes模型</h3><blockquote><p>Naive Bayes is an example of <strong>supervised machine learnin</strong>g, and shares <strong>many similarities with the logistic regression</strong> method </p></blockquote><h4 id="Why-Naive-单纯地靠词频-→-概率"><a href="#Why-Naive-单纯地靠词频-→-概率" class="headerlink" title="Why Naive? 单纯地靠词频 → 概率"></a><font color="#dd0000">Why Naive? 单纯地靠词频 → 概率</font></h4><p>this method makes the assumption that <strong>the features(比如  句中的词前后是有关系的，或者说有某些词总是常见伴随出现的，这些相关性会影响词频) you’re using for classification are <u>all independent</u></strong>, which in reality is <strong>rarely the case</strong>.</p><h5 id="不足-Naive-的2个理想前提"><a href="#不足-Naive-的2个理想前提" class="headerlink" title="(不足)Naive 的2个理想前提"></a>(不足)Naive 的2个理想前提</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmww5z78ej30z607kq58.jpg" alt="2个assumption" style="zoom:33%;" /><h6 id="1-忽视本身多个词的关联性"><a href="#1-忽视本身多个词的关联性" class="headerlink" title="1. 忽视本身多个词的关联性"></a>1. 忽视本身多个词的关联性</h6><p>Some words <strong>①often appear together</strong> and/or they might also <strong>②be related to the thing they’re always describing</strong>. </p><p>These words in a sentence are not always necessarily independent of one another, but <strong>Naive Bayes assumes that they are</strong>. This could lead you to <strong><font color="#dd0000">under or over estimates the conditional probabilities of individual word</font>——一般把常常一起出现的词，当做互相独立，更多是<u>underestimate</u>它们的条件概率</strong>. Naive Bayes <strong>might assign equal probability to all words</strong> even though <strong>from the context</strong> you can see that one of them <strong>is the most likely candidate</strong>. </p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmx4bziloj31a207qjxg.jpg" style="zoom:50%;" /><h6 id="2-各个类比例不均匀分布的原始数据集-distribution-of-the-training-data-sets"><a href="#2-各个类比例不均匀分布的原始数据集-distribution-of-the-training-data-sets" class="headerlink" title="2. 各个类比例不均匀分布的原始数据集 distribution of the training data sets"></a>2. 各个类比例不均匀分布的原始数据集 distribution of the training data sets</h6><p>A good data set will <strong>contain the same proportion of positive and negative tweets</strong> as <strong><u>a random sample would</u></strong>. Most of the available annotated corpora are <u><strong>artificially</strong></u> balanced just like the data set you’ll use for the assignment. </p><p>Howver, in the real tweet stream, positive tweet is sent to occur <strong>more often</strong> than their negative counterparts. One reason for this is that negative tweets might contain content that is banned by the platform or muted by the user such as inappropriate or offensive vocabulary. Assuming that reality behaves as your training corpus, this could <strong>also result in a very optimistic or very pessimistic model——一样影响了prevalence p(pos) p(neg)，进而影响模型</strong></p><h5 id="优势-简单快捷"><a href="#优势-简单快捷" class="headerlink" title="优势: 简单快捷"></a>优势: 简单快捷</h5><p>但Naive Bayes依然可以用于<strong>简单的分类问题</strong></p><p>It takes a <strong>short time</strong> to train and also has a short prediction time.</p><hr><p>先说结果：</p><h4 id="Training-pipeline-5步"><a href="#Training-pipeline-5步" class="headerlink" title="Training pipeline 5步"></a>Training pipeline 5步</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlr7qj6mxj30yg0u04hx.jpg" alt="朴素贝叶斯pipeline" style="zoom:43%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlr8wg82vj311i0di46n.jpg" alt = "training过程总结" style="zoom:30%;" /><h4 id="学习步骤-本质是naive的-频率-→-概率"><a href="#学习步骤-本质是naive的-频率-→-概率" class="headerlink" title="学习步骤: 本质是naive的 频率 → 概率"></a>学习步骤: 本质是naive的 频率 → 概率</h4><p>Step1 同逻辑斯特回归，计算词频</p><p>Step2 根据词频，计算在各类中的<strong><u>条件概率</u></strong> </p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlpxj3b6wj31eo0n6n93.jpg" alt="根据词频计算条件概率" style="zoom:33%;" /><p>Step3 <strong>同一个</strong>词的正负类<strong>比例相除</strong>，再<strong>各个词的相乘</strong>——最后得到<strong>偏向类</strong>，<strong>以1位界限</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlq1b1z5qj31gk0ia14g.jpg"  style="zoom:40%;" /><p>比例为1，则为neutral；大于1，则偏向正类；……</p><h5 id="改进1-Laplacian-Smoothing-平滑处理-避免出现概率为0"><a href="#改进1-Laplacian-Smoothing-平滑处理-避免出现概率为0" class="headerlink" title="改进1: Laplacian Smoothing 平滑处理-避免出现概率为0"></a>改进1: Laplacian Smoothing 平滑处理-避免出现概率为0</h5><h6 id="背景-为什么要做平滑处理"><a href="#背景-为什么要做平滑处理" class="headerlink" title="背景:为什么要做平滑处理?"></a>背景:为什么要做平滑处理?</h6><p>　　<strong>零概率问题</strong>：在计算实例的概率时，如果某个量x，<strong>只是因为<u>在观察样本库（训练集）中</u>没有出现过，它的频率为0，会导致整个实例的概率结果是0</strong>。</p><p>具体的如，在文本分类的问题中，当一个词语没有在训练样本中出现，该词语调概率为0，使用连乘计算文本出现概率时也为0。</p><p>这是不合理的，<strong>不能因为一个事件，<u>在当前数据集中</u>没有观察到，就武断的认为该事件的概率就是0</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlq010yc8j30le0hcwl8.jpg" alt="当词频(概率)为0" style="zoom:33%;" /><p>回到NLP，因为naive公式，要各个词在各个类的条件概率的比值，相乘——但如果出现<strong>频率为0</strong>的词，一样会导致乘法结果无意义</p><p><strong><a href="https://dcpnonstop.github.io/2017/11/24/%E5%B9%B3%E6%BB%91%E5%A4%84%E7%90%86-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%EF%BC%88laplace%EF%BC%89/" target="_blank" rel="noopener">Laplacian Smoothing</a>(拉布拉斯平滑处理，又称加1平滑)</strong>相当于加了个bias，让<strong>概率转化为接近0的数 而避免了0</strong>。这种转化对于结果影响自然很小</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlovcgbq2j30x608odhw.jpg" style="zoom:33%;" /><h6 id="Smoothing公式"><a href="#Smoothing公式" class="headerlink" title="Smoothing公式"></a>Smoothing公式</h6><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlowop7gij31gu0jy7e8.jpg" style="zoom:33%;" /><h5 id="Naive-Bayes-Inference"><a href="#Naive-Bayes-Inference" class="headerlink" title="Naive Bayes Inference"></a>Naive Bayes <u>Inference</u></h5><h6 id="ratio定义"><a href="#ratio定义" class="headerlink" title="ratio定义"></a>ratio定义</h6><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlp0hfq3zj31fi0n0aoy.jpg" style="zoom:33%;" /><p>ratio的别名：<strong>likelihood</strong></p><h6 id="题外话：Prior-ratio先验分布-——有用，尤其当数据集是unbalanced的"><a href="#题外话：Prior-ratio先验分布-——有用，尤其当数据集是unbalanced的" class="headerlink" title="题外话：Prior ratio先验分布 ——有用，尤其当数据集是unbalanced的"></a>题外话：Prior ratio先验分布 ——有用，尤其当数据集是unbalanced的</h6><p><font color="#dd0000">Why prior ?</font></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlt1seunwj31oo0ce0vk.jpg" style="zoom:53%;" /><p>如果数据本身unbalanced，则Naive Bayes公式前面必须多乘一项，先验分布的比例！</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlpcysq7tj30pe0bsgpp.jpg" alt="所以正规朴素贝叶斯形式应该如下" style="zoom:40%;" /><p>课程内的样本数据集是理想的，均匀分布：I haven’t mentioned it till now because in this small example, we have <strong>exactly the same number of</strong> positive and negative tweets, making the ratio one. In this week’s assignments, you’ll have a balanced datasets, so you’ll be working with a ratio of one. </p><blockquote><p>In the future though, when you’re building your own application, remember that <strong>this term becomes important for unbalanced datasets.</strong> </p></blockquote><h5 id="改进2-log-likelihood-概率太小数了，取对数方便计算"><a href="#改进2-log-likelihood-概率太小数了，取对数方便计算" class="headerlink" title="改进2: log likelihood-概率太小数了，取对数方便计算"></a>改进2: log likelihood-概率太小数了，取对数方便计算</h5><blockquote><p>Carrying out <strong>small number multiplications</strong> runs the risk of <strong>numerical underflow</strong> when the number returned is so small it can’t be stored on the device</p></blockquote><p>累程 变 <strong>累加</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlpi83j76j31gs0dejy7.jpg" alt="用log-likelihood ratio后的朴素贝叶斯公式" style="zoom:33%;" /><p>我们将log-likelihood ratio，新定义为<strong>λ</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlpns2a6mj31h80g2gyg.jpg" alt="0就是界限，为neutral" style="zoom:33%;" /><p>然后再对<strong>各个λ求和</strong>，如：大于0，则该句子情感为pos类…</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlpqjrj4nj31hm0eigw7.jpg" style="zoom:33%;" /><p>好处1: <strong>重新定义分类界线——从1改成0</strong></p><p>即neg类概率更大时 ratio可以为负，负数更直观</p><p>好处2: 原始ratio的区间<u>长度并不对称</u>，neg类只能<font color="#dd0000">取值[0,1)，neg sentiment程度不明显</font>！用了log就是<u><strong>长度完全对称的区间</strong></u>！</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlptv6i4ij31ca0je46t.jpg" alt="区间的改变" style="zoom:33%;" /><h4 id="Testing-用于predict"><a href="#Testing-用于predict" class="headerlink" title="Testing: 用于predict"></a>Testing: 用于predict</h4><h5 id="如果测试时，出现模型之前没见到过词，就当neutral！"><a href="#如果测试时，出现模型之前没见到过词，就当neutral！" class="headerlink" title="如果测试时，出现模型之前没见到过词，就当neutral！"></a>如果测试时，出现模型之前没见到过词，就当neutral！</h5><blockquote><p>The values that don’t show up in the table <strong>are considered neutral</strong> and don’t contribute anything to this score. The <strong>ML model can only give a score for words it’s seen before.</strong></p></blockquote><p>别忘了用了log累加时，如果数据不平很 最后得加上prior的log！</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlrtq4ujyj31h00msh1e.jpg" alt="interview这个词没学过，则为neutral" style="zoom:33%;" /><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlryf207cj317g0myk0s.jpg" style="zoom:33%;" /><h4 id="Naive-Bayes的延伸分类应用"><a href="#Naive-Bayes的延伸分类应用" class="headerlink" title="Naive Bayes的延伸分类应用"></a>Naive Bayes的延伸分类应用</h4><p>上面是bayes公式，上下相除<strong>抵消P(tweet)</strong>, 可得下面的公式</p><p>下面是之前bayes训练的公式(本身就有 <strong>P(w/pos)的累乘 = P(tweet/pos)</strong>)</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmvic7kvmj30rk0ec0yt.jpg" alt="该分类模型延伸应用的公式推导" style="zoom:53%;" /><p>下面左边，就是<strong>模型预测的结果(ratio)</strong>——<font color="#dd0000">这种形式，可以延伸应用到<strong>其他2分类领域</strong></font></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmvje5q7nj312c0hi0ym.jpg" alt="延伸领域" style="zoom:35%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmwqw0z0ej313k0k6aq1.jpg" alt="消除二分歧义" style="zoom:33%;" /><h4 id="单靠词频的Naive-Bayes的潜在error"><a href="#单靠词频的Naive-Bayes的潜在error" class="headerlink" title="单靠词频的Naive Bayes的潜在error"></a>单靠<u>词频</u>的Naive Bayes的潜在error</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmxep6f9cj30ui0buacj.jpg" alt="image-20200811155634232" style="zoom:50%;" /><h5 id="1-semantic-meaning-lost-in-the-pre-processing-step"><a href="#1-semantic-meaning-lost-in-the-pre-processing-step" class="headerlink" title="1. semantic meaning lost in the pre-processing step"></a>1. <u>semantic meaning lost</u> in the pre-processing step</h5><p>教训：还是务必查看原始句子语义，而不是单纯的移除标点符号和stop words</p><h5 id="移除特殊标点-构成了表情，决定了情感"><a href="#移除特殊标点-构成了表情，决定了情感" class="headerlink" title="移除特殊标点(构成了表情，决定了情感)"></a>移除特殊标点(构成了表情，决定了情感)</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmxj90vh5j30r80a6adu.jpg" alt="表情符号不可移除！" style="zoom:33%;" /><p>The sad face punctuation in this case is very <strong>important to the sentiment</strong>  because it tells you what’s happening. </p><p>But if you’re removing punctuation, then the processed tweet will leave behind only  beloved grandmother, which looks like a very positive tweet. </p><h5 id="移除stop-words"><a href="#移除stop-words" class="headerlink" title="移除stop words"></a>移除stop words</h5><p>not这二元180°转义词，<strong>也是stop word！</strong></p> <img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmxn45612j31ea0860z7.jpg" style="zoom:33%;" /><p>If you remove neutral words like not and this, what you’re left with is the [Good, attitude, close, nice]</p><p>From this set of words, any classifier will infer that this is something very positive.</p><h5 id="2-word-order-affects-the-meaning-of-a-sentence"><a href="#2-word-order-affects-the-meaning-of-a-sentence" class="headerlink" title="2. word order affects the meaning of a sentence"></a>2. word order affects the meaning of a sentence</h5><p>word order自然是语义的一部分</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmxor6jelj312k0ciwmh.jpg" style="zoom:33%;" /><h5 id="3-quirks-怪癖-of-languages-come-naturally-to-humans-but-confuse-models"><a href="#3-quirks-怪癖-of-languages-come-naturally-to-humans-but-confuse-models" class="headerlink" title="3. quirks(怪癖) of languages come naturally to humans but confuse models."></a>3. quirks(怪癖) of languages come naturally to humans but <u>confuse</u> models.</h5><p>Quirk： 人类语言中的 带有sarcasm irony讽刺、euphemism委婉等色彩，adversarial(<strong>恰恰反义、敌对</strong>)性质词</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmxqfa2p7j31fo0he4ab.jpg" alt="这种反义，属于adversarial" style="zoom:33%;" /><hr><h2 id="2-Word-Vector-和-Vector-Space入门"><a href="#2-Word-Vector-和-Vector-Space入门" class="headerlink" title="2. Word Vector 和 Vector Space入门"></a>2. Word Vector 和 Vector Space入门</h2><p>词向量，向量空间模型</p><h3 id="Why-vector-space-形式和应用"><a href="#Why-vector-space-形式和应用" class="headerlink" title="Why vector space? 形式和应用"></a>Why vector space? 形式和应用</h3><h4 id="形式"><a href="#形式" class="headerlink" title="形式"></a>形式</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmylqhg13j30m805edhp.jpg" style="zoom:43%;" /><h4 id="应用-底层"><a href="#应用-底层" class="headerlink" title="应用(底层)"></a>应用(底层)</h4><p>Vector space models will </p><ol><li>验证句子语义的相似性：identify whether the first pair of questions or the second pair <strong>are similar in meaning</strong> even if they do not share the same word 即<strong>identify similarity</strong> for a question answering, paraphrasing, and summarization.</li></ol><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmyakwvu0j319a0j6gw0.jpg"  style="zoom:33%;" /><ol start="2"><li><font color="#dd0000"><em>发掘语言中，词之间*</em>关联性<strong></font>: to **<font color="#dd0000">capture dependencies between words</font></strong>——应用很广！！！</li></ol><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghmyi6ab4oj31e40iuk6o.jpg" alt="这个可以引用到很多领域，常见如下" style="zoom:45%;" /><h3 id="共现矩阵"><a href="#共现矩阵" class="headerlink" title="共现矩阵"></a>共现矩阵</h3><h4 id="word-by-word的matrix"><a href="#word-by-word的matrix" class="headerlink" title="word by word的matrix"></a>word by word的matrix</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmz1fgbtgj31ea0f8n6b.jpg" alt="若词出现在k距离内次数越频繁，则这组词越相关" style="zoom:33%;" /><h4 id="word-by-doc的"><a href="#word-by-doc的" class="headerlink" title="word by doc的"></a>word by doc的</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnzcssgqnj313s0kgdqr.jpg" alt="在doc库的各种类，出现的频率" style="zoom:33%;" /><h4 id="引出Vector-Space"><a href="#引出Vector-Space" class="headerlink" title="引出Vector Space"></a>引出Vector Space</h4><p>这些<strong><font color="#dd0000">有了维度</font>的</strong>数据，就可以<font color="#dd0000"><strong>放入vector space</strong>， 进行相似度分析</font></p><p>然后可以看成N-维向量(N为词的数量)，通过比较<strong>向量的”相似性“指标，如距离</strong>——得出句子/语料的相似性**</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnzi90xuuj31ei0ligyp.jpg" style="zoom:30%;" /><h5 id="vector-space的应用：挖掘word-analogies"><a href="#vector-space的应用：挖掘word-analogies" class="headerlink" title="vector space的应用：挖掘word analogies"></a>vector space的应用：挖掘word analogies</h5><p><strong>infer unknown relations</strong> among words</p><h6 id="如通过词已知的关系，推导出，词之间相似但未知的关系"><a href="#如通过词已知的关系，推导出，词之间相似但未知的关系" class="headerlink" title="如通过词已知的关系，推导出，词之间相似但未知的关系"></a>如通过词已知的关系，推导出，<u>词之间相似但未知的关系</u></h6><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho0me9qq5j30zy0jgwn9.jpg" alt="推测未知的首都" style="zoom:33%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho0nh4gpxj31hk0j6ajs.jpg" alt="用向量减法；再在[10,4]处找，相似指标最大的向量" style="zoom:33%;" /><h5 id="指标1：Euclidean-distance-欧几里得距离❌"><a href="#指标1：Euclidean-distance-欧几里得距离❌" class="headerlink" title="指标1：Euclidean distance 欧几里得距离❌"></a>指标1：Euclidean distance 欧几里得距离❌</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnzmivjmhj31ck0kgn7q.jpg" style="zoom:40%;" /><p>Generalize到更高维的</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnzny5azlj313m06ygnh.jpg" alt="该公式也是向量的norm，向量/矩阵的长度或大小" style="zoom:40%;" /><h5 id="指标2：cosine-similarity✅"><a href="#指标2：cosine-similarity✅" class="headerlink" title="指标2：cosine similarity✅"></a>指标2：cosine similarity✅</h5><p>欧几里得距离的缺点：明显<strong>会受到corpus size的影响</strong>——<strong>导致vector size</strong>长短不一，影响判断</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnzy5kjw3j31g20nqwsh.jpg" alt="food和农业关联更大，但因food语料库太小，欧几里得误判history关系更大" style="zoom:50%;" /><p><strong>cosine similarity</strong></p><p>即向量的<strong>点积</strong>公式:   多考虑了<strong>语料的大小，即向量长度</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho04e04nij30n80b8wgg.jpg" alt="点积" style="zoom:33%;" /><p>这和向量的”相似度“<strong>有何关联</strong>？</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho07dveoxj31fe0hwn4x.jpg" alt="0-90°的夹角，也对相似度有影响" style="zoom:33%;" /><h4 id="word-embeddings-词嵌入-向量"><a href="#word-embeddings-词嵌入-向量" class="headerlink" title="word embeddings 词嵌入(向量)"></a><font color="#dd0000">word embeddings 词嵌入</font>(向量)</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghp2rgex4zj31p005mwh8.jpg" alt="word represents by vector" style="zoom:53%;" /><p>….</p><hr><h4 id="PCA-高纬数据的降维"><a href="#PCA-高纬数据的降维" class="headerlink" title="PCA: 高纬数据的降维"></a>PCA: 高纬数据的降维</h4><p>a <strong>statistical technique</strong></p><h5 id="目的-方便可视化"><a href="#目的-方便可视化" class="headerlink" title="目的: 方便可视化"></a>目的: 方便可视化</h5><p>降维到2D方便可视化，来找关系</p><p><strong>Word embeaddings</strong> end up having <strong>vectors in very, very high dimensions</strong>.</p><p>PCA：a way to <strong>reduce the dimension of these vectors to two dimensions so you can plot it on an X-Y axis, 2D plot.</strong> </p><p>helpful for <u><strong>visualizing</strong></u> your data to check if your representation is <strong><u>capturing relationships</u> among words</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghp7blx33uj31po092adg.jpg" alt=""></p><h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho4f0u4nrj31hu0j2drh.jpg" style="zoom:33%;" /><p>需要：向量的<strong>特征值和特征向量</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho6suirinj317u074goy.jpg" style="zoom:33%;" /><p>为什么要不相关的feature? 因为<strong>自然语言文本总是上下文相关的</strong>，所以feature之间总有一定相关性，</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho6ula7l9j31io0ne12y.jpg" alt="Step1 生成uncorrelated features" style="zoom:33%;" /><p>注：PCA works better <strong>if the data is <u>centered</u></strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho6veh9o8j31cu0guwo4.jpg" alt="Step2 通过和特征向量的点积，求出压缩后的向量" style="zoom:33%;" /><hr><h2 id="3-词向量Task-机器翻译和相似doc搜索"><a href="#3-词向量Task-机器翻译和相似doc搜索" class="headerlink" title="3. 词向量Task: 机器翻译和相似doc搜索"></a>3. 词向量Task: 机器翻译和相似doc搜索</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghp8c3q9suj31bw0h2n5y.jpg" alt="相关知识" style="zoom:50%;" /><h3 id="Task-翻译"><a href="#Task-翻译" class="headerlink" title="Task: 翻译"></a>Task: 翻译</h3><p><a href="https://kennyng-19.github.io/Kenny_Ng.github.io/2020/01/29/NLP-intro/#1-case-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91" target="_blank" rel="noopener">一种实现：维特比算法</a></p><p>另一种实现：<strong>已知两种语言的word embeddings</strong>，通过<strong>寻找<u>transform matrix</u></strong>，找到转换后 <strong>目标语言中最相似的word vector</strong>——我们称该过程为<strong>“align word vectors”</strong></p><h4 id="Step1-Transfor-vector-by-matrix"><a href="#Step1-Transfor-vector-by-matrix" class="headerlink" title="Step1: Transfor vector by matrix"></a>Step1: Transfor vector by <u>matrix</u></h4><p>find <strong>a transformation matrix</strong> from English to French vector space embeddings. </p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghp9x50omjj31e40hadnt.jpg" alt ="align word vectors" style="zoom:33%;" /><p>Such a transformation matrix is <strong>a matrix that <font color="#dd0000">rotates and scales vector spaces</font></strong>——回忆《<strong>线性代数的本质</strong>》</p><h5 id="然后怎么计算该矩阵呢？还是优化问题"><a href="#然后怎么计算该矩阵呢？还是优化问题" class="headerlink" title="然后怎么计算该矩阵呢？还是优化问题"></a><strong>然后怎么计算该矩阵呢？还是优化问题</strong></h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghpa0ho7c8j30ya0gk78h.jpg" alt="还是梯度下降，这不过这次是矩阵的" style="zoom:50%;" /><h5 id="补充notation-Frobenius范数-即矩阵元素的平方和的开方"><a href="#补充notation-Frobenius范数-即矩阵元素的平方和的开方" class="headerlink" title="补充notation-Frobenius范数: 即矩阵元素的平方和的开方"></a>补充notation-Frobenius范数: 即矩阵元素的平方和的开方</h5><p>F范数是<strong>针对矩阵而言</strong>的，具体定义可以<strong>类比向量的L2范数</strong></p><img src="https://pic3.zhimg.com/80/ded8e501d6d54eaf3d218491423380df_1440w.jpg?source=1940ef5c" alt="F范数" style="zoom:80%;" /><p>当然在ML中，为了简化计算(因为范数只是用于<strong><u>最优化</u></strong>)，可以<strong>不开方而是保留平方</strong></p><p>所以保留开方后，Loss函数最终形式为：</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghpa22ro8jj30z00b876y.jpg" alt="Loss函数最终形式" style="zoom:33%;" /><p>拓展，<strong>3 main vector transformations的几何意义</strong></p><p>(更多，请回忆《<strong>线性代数的本质</strong>》)</p><ul><li>Scaling</li><li>Translatio</li><li>Rotation</li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghp9qtx6x8j313f0u0dr9.jpg" alt="关于施加rotation矩阵的结论" style="zoom:50%;" /><h5 id="Step-2-寻找最相似的几个翻译结果by-KNN"><a href="#Step-2-寻找最相似的几个翻译结果by-KNN" class="headerlink" title="Step 2: 寻找最相似的几个翻译结果by KNN"></a>Step 2: 寻找最相似的<u>几个</u>翻译结果by <u>KNN</u></h5><p>因为word embedding空间不一定有，和matrix转换的<strong>结果数值一模一样的词向量</strong>，且存在<u>近义词</u>——所以一般是会输出<strong>几个最近似</strong>的词向量，供选择。这里会用到KNN算法</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghpa5okk64j314g0fm0zw.jpg" style="zoom:30%;" /><h5 id="改进版-faster-approximate-KNN"><a href="#改进版-faster-approximate-KNN" class="headerlink" title="改进版 faster approximate KNN"></a>改进版 faster <u>approximate</u> KNN</h5><h5 id="启发思路：空间划分"><a href="#启发思路：空间划分" class="headerlink" title="启发思路：空间划分"></a>启发思路：空间划分</h5><p>(下图只是<strong>简单的2D空间</strong>) <strong>slice the space <u>into regions</u></strong>: you could <strong>search just <u>within</u> those regions</strong>. When you think about organizing subsets of a dataset efficiently, you may think about placing your data <strong>into <u>buckets</u></strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqbl9ocp8j31220jywwe.jpg" style="zoom:33%;" /><p>If you think about buckets, then you’ll definitely want to think about <font color="#dd0000"><strong><u>hash tables</u></strong></font>.</p><h6 id="提升KNN，处理高维数据的效率：Locality-Sensitive-Hashing"><a href="#提升KNN，处理高维数据的效率：Locality-Sensitive-Hashing" class="headerlink" title="提升KNN，处理高维数据的效率：Locality Sensitive Hashing"></a>提升KNN，处理<u>高维数据</u>的效率：Locality Sensitive Hashing</h6><p>本质是 a <strong>hash function</strong>, to be locality sensitive; an <strong><a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" target="_blank" rel="noopener">algorithmic technique</a></strong> that hashes <strong>similar input items into the same “buckets” with <u>high probability</u></strong>——所以说是一种<strong>近似法, “approximate”</strong></p><p>白话定义：把vector根据在<strong>vector space中的距离足够近</strong>的<strong>分到一起</strong>，的Hashing方法</p><blockquote><p>Locality is another word for location, sensitive is another word for caring </p><p>This is kNN in simple terms: You have a labelled dataset and now you are trying to label a new data point. Find the k nearest data points from your labelled dataset to the new point. The majority vote among the k nearest neighbors is the label of the new point. Add the new point and it’s label to your dataset</p><p>One of the <strong>biggest problems with kNN 处理高维数据时</strong> is that <strong>常规的暴力法下，for each new data point, you have to calculate its distance from all existing points in your dataset.</strong> The LSH technique, differing from <a href="https://en.wikipedia.org/wiki/Hash_function" target="_blank" rel="noopener">conventional hashing techniques</a> in that hash collisions are maximized, not minimized,  can be seen as a way to <strong>reduce the dimensionality of high-dimensional data</strong>; high-dimensional input items can be <strong>reduced to low-dimensional versions while preserving <u>relative distances</u> between items</strong>. And this problem is what LSH is <strong>trying to solve</strong>.</p></blockquote><p>So locality sensitive hashing is a <strong>hashing method</strong> that’s <strong>cares very deeply about assigning items based on where they’re <u>located in vector space</u></strong>.</p><h6 id="核心：Multiplanes-hash-functions"><a href="#核心：Multiplanes-hash-functions" class="headerlink" title="核心：Multiplanes hash functions"></a>核心：<u><strong>Multiplanes hash functions</strong></u></h6><blockquote><p>In order to divide your vector space <strong>into <u>manageable</u> regions</strong>, you’ll want to use <strong><u>more than one plane</u></strong>. Based on the idea of <strong>numbering every single region</strong> that is <strong>formed by the <u>intersection of n planes</u>.</strong></p></blockquote><p><a href="https://kennyng-19.github.io/Kenny_Ng.github.io/2020/07/25/ML-concepts-memorize/#6-ml理论中常见的超平面概念" target="_blank" rel="noopener">思路</a>：每一个plane，实际就是定义一个<strong>法向量</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghrkcqi8smj313602kjsl.jpg" style="zoom:33%;" /><p>几何上，you have multiple planes and it helps us to divide the vector space into smaller sub regions. But you <strong>want to have a single hash value</strong> to know <strong>which bucket to assign the vectoring</strong>. You do this by <strong>combining the signals from all the planes</strong> into a single hash value.</p><p>那么定义一组plane就等于一组法向量 output value is a <strong>combination of the side of the plane</strong> where the vector is localized with respect to the collection of planes.</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghrkdjszw6j30zu0lygq0.jpg" style="zoom:33%;" /><p>Locality Sensitive Hashing<strong>最终计算公式</strong>：看sign定boolean值h，再用2的幂次求和公式</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghrkhg1by3j312a0k0k0i.jpg" style="zoom:40%;" /><h6 id="注意-因为是随机生成的法向量-plane，请重复多次得到更合理的结果-make-sets-of-random-planes"><a href="#注意-因为是随机生成的法向量-plane，请重复多次得到更合理的结果-make-sets-of-random-planes" class="headerlink" title="注意: 因为是随机生成的法向量-plane，请重复多次得到更合理的结果 make sets of random planes"></a>注意: 因为是随机生成的法向量-plane，请重复多次得到更合理的结果 make sets of <strong>random planes</strong></h6><p>You will make <u>multiple</u> sets of <strong><u>random planes</u></strong> in order to make the approximate nearest neighbors <strong>more accurate.</strong></p><h3 id="Task-相似doc搜索"><a href="#Task-相似doc搜索" class="headerlink" title="Task: 相似doc搜索"></a>Task: 相似doc搜索</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghp89bzp7ej31e00bgq9h.jpg" style="zoom:33%;" /><p>同理用fast KNN</p><p>虽然doc的表示和word的vector表示不完全相同，但doc也是word组成——可以用word embedding中存在的word vector值的<strong>累加</strong>，表示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqhe1yls3j30zy0my7ai.jpg" alt="累加word vector的embedding值" style="zoom:33%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqhihbgq4j310k0gw4br.jpg" alt="拆分doc成word, 用embedding中存在的word vector累加" style="zoom:33%;" /><p>有了所有doc的vector，剩下的寻找<strong>近似目标</strong>，就和上述KNN过程一样了…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Course1-NLP-with-Classification-and-Vector-Spaces&quot;&gt;&lt;a href=&quot;#Course1-NLP-with-Classification-and-Vector-Spaces&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai5月新课-AI4Medicince笔记</title>
    <link href="http://yoursite.com/2020/07/25/AI4m-DeepLearning-ai/"/>
    <id>http://yoursite.com/2020/07/25/AI4m-DeepLearning-ai/</id>
    <published>2020-07-25T09:26:33.000Z</published>
    <updated>2020-08-26T04:52:57.170Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Course1-AI-for-Medical-Diagnosis-诊断"><a href="#1-Course1-AI-for-Medical-Diagnosis-诊断" class="headerlink" title="1. Course1-AI for Medical Diagnosis(诊断)"></a>1. Course1-AI for Medical Diagnosis(诊断)</h1><h2 id="I-ML进阶，可能遇到的更实际的进阶问题：3个"><a href="#I-ML进阶，可能遇到的更实际的进阶问题：3个" class="headerlink" title="I. ML进阶，可能遇到的更实际的进阶问题：3个"></a>I. ML进阶，可能遇到的<strong><u>更实际</u>的</strong><u>进阶</u>问题：3个</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4delyjewj31520f2dmd.jpg" style="zoom:50%;" /><h3 id="a-训练数据分布-Class-imbalance问题"><a href="#a-训练数据分布-Class-imbalance问题" class="headerlink" title="a. 训练数据分布-Class imbalance问题"></a>a. 训练数据分布-Class imbalance问题</h3><p>如 实际在收集要拿來做DL的医学数据時，常识是 <strong>正常的非病患的数据比不正常的、有疾病的多上很多</strong></p><p>loss function用的是: <strong>Binary cross-entropy loss</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh3d7i1s7tj30x606cwg8.jpg" style="zoom:33%;" /><p>model输出是：P(不正常影像), 对于<strong>不正常影像(y=1)的预测</strong>！(不是看label预测哦，测试数据压根就没有label)</p><h4 id="导致问题：impact-on-Loss-calculation"><a href="#导致问题：impact-on-Loss-calculation" class="headerlink" title="导致问题：impact on Loss calculation"></a>导致问题：impact on <strong>Loss calculation</strong></h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh3daki4igj31g60oo7ge.jpg" alt="假设初始的classifier预测P均为0.5" style="zoom:30%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh3dbxlo62j30rs07k0wq.jpg" alt="loss占比越大的，就是优化的bias" style="zoom:33%;" /><p>但这会和目的「让模型往更准确预测<strong>不正常</strong>图片的方向发展」相悖：the algorithm is optimizing  its updates to <strong>get the normal examples,</strong>  and not giving much relative weight to the mass examples. If learning with a highly unbalanced dataset, as we are seeing here, then the algorithm will be <strong>incentivized(刺激) to prioritize the <u>majority class</u></strong>, since it contributes more to the loss.</p><h4 id="解决："><a href="#解决：" class="headerlink" title="解决："></a>解决：</h4><h5 id="解1-给loss加上权重：weighted-loss"><a href="#解1-给loss加上权重：weighted-loss" class="headerlink" title="解1 给loss加上权重：weighted loss"></a>解1 给loss加上权重：weighted loss</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh3dk2fxymj30y20hgdky.jpg" alt="weight就是类数量比的反向" style="zoom:33%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh3djqzy7aj315a0ju480.jpg" alt="可以让loss的贡献接近" style="zoom:33%;" /><h5 id="解2-人工重新采样Resampling恢复balance-有缺点"><a href="#解2-人工重新采样Resampling恢复balance-有缺点" class="headerlink" title="解2  人工重新采样Resampling恢复balance(有缺点)"></a>解2  人工重新采样Resampling恢复balance(有缺点)</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh3dvgvajhj31eq0mawt7.jpg" style="zoom:33%;" /><p>原本的training set有6張正常的影像，和2張不正常的影像，以<strong>人工的方式</strong>將正常的影像<strong>去掉2張之後</strong>+再把2張不正常的影像<strong>重复</strong>放入變成4張。</p><p>这样，正常的影像和不正常的影像都分別有四張，便可以解決先前的Class Imbalance 問題。但是這個解決方式會有明显的缺点：原本该有6種不同的正常影像，变成了4種——也就是說會有<strong>部分数据 沒有参与這次训练</strong>+而有(<strong>重复数据参与了训练</strong>)</p><h3 id="b-Multi-task问题-让模型一次性做多重的分类任务"><a href="#b-Multi-task问题-让模型一次性做多重的分类任务" class="headerlink" title="b. Multi-task问题: 让模型一次性做多重的分类任务"></a>b. Multi-task问题: 让模型一次性做<u>多重的</u>分类任务</h3><p>在一开始的例子当中，我们只以有肿块mass和没有砖块来做区分，但是现实生活中的医学影像却是不只有两种情况，就如胸腔Ｘ光来说，可能会发现有许多<strong>其他种类</strong>的疾病，例如：肿块、肺炎、肺积水…等。</p><p>当我们想要<strong><u>同时</u></strong>检测的疾病<strong>种类不只一个</strong>的时候这样就是<strong>属于Multi-task的问题</strong></p><h4 id="解决-只用一个模型。好处？"><a href="#解决-只用一个模型。好处？" class="headerlink" title="解决: 只用一个模型。好处？"></a>解决: 只用一个模型。好处？</h4><p>如何建立<strong>一个</strong>具有处理Multi-task(多重任务)问题的模型？只用一个模型的<strong>好处</strong>？can <strong>learn features</strong> that are <strong>common</strong> to identifying more than one disease,  allowing us to <strong>use our existing data more efficiently</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4dkfbai0j31ao0ba7as.jpg" alt="1 model" style="zoom:33%;" /><h4 id="Mult-task和单任务比，的不同"><a href="#Mult-task和单任务比，的不同" class="headerlink" title="Mult-task和单任务比，的不同"></a>Mult-task和单任务比，的不同</h4><p>如图中所示加入我们要在X光胸腔分辨mass、pneumonia和Edema时, 可以看到左边所显示的是<strong>每一张图片的desired label</strong>+右边是机器给的prediction probability值，这时候计算多重任务的Loss时他只要像右上角说显示的一样<strong>将各别的Loss值相加</strong>就可以。这样所计算出来的加总过后的Loss就会是训练更新模型的Loss。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4diz8x92j30xc0gawjg.jpg" alt="训练examples和loss形式的对应改变" style="zoom:43%;" /><p>data分布，也可能出class imbalance问题：像这样一次处理许多疾病，在收集资料的时候也有可能出现比重不均的情况——所以在多重任务的情况下也可以使用解决方法的加上权重。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4dt34bvlj30vo09wgnp.jpg" alt="权重w，按各自病的数据分开算哦！" style="zoom:50%;" /><h3 id="c-Dataset-Size问题：数据量"><a href="#c-Dataset-Size问题：数据量" class="headerlink" title="c. Dataset Size问题：数据量"></a>c. Dataset Size问题：数据量</h3><p>CNN，尤其是<strong>规模更大的</strong>CNN，是需要大量的labeled data做训练才会有比较好的效果。通常在医学影像的领域当中，在处理影像时只会包含10,000到100,000个不同的样本，像这样的资料量相对在其他领域例如分辨猫和狗的影像来说是非常少的，就是工程师们可能会遇到的Data Size的问题。</p><h4 id="2种解决的方式"><a href="#2种解决的方式" class="headerlink" title="2种解决的方式"></a>2种解决的方式</h4><h5 id="1-Transfer-Learning，使用pre-training。"><a href="#1-Transfer-Learning，使用pre-training。" class="headerlink" title="1. Transfer Learning，使用pre-training。"></a><strong>1. Transfer Learning，使用pre-training。</strong></h5><p>下面这张图在pretraining的部分，这一个模型先被拿去做分辨企鹅的训练等到分辨企鹅的训练完成之后再转移到第二个步骤Fine-tuning，这时候工程师可以视情况决定需不需要升级这个训练模型，最后的目标是要让这一个模型可以分辨胸腔X光并且输出是哪一个疾病。</p><p>为什么用企鹅当作例子？</p><p>讲师在这边有提到因为企鹅整体的形状很像肺形状，所以在初期几层的神经网路当中可以去让模型分辨一些比较大的，例如边缘形状等等的问题，在第二次Fine -tuning的时候这个模型就会被修改并且能够分辨比较细节的部分。其实像这样的方式就是所谓的Transfer Learning。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4ezmo6yvj30x80ey0y2.jpg" style="zoom:50%;" /><h6 id="题外话：而当数据充足时，你可以："><a href="#题外话：而当数据充足时，你可以：" class="headerlink" title="题外话：而当数据充足时，你可以："></a><strong>题外话</strong>：而当<strong>数据充足</strong>时，你可以：</h6><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4h5xu7cmj31i40u0gst.jpg" alt="从头开始:新训练模型" style="zoom:33%;" /><h5 id="2-Data-Augmentation数据增强"><a href="#2-Data-Augmentation数据增强" class="headerlink" title="2. Data Augmentation数据增强"></a><strong>2. Data Augmentation数据增强</strong></h5><p>如果喂给机器训练的影像数量太小的话，还有一种方法可以让影像变多！那就是以人工的方式将影像进行一些变化后再重新把变化过后的影像喂回机器做训练，这个就是Data Augmentation。</p><p>但是这边要注意一点像是胸腔X光来说，我们可以旋转图片重新喂给模型或是改变对比度去产生一张新的影像，然而如果我们将胸腔X光做<strong>180度的翻转</strong>时，这样机器就无法分辨他是正常的胸腔或是dextrocardia（右位心）——也就是说这种情况下，<strong>label的状态改变</strong>了，由正常的影像变成右位心的影像。</p><p>所以结论就是，当我们想要利用这种方式去增加影像的数量时，我们必须<strong>考虑到这一张影像的label状态会不会改变</strong>。</p><img src="https://miro.medium.com/max/1390/1*PstRJ3s2Nw3mQEG3c5joug.png" alt="数据增强不是任意的，是有规范的" style="zoom:43%;" /><hr><h2 id="II-再聊训练验证和测试集的进阶问题"><a href="#II-再聊训练验证和测试集的进阶问题" class="headerlink" title="II. 再聊训练验证和测试集的进阶问题"></a>II. 再聊训练验证和测试集的<u>进阶</u>问题</h2><img src="https://miro.medium.com/max/2279/1*GBsxeGzhvdr-iVTGc-dZaw.png" alt="训练、验证、测试的别名" style="zoom:50%;" /><p>一般来说机器学习会将我们所拥有的数据分做三组<br>1.Training set训练集 2.Validation set验证集 3.Test set测试集</p><p>分别有<strong>别名</strong>:</p><p>development set, tuning set 和 holdout set</p><h2 id="Dataset可能存在的进阶问题"><a href="#Dataset可能存在的进阶问题" class="headerlink" title="Dataset可能存在的进阶问题"></a>Dataset可能存在的进阶问题</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4gt6lqzwj319g0bwtc2.jpg" style="zoom:50%;" /><h3 id="Q1-Data-leakage-数据-因果关系-的泄露"><a href="#Q1-Data-leakage-数据-因果关系-的泄露" class="headerlink" title="Q1: Data leakage 数据(因果关系)的泄露"></a>Q1: Data leakage 数据(因果关系)的泄露</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>Data Leakage 跟其他场合说的<strong>数据安全数据泄漏</strong>, <strong>完全不一样</strong>: 不是数据量因为泄露少了，<strong>而是因果关系的提前泄漏</strong>。是由于<strong>前期</strong>的<strong>准备数据/数据采样</strong>的时候出了问题，误将与<strong>结果直接相关或存在颠倒因果关系的feature纳入了数据集</strong>，使模型使用<strong>颠倒的因果关系的数据</strong>进行预测，得到<strong>over-optimistic 过于乐观的</strong>结果。</p><h5 id="具体实例-Patient-Overlap"><a href="#具体实例-Patient-Overlap" class="headerlink" title="具体实例: Patient Overlap"></a>具体实例: Patient <u>Overlap</u></h5><p>如果<strong>重复的病人数据</strong>被分别使用在训练组以及测试组的时候，机器可能会<strong>”记忆“</strong>该位病人的某项特殊特征，误将将这项特殊的特征当作是一个可以判断的依据，这种现象是Over-optimistic test set performance。就有点像是考试前你已经看过考题了一样，机器就会像这样子把答案记下来，并非像是你想让他做的-倚靠其他更有依据的线索找到答案。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6rdqztzuj310u0ng4e2.jpg" style="zoom:33%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6r9x0iyej315g0lw7ca.jpg" alt="不同时期normal的照片加入2个set..." style="zoom:50%;" /><p>例如这张图片所示，是<strong>normal的</strong>病人在不同时期的照片，同时2次都带着项链。机器在test set中将这位病人的影像判定为正常，<strong>可能</strong>是<strong>依据病人穿戴的项链</strong>，而不是依据病人肺部的现象——不要小看DL model的记忆力: it’s possible that the model actually <strong>memorized to output normal when it saw the patient with a necklace on</strong>. This is <strong>not hypothetical</strong>, deep learning models can unintentionally memorize training data, and the model could memorize <strong>rare or unique training data aspects</strong> of the patient,</p><h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>解决方式其实很简单：只要<strong>将同一位病人的数据放在<u>同一个组别</u></strong>即可——这样也是保证学习的目的：通过学习一些病人的特征，可以<strong>泛化到</strong><u>更多的病人</u>身上。</p><p>以往在分数据的时候可能会从按照图像类别直接分——<strong>保证分布的一致性</strong>；但是医学影像的话反其道而行之，以病人作为拆分到不同组的根据。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6rkq9a50j30vh0u0wy5.jpg" alt="split by image VS patients" style="zoom:40%;" /><h3 id="Q2-Set-sampling当数据本身分布不均匀时"><a href="#Q2-Set-sampling当数据本身分布不均匀时" class="headerlink" title="Q2: Set sampling当数据本身分布不均匀时"></a>Q2: Set sampling当数据本身分布不均匀时</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>就举医学影像为例，正常不患病的数量会远大于有病的医学影像数量，即分别问大类和小类。所以在分数据到不同的三个不同的数据集时，很有可能<strong>测试组里面没有分到一张患病的影像</strong>。</p><h4 id="解-同时引出此情况下，各个集的sampling顺序"><a href="#解-同时引出此情况下，各个集的sampling顺序" class="headerlink" title="解(同时引出此情况下，各个集的sampling顺序)"></a>解(同时引出此情况下，<u>各个集的sampling顺序</u>)</h4><p>解决的方式是</p><ol><li><p>在分数据的时候被<strong>设定至少有百分之X的</strong>小类X会<strong>被设成50%</strong></p></li><li><p>在测试组的数据<strong>确认之后</strong>，接下来<strong>要设定的就是验证组的数据</strong>，验证集设定策略<strong>和测试集合的基本上一样</strong></p></li><li><p>当这两组的数据设定完之后<strong>剩下的所有数据</strong>，会被用作是<strong>训练集</strong>。</p></li></ol><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4gx81troj31d20d87bu.jpg" style="zoom:33%;" /><h5 id="即data-sampling顺序-是test→validation→train"><a href="#即data-sampling顺序-是test→validation→train" class="headerlink" title="即data sampling顺序: 是test→validation→train"></a>即data sampling顺序: 是test→validation→train</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4gygxttdj30ba06m0tz.jpg" style="zoom:50%;" /><h3 id="Q3-Ground-Truth-“正确”的label"><a href="#Q3-Ground-Truth-“正确”的label" class="headerlink" title="Q3: Ground Truth(“正确”的label)"></a>Q3: Ground Truth(“正确”的label)</h3><h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><p>在医学里面数据label，也就是学习结果的「正确答案」，在机器学习里面常<strong>被称作Ground truth</strong>， 而在医学上面同样的东西会被称作Reference Standard。</p><p>医学里面会常常<strong>会有没有正确解答的现象</strong>，就举胸腔X光来说，也许某一位放射科医师认为某张影像是肺炎，但同样的影像另外一位放射科医生可能<strong>会有不同的意见</strong>，这个叫做Inter-observer disagreement。</p><h4 id="定义”正确”的方法"><a href="#定义”正确”的方法" class="headerlink" title="定义”正确”的方法"></a>定义”正确”的方法</h4><p>如此一来决定Ground truth的方法也变得很重要，常见的方法有：</p><p><strong>1.Consensus Voting</strong> ✅<br>就以胸腔X光来说，这个方式就是由一组放射科医生可能是<u>投票决定又或者是经由讨论达到某个共识</u>而决定最后的答案。</p><p><strong>2.Additional Medical Testing</strong></p><p>例如就像刚刚举例的胸腔X光，如果当放射科医生无法从胸腔X光得到最后的Ground truth时，这时病人会被建议去做其他的测试，例如CT，得到更精确的解答，验证胸腔X光的Ground truth。除了X光影像之外，例如皮肤癌照片也通常会由组织切片的验证结果才得到该照片的Ground truth。不过这个方法比较费时费力，所以<strong>目前研究大多都是用第一个方式</strong>。</p><hr><h2 id="III-再聊分类问题的evaluation-metrics"><a href="#III-再聊分类问题的evaluation-metrics" class="headerlink" title="III. 再聊分类问题的evaluation metrics"></a>III. 再聊<u>分类问题</u>的evaluation metrics</h2><h3 id="accuracy准确性的计算"><a href="#accuracy准确性的计算" class="headerlink" title="accuracy准确性的计算"></a>accuracy准确性的计算</h3><h4 id="accuracy定义和概率计算，有数学上的一致性"><a href="#accuracy定义和概率计算，有数学上的一致性" class="headerlink" title="accuracy定义和概率计算，有数学上的一致性"></a>accuracy定义和概率计算，有数学上的一致性</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghbb8xauogj319s0jytgv.jpg" alt="概率计算得到acc" style="zoom:43%;" /><p>所以只<strong>要有sensitivity和specificity</strong>，结合统计得到的prevalence，就可以<strong>算出accuracy</strong></p><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6yd5wykaj31ho0kyn70.jpg" alt="Confusion Matrix"></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghb6apedduj30rw07wq4i.jpg" alt="注意这两个：实际是!" style="zoom:50%;" /><h4 id="助记！"><a href="#助记！" class="headerlink" title="助记！"></a>助记！</h4><h5 id="铺垫：什么NP-R，TP-R这种rate怎么算："><a href="#铺垫：什么NP-R，TP-R这种rate怎么算：" class="headerlink" title="铺垫：什么NP R，TP R这种rate怎么算："></a>铺垫：什么NP R，TP R这种rate怎么算：</h5><p>rate就不只是直接拿confusion matrix里的值了，而是<strong><font color="#dd0000">再除以对应行/列的<u>总和</u></font></strong>，即做分母!!</p><p>(以下均为rate)：</p><ul><li><p>灵敏度(Sensitivity, <strong>TPR</strong>)和特异度(Specificity, <strong>TNR</strong>)，作为分母的是“<strong>实际</strong>有病/没病的总数“，所以可以看到是(TP，FN)和(TN, FP)这种<font color="#dd0000"><strong>两两相反，四个字母完全不同</strong></font>的搭配！！</p></li><li><p>查准率PPV(Precision)和NPV，作为分母的是“<strong>预测</strong>有病/没病的总数“，所以可以看到是(TP，FP)和(TN, FN)这种<font color="#dd0000"><strong>两两预测的pos和neg类相同，而结果的正确性true/false不同</strong></font>的搭配！！</p></li></ul><ul><li>查全率(Recall, Sensitivty) 和查准率(Precision, PPV)<font color="#dd0000">都是关于<strong>postive类</strong>的指标——<strong>找到positve类(比如患病)</strong>一般才是<strong>ML预测的主要对象, 而不是关注negative类</strong></font></li></ul><hr><h3 id="先验-prevalence-做分母：sensitivity灵敏度-即查全率-和specificity特异度"><a href="#先验-prevalence-做分母：sensitivity灵敏度-即查全率-和specificity特异度" class="headerlink" title="先验(prevalence)做分母：sensitivity灵敏度(即查全率)和specificity特异度"></a>先验(prevalence)做分母：sensitivity灵敏度(即查全率)和specificity特异度</h3><blockquote><p>Sensitivity <strong>only</strong> considers output on people <strong>in the positive class</strong></p><p>Similarly, specificity <strong>only</strong> considers output on people <strong>in the negative class</strong>.</p></blockquote><ul><li><p><strong>灵敏度/查全率(recall) P(+|disease)</strong>：model预测为pos且实际为pos/所有实际为pos(TP+<strong>FN</strong>)，的比例</p><p>理解<u><strong>查全</strong></u>：<strong>在实际pos的样本中，model预测为正且预测正确</strong>的比例——<strong>非漏诊性！越高，说明放过<u>更少的</u>患病者</strong></p><p>计算公式为：TPR=TP/ (TP+ FN)</p></li></ul><ul><li><p><strong>特异度P(—|non-disease)，P(—|neg)</strong> 简称<strong>TNR，TN <u>Rate</u></strong>：model预测为neg且实际为neg/所有实际为neg，的比例——<strong>分母是<font color="#dd0000">non-disease，诊断没病的查全率</font>，非误诊性！越高，说明<u>将越多的非病人正确排除</u></strong></p><p>计算公式为：TNR= TN / (FP + TN)</p></li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghba7wvrgaj30xf0u04qp.jpg" alt="灵敏度和特异度的解释" style="zoom:43%;" /><h5 id="结论：敏感度高-漏诊率低，查全率高；特异度高-误诊率低"><a href="#结论：敏感度高-漏诊率低，查全率高；特异度高-误诊率低" class="headerlink" title="结论：敏感度高=漏诊率低，查全率高；特异度高=误诊率低"></a>结论：敏感度高=漏诊率低，查全率高；特异度高=误诊率低</h5><p>注：(P(+|disease)<u>分子的+</u>,是<strong>model</strong>预测的结果！！！)</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6v7xrtswj314w0hswkh.jpg" alt="注意: 先验概率+是model判定为正的！！" style="zoom:33%;" /><h5 id="练习题：坑"><a href="#练习题：坑" class="headerlink" title="练习题：坑"></a>练习题：坑</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghbd7v75vtj317y0tw7cp.jpg" style="zoom:40%;" /><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>理想情况下我们<strong>希望敏感度和特异度都很高</strong>，然而实际上一般在敏感度和特异度中<strong>寻找一个平衡点</strong>，这个过程可以<strong>用ROC(Receiver Operating Characteristic)曲线</strong>来表示, 和 <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve" target="_blank" rel="noopener">AUC</a>值(Area Under the Curve) 来精确表示：</p><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><ul><li>纵坐标：<strong>RPR，sensitivity</strong></li><li>横坐标：<strong>FPR = 1 — specificity(TNR)</strong>——<strong>误诊率</strong></li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghbg9nfon6j30f80fi42l.jpg" style="zoom:50%;" /><h4 id="为什么横坐标用误诊率？1-特异性？"><a href="#为什么横坐标用误诊率？1-特异性？" class="headerlink" title="为什么横坐标用误诊率？1 - 特异性？"></a>为什么横坐标用误诊率？1 - 特异性？</h4><p>为了满足：<strong>Sensitivity、Specificity这两个指标越大</strong>的情况</p><p>理想情况下，<strong>TPR应该接近1，FPR应该接近0</strong>, 即TPR=1，FPR=0，即<strong>图中(0,1)点。故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好</strong>，<strong>Sensitivity、Specificity越大</strong>效果越好。</p><h4 id="深度理解ROC"><a href="#深度理解ROC" class="headerlink" title="深度理解ROC"></a>深度理解ROC</h4><p>ROC曲线的<strong>横坐标和纵坐标其实是<font color="#dd0000">没有相关性的</font></strong>，所以<font color="#dd0000"><strong>不能把ROC曲线当做一个函数曲线来分析，应该把ROC曲线看成无数个点</strong></font>，每个点都代表一个分类器，其横纵坐标表征了<font color="#dd0000">这个<strong>分类器的性能</strong></font>。为了更好的理解ROC曲线，我们先引入ROC空间</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghbfjctbu0j30n80nawhi.jpg" alt="ROC space" style="zoom:50%;" /><p>明显的，C’的性能最好。而B的准确率只有0.5，几乎是随机分类。特别的，图中左上角坐标为（1,0）的点为<strong>完美分类点（perfect classification），它代表所有的分类全部正确</strong>，即模型<strong>预测为1的点全部正确（TPR=1），归为0的点没有错误（FPR=0）</strong>。</p><p>通过ROC空间，我们明白了一条ROC曲线其实<strong>代表了无数个分类器</strong>：那么我们为什么常常用一条ROC曲线来<strong>描述一个分类器</strong>呢？</p><p>仔细观察ROC曲线，发现其都是上升的曲线（斜率大于0），且都通过点（0,0）和点（1,1）。其实，这些点是一个个的分类器，而每个分类器实际<strong>习得的<font color="#dd0000">也是一个最佳阈值</font></strong>。所以ROC,可以代表着<strong>一个</strong>分类器<strong>在不同阈值下</strong>的分类效果，即曲线从左往右可以<strong>认为是阈值</strong>变化过程——但是<strong><font color="#dd0000">不一定是从0到1的，也可能是反过来——得看具体场景</font></strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghbcutbqclj30z50u0awm.jpg" alt="完美划分时, 无论阈值是什么总恰有一个坐标=1" style="zoom:43%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghbczdv3vej30es0fawgl.jpg" alt="完美划分时的ROC" style="zoom:33%;" /><h4 id="AUC值"><a href="#AUC值" class="headerlink" title="AUC值"></a><strong>AUC</strong>值</h4><p>那么AUC值的含义是什么呢？</p><blockquote><p>The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example.</p></blockquote><p>这句话有些绕，我尝试解释一下：首先<strong>AUC值是一个<u>概率值</u></strong>——随机挑选一个正样本以及一个负样本，当前的分类算法，根据计算得到的Score值并<strong>将这个正样本排在负样本前面的概率</strong>就是AUC值。当然，<strong>AUC值越大</strong>，当前的分类算法<strong>越有可能</strong>将正样本排在负样本前面，即能够更好的分类。</p><p>从AUC判断分类器（预测模型）优劣的标准：</p><ul><li>AUC = 1，是完美分类器，采用这个预测模型时，<strong><font color="#dd0000">存在至少一个</font>阈值能得出完美预测</strong>(绝大多数预测的场合，不存在完美分类器)</li><li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li><li>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li><li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li></ul><p>三种AUC值示例：</p><p><img src="https://pic3.zhimg.com/80/v2-f9d1bf42ddcaaab151464e1d2e9f1d30_1440w.jpg" alt="img"></p><p>简单说：<strong>AUC值越大的分类器，正确率越高</strong></p><h4 id="为什么使用ROC曲线"><a href="#为什么使用ROC曲线" class="headerlink" title="为什么使用ROC曲线"></a>为什么使用ROC曲线</h4><p>既然已经这么多评价标准，为什么还要使用ROC和AUC呢？</p><p>因为ROC曲线有个<strong>很好的特性</strong>：当测试集中的<strong>正负样本的分布变化的时候，ROC曲线能够保持不变</strong>——在实际的数据集中经常会出现类<strong>不平衡（class imbalance）</strong>现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化</p><hr><h3 id="后验-预测结果-做分母：PPV查准率-又Precision-、NPV"><a href="#后验-预测结果-做分母：PPV查准率-又Precision-、NPV" class="headerlink" title="后验(预测结果)做分母：PPV查准率(又Precision)、NPV"></a>后验(预测结果)做分母：PPV查准率(又Precision)、NPV</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6vcxavpoj30v80awq6f.jpg" alt="PPV" style="zoom:33%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6vdcaqw1j30vq0amdjl.jpg" alt="NPV" style="zoom:33%;" /><hr><h3 id="PRC曲线-precision-recall"><a href="#PRC曲线-precision-recall" class="headerlink" title="PRC曲线(precision-recall)"></a>PRC曲线(precision-recall)</h3><h4 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h4><p>和ROC曲线用于权衡灵敏度和特异度的作用类似，理想情况下我们<strong>希望precision和recall都高”</strong>， “实际上一般在敏感度和特异度中<strong>寻找一个平衡点</strong>，</p><p>ROC shows the trade-off between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision,</p><h4 id="F1-score-量化PRC"><a href="#F1-score-量化PRC" class="headerlink" title="F1-score(量化PRC)"></a>F1-score(量化PRC)</h4><p>同理，类似AUC值的精确作用，F1 score: harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.</p><h1 id="2-Course2-AI-for-Medical-Prognosis-预断-预后"><a href="#2-Course2-AI-for-Medical-Prognosis-预断-预后" class="headerlink" title="2. Course2-AI for Medical Prognosis(预断,预后)"></a>2. Course2-AI for Medical Prognosis(预断,预后)</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Course1-AI-for-Medical-Diagnosis-诊断&quot;&gt;&lt;a href=&quot;#1-Course1-AI-for-Medical-Diagnosis-诊断&quot; class=&quot;headerlink&quot; title=&quot;1. Course1-AI for 
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="advancedML" scheme="http://yoursite.com/tags/advancedML/"/>
    
  </entry>
  
  <entry>
    <title>ML,DL的混淆点/易错点</title>
    <link href="http://yoursite.com/2020/07/25/ML-concepts-memorize/"/>
    <id>http://yoursite.com/2020/07/25/ML-concepts-memorize/</id>
    <published>2020-07-25T05:00:22.000Z</published>
    <updated>2020-10-20T06:33:46.426Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ML-数学-中的常见混淆-易错点和一般性结论"><a href="#ML-数学-中的常见混淆-易错点和一般性结论" class="headerlink" title="ML(数学)中的常见混淆, 易错点和一般性结论"></a>ML(数学)中的常见混淆, 易错点和一般性结论</h1><h2 id="0-似然？"><a href="#0-似然？" class="headerlink" title="0. 似然？"></a>0. 似然？</h2><p>似然和概率在统计学中是经常见到的两个术语，有时候这两个概念是一个意思，有时候却有很大区别。这里梳理下这两个术语所代表的具体含义。</p><h4 id="本文中数学符号及含义"><a href="#本文中数学符号及含义" class="headerlink" title="本文中数学符号及含义"></a>本文中数学符号及含义</h4><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>O</td><td>观测值</td></tr><tr><td>θ</td><td>随机过程中的参数</td></tr><tr><td><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifp45h7myj303c02q745.jpg" style="zoom:25%;" /></td><td>参数的估计</td></tr><tr><td>P(O|θ)</td><td>概率</td></tr><tr><td>L(θ|O)</td><td><strong>似然函数</strong></td></tr></tbody></table><h4 id="wiki中关于“似然”和“概率”的解释"><a href="#wiki中关于“似然”和“概率”的解释" class="headerlink" title="wiki中关于“似然”和“概率”的解释"></a>wiki中关于“似然”和“概率”的解释</h4><ul><li>在频率推论中，似然函数（常常简称为似然）是一个在<strong>给定了数据</strong>后<strong>关于模型参数的</strong>函数。<strong>在非正式情况下，“似然”通常被用作“概率”的同义词。</strong></li><li>在<strong><u>数理统计</u></strong>中，两个术语<strong><u>则有不同的意思</u></strong>：<ul><li>“概率”描述了<strong>给定模型参数后</strong>，输出<strong>结果的合理性(可能性大小)</strong>，而不涉及任何观察到的数据。</li><li>“似然”则描述了<strong>给定了特定观测结果</strong>后，描述模型<strong>参数是否合理</strong></li></ul></li></ul><h4 id="似然函数公式"><a href="#似然函数公式" class="headerlink" title="似然函数公式"></a>似然函数公式</h4><p>先看似然函数的定义，它是给定联合样本值x下关于(未知)参数θ的函数：(等号应该是<strong><u>正比于</u></strong>)<img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29" alt=""></p><ul><li><p>这里的小x 指联合样本随机变量X取到的值，即X=x；</p></li><li><p>这里的θ是指未知参数，属于参数空间；</p></li><li><p>这里的f(x|θ)是一个<strong>概率密度函数</strong>，特别地表示(给定)θ下，关于联合样本值x的<strong>联合密度函数</strong>。</p></li></ul><p>所以从定义上，似然函数和密度函数是完全不同的两个<strong>数学对象</strong>：前者是关于θ的函数，后者是关x的函数。所以这里的等号=，应理解为<strong><font color="#dd0000">函数值形式</font>的相等</strong>，而<font color="#dd0000">不是两个函数本身是同一函数</font>。所以这个式子的<strong>严格书写方式</strong>是<img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%3B+%5Ctheta%29" alt="">)(分号示把参数隔开) 即<strong>θ在右端只当作参数</strong>。</p><h4 id="概率角度看ML-频率派-vs-贝叶斯派"><a href="#概率角度看ML-频率派-vs-贝叶斯派" class="headerlink" title="概率角度看ML: 频率派 vs 贝叶斯派"></a>概率角度看ML: 频率派 vs 贝叶斯派</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjo3vb3pqgj31450u016u.jpg" alt="概率角度分析两派的机器学习" style="zoom:50%;" /><p><strong>频率派，统计机器学习: MLE 极大似然估计</strong>：L(θ) best θ = f(X|θ), <strong>f为θ fit数据的概率函数</strong>！<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjo3zx4hv4j30e20agwie.jpg" alt="MLE 极大后验估计" style="zoom:50%;"/></p><p>given data, 寻找<strong>fit程度最大的概率分布/θ</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjie9ys7s0j31qk0tekhj.jpg" alt="最大似然估计" style="zoom:30%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjie8c3kjpj315m0bedi7.jpg" alt="在寻找最优分布能fit/产生这些given data" style="zoom:33%;" /><p><strong>贝叶斯派机器学习</strong>：<strong>MAP，极大后验估计</strong>，<strong>对概率公式积分——求概率值</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjo41bewn3j30to0lw4ef.jpg" alt="MAP 考虑了prior的极大似然估计" style="zoom:40%;" /><h4 id="对比本质-“似然”和“概率”是站在两个角度看待问题"><a href="#对比本质-“似然”和“概率”是站在两个角度看待问题" class="headerlink" title="对比本质: “似然”和“概率”是站在两个角度看待问题"></a>对比本质: “似然”和“概率”是站在<u>两个角度</u>看待问题</h4><h5 id="例如"><a href="#例如" class="headerlink" title="例如"></a>例如</h5><ul><li>概率</li></ul><blockquote><p>抛一枚均匀的硬币，拋20次，问15次拋得正面的可能性有多大？ 这里的可能性就是”概率”，均匀的硬币就是给定参数θ=0.5，“拋20次15次正面”是观测值O。求概率P(H=15|θ=0.5)=？的概率。</p></blockquote><ul><li><strong>“似然”描述了给定了特定观测值后，描述<u>模型参数</u>是否合理。</strong></li></ul><blockquote><p>拋一枚硬币，拋20次，结果15次正面向上，问其为均匀的可能性？ 这里的可能性就是”似然”</p><p>“拋20次15次正面”为观测值O为已知，参数θ并不知道，求L(θ|H=15)=P(H=15|θ=0.5)(prior和evidence都是常数) 的<strong>最大化下的θ 值</strong></p></blockquote><p>例B. 对于统计<strong>概率分布</strong>的似然</p><h4 id="注意：计算题中的似然"><a href="#注意：计算题中的似然" class="headerlink" title="注意：计算题中的似然"></a>注意：计算题中的似然</h4><p>贝叶斯公式中，求似然概率值时——就是<u>分子本身的概率公式/分布函数</u>，而已经given的分母, 与其无关！</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gji415ep10j30fa03udhm.jpg" alt="若有分母一定时，直觉上可得的分子概率，那就是似然值！"></p><h4 id="离散随机变量的“似然”与“概率”"><a href="#离散随机变量的“似然”与“概率”" class="headerlink" title="离散随机变量的“似然”与“概率”"></a>离散随机变量的“似然”与“概率”</h4><h4 id="连续型随机变量的“似然”与“概率”"><a href="#连续型随机变量的“似然”与“概率”" class="headerlink" title="连续型随机变量的“似然”与“概率”"></a>连续型随机变量的“似然”与“概率”</h4><h2 id="1-均值化？归一化？傻傻分不清"><a href="#1-均值化？归一化？傻傻分不清" class="headerlink" title="1. 均值化？归一化？傻傻分不清"></a>1. 均值化？归一化？傻傻分不清</h2><p>“标准化”和”归一化”这两个中文词要指代<strong>四种</strong>Feature scaling(特征缩放)方法, 实质是<font color="#dd0000"><strong>一种线性变换</strong>: <strong>对向量 X 按照比例α压缩, 再进行平移</strong></font>。线性变换有很多良好的性质，这些性质<strong>决定了对数据改变后<u>不会造成“失效”，反而能提高数据的表现</u></strong>，这些性质是归一化/标准化的前提, 详情见<a href="https://www.zhihu.com/question/20455227/answer/370658612" target="_blank" rel="noopener">特征工程中的「归一化」有什么作用？- 知乎</a> 。</p><p>先说<strong><u>数学领域</u>的normalization <u>标准化</u></strong>: 泛指将一组数，<strong><u>除以它们的sum</u></strong>, <strong>得到各自占比且总和为1</strong>——常见的就是求<strong>概率值</strong>或<strong>概率分布</strong></p><p>再到了<strong><u>ML方面处理feature</u></strong>时，具体这四种常用的分别是：</p><ol><li><p><strong>归一化</strong> Rescaling (<strong>min-max normalization</strong>)  ，<img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-min%28x%29%7D%7Bmax%28x%29-min%28x%29%7D" alt="[归一化]"> </p></li><li><p><strong>均值归一化</strong> mean normalization  <img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-mean%28x%29%7D%7Bmax%28x%29-min%28x%29%7D+" alt="[均值归一化]"> </p><p>先说<strong>x - mean</strong>：这叫<strong>centralize中心化</strong>。因为 sum = mean * N, 而处理是每一项减去mean就是总共减去了mean * N，这样可以保证<strong>处理后的sum为0</strong>！</p></li></ol><p>   归一化性质总结：把数据变成(0,1)或者（1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1<strong>范围</strong>之内处理，更加便捷快速</p><p>   目的关键词：归一化的缩放，顾名思义，”归一”——<strong>“拍扁”压缩到<font color="#dd0000">区间</font>（仅由极值决定）</strong></p><ol start="3"><li><p><strong>标准化 Standardization</strong>(<strong><u>Z-score</u> normalization</strong>)  <img src="https://www.zhihu.com/equation?tex=+x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-mean%28x%29%7D%7B%5Csigma%7D" alt="[标准化]"> </p><p>标准化总结：<strong>能伸能缩</strong>，当数据较为集中时， α更小，于是数据在标准化后就会<strong>更加分散</strong>。如果数据本身分布很广，那么 α 较大，数据就会被<strong>更加集中</strong>，到更小的范围内。</p><p>目的关键词：标准化的缩放<strong>是更加“弹性”和“动态”的能伸能缩，和<font color="#dd0000">整体样本的分布</font>有很大的关系，每个点都在贡献缩放，通过方差（variance）体现出来</strong>。</p><p>补充说明：</p><ul><li><p>在Batch Normalization(BN)出现之前是很有必要的，因为这样<strong>拉到均值附近，学习的时候更加容易</strong>，毕竟<strong>激活函数是以均值为中心的，学习到这个位置才能将不同的类分开</strong>。</p></li><li><p>但是BN出现之后，这个操作就完全没必要了。因为每次卷积后都有BN操作，BN就是把数据拉到0均值1方差的分布，而且这个均值和方差是动态统计的，不是只在原始输入上统计，因此更加准确。</p></li></ul></li><li><p>中心化：x’ = x - μ</p><p>平均值为0，对标准差无要求</p></li><li><p>Scaling to unit length  <img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx%7D%7B%7C%7Cx%7C%7C%7D" alt=""> </p></li></ol><p>什么时候用归一化？什么时候用标准化？</p><ul><li><p>归一化： 输出范围在0-1之间</p></li><li><p>标准化：输出范围是<strong>负无穷到正无穷</strong>，灵活</p><p> （1）如果对输出结果<strong><u>范围</u>有要求</strong>，用归一化。<br>           如果数据较为稳定，<strong>不存在极端的最大最小值</strong>，用归一化。<br> （2）如果数据存在<strong>异常值和较多噪音</strong>，用<strong>标准化</strong>，可以间接通过中心化避免异常值和极端值的影响。</p></li></ul><p><strong>一般来说，我个人建议<u><font color="#dd0000">优先使用标准化</font></u>。</strong>在对特征形式有要求时再尝试别的方法，如归一化或者更加复杂的方法。很多方法都可以将输出调整到0-1，如果我们对于数据的分布有假设的话，更加有效方法是使用相对应的概率密度函数来转换</p><h2 id="2-理清ML-flow各个部分的关系"><a href="#2-理清ML-flow各个部分的关系" class="headerlink" title="2. 理清ML flow各个部分的关系"></a>2. 理清ML flow各个部分的关系</h2><p>从上到下是 应用层→数据底层：</p><ol><li><p>最优模型(hypothesis)，作为ML flow的最红产物，本质是为<strong>实际业务</strong>服务的——最终用于输入实际数据，输出需要的结果</p></li><li><p>cost/error func是为<strong>寻找最优模型</strong>服务的，作为metric, 找出<strong>最优参数</strong>——是最优化算法去<strong>优化</strong>的对象，(输出是错误metric, 仅用于评估), 但实际为模型服务的是是取到极值的最优参数——正则化项也在其中(既然是为了调参)</p></li><li><p>学习/优化算法，是为<strong>cost func</strong>服务的——<strong>通过数学方法让metric快速取到极值</strong></p></li></ol><h2 id="3-训练的目的是输出最佳参数"><a href="#3-训练的目的是输出最佳参数" class="headerlink" title="3. 训练的目的是输出最佳参数"></a>3. 训练的目的是输出最佳参数</h2><p>用训练数据的训练过程，根本目的是<strong>输出最佳的参数(即带着该参数的hypothesis)，而非<u>最低的错误率——错误率只是判定的指标</u></strong>。所以相应的<strong>简化训练过程/提高效率的技巧</strong>，如PCA降维，只能用在训练集(但因为用了PCA，同一个数据降维的mapping, Ureduce 也需要对验证/测试集，使用！)</p><h2 id="4-无监督学习的所有输入数据都是unlabeled"><a href="#4-无监督学习的所有输入数据都是unlabeled" class="headerlink" title="4. 无监督学习的所有输入数据都是unlabeled?"></a>4. 无监督学习的所有输入数据<u>都是unlabeled</u>?</h2><p>无监督学习，只是说 模型<strong>上线后，用于实际业务</strong>时，输入数据是unlabeled——在模型<strong>训练/验证/测试</strong>过程中，<strong>也是要求</strong>最优参数的(如，异常检测的异常阈值)，当然<strong>可以使用labeled data</strong></p><h2 id="5-数据增强器对于不同数据集"><a href="#5-数据增强器对于不同数据集" class="headerlink" title="5. 数据增强器对于不同数据集"></a>5. 数据增强<u>器</u>对于<u>不同数据集</u></h2><p>已知给training set构造了ImageDataGenerator,  我们应该构造a <u><strong>separate</strong></u> generator for <strong>valid and test sets</strong></p><h3 id="Why用构造不同的generator"><a href="#Why用构造不同的generator" class="headerlink" title="Why用构造不同的generator?"></a>Why用构造不同的generator?</h3><p><strong>Why can’t we use the same generator as for the training data?</strong></p><p>LOk back at the generator we wrote for the training data.</p><ul><li>It normalizes each image <strong>per batch</strong>, meaning that it uses <strong>batch statistics</strong>.</li><li>We should not do this with the test and validation data, since in a real life scenario we <strong>don’t process incoming images <u>a batch</u> at a time</strong> (we process <strong><u>one image</u> at a time</strong>).</li><li>Knowing the average per batch of test data would effectively give our model an advantage.<ul><li>The model should <strong><font color="#dd0000">not have any information about the test data</font></strong>.</li></ul></li></ul><h3 id="但又该如何构造val-test-set的数据增强generator呢？"><a href="#但又该如何构造val-test-set的数据增强generator呢？" class="headerlink" title="但又该如何构造val/test set的数据增强generator呢？"></a>但又该如何构造val/test set的数据增强generator呢？</h3><p>What we need to do is <strong>normalize</strong> incoming test data using the <font color="#dd0000">statistics <strong>computed from the training set</strong></font>.</p><p>There is one technical note. </p><ul><li><p>Ideally, we would want to compute our <strong>sample mean and standard deviation</strong> using the <strong>entire training set</strong>. However, since this is <strong>extremely large,</strong> that would be very time consuming.</p></li><li><p>In the interest of time, we’ll <strong><font color="#dd0000">take a random sample of the dataset</font></strong> and do the calcualtion.</p></li></ul><h2 id="6-ml理论中常见的超平面概念"><a href="#6-ml理论中常见的超平面概念" class="headerlink" title="6. ml理论中常见的超平面概念"></a>6. ml理论中常见的超平面概念</h2><p>超平面一般化的广义叫法：</p><p>超平面是【分解平面】的一般化：</p><ul><li><p>在一维的平面中，它是点 </p></li><li><p>在二维的平面中，它是线 </p><ul><li><p><font color="#dd0000"><strong>为什么</strong></font>是平面？因为<strong>normal vector法向量和plane是一一对应的</strong>，等价！而实际正是normal vector在分割</p><p>A plane would be this magenta line into two-dimensional space, and it <strong>actually represents all the possible vectors that would be sitting on that line(之后都用plane).</strong> In other words, they would be parallel to the plane, such as this blue vector or this orange vector. </p><p>You can define a plane with a single vector. This magenta vector is <strong>perpendicular to the plane</strong>, and it’s called the <strong><u>normal vector</u> to that plane</strong>. So normal vector is perpendicular to <strong>any vectors that lie on the plane</strong>. </p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqf03pey5j30ui0ki0vw.jpg" style="zoom:33%;" /><ul><li><p>关于分割：实际就是normal vector在发挥作用（而该vector对应它所垂直的plane！)</p><p>如何在数学上而不是几何视觉上，计算分割的结果we are able to see <strong>visually</strong> when the vector is <strong>on one side of the plane</strong> or the other, but how do you <strong>do this <u>mathematically</u></strong>?  把目标向量和normal vector，<strong>做dot product</strong>，正负号表明在同侧/异侧——实际是<strong>俩向量，夹角的cos</strong>在决定！！</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqf6a99ekj31d60ig7c3.jpg" alt="2D plane，数学上是这样起到“分割”作用的" style="zoom:43%;" /></li></ul></li></ul></li><li><p>在三维的平面中，它是面 </p></li><li><p>…</p></li><li><p>在更高的维度中，我们称之为超平面** <br>所以，<strong>广义上</strong>，1D直线、2D平面，都可叫超平面</p></li></ul><h2 id="7-区分点积-内积-、叉积-外积-、矩阵乘法"><a href="#7-区分点积-内积-、叉积-外积-、矩阵乘法" class="headerlink" title="7. 区分点积(内积)、叉积(外积)、矩阵乘法"></a>7. 区分点积(内积)、叉积(外积)、矩阵乘法</h2><h3 id="a-内积-点积-结果是数-标量"><a href="#a-内积-点积-结果是数-标量" class="headerlink" title="a. 内积(点积): 结果是数(标量)"></a>a. 内积(点积): 结果是数(标量)</h3><p>向量的点乘,也叫向量的内积、数量积，对两个向量执行点乘运算，就是对这两个向量对位①<strong>一一相乘</strong>之后 ②<strong>再求和</strong>的操作，点乘的结果是一个标量。</p><p>对于向量a和向量b，要求一维向量a和向量b的<strong>行列数相同</strong>，点积公式为：</p><p><img src="https://img-blog.csdn.net/20160902214456788" alt=""></p><p>可以看出，计算结果是个标量！</p><h4 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h4><p>in order to compute <strong>angles, lengths and distances</strong> of vectors.</p><h4 id="实现：numpy-dot"><a href="#实现：numpy-dot" class="headerlink" title="实现：numpy.dot"></a>实现：numpy.dot</h4><p>注：虽然数学公式很清楚，但该np.dot要求了2个输入向量的维度，同<strong>矩阵乘法</strong>一样：<strong>第一个v的行数 = 第二个v的列数</strong>。</p><p>如：2个等长的向量v1, v2相乘——写成np.dot(v1, v2<strong>.T</strong>)</p><h4 id="内积-点积的-generalisation"><a href="#内积-点积的-generalisation" class="headerlink" title="内积: 点积的 generalisation"></a>内积: 点积的 generalisation</h4><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>An <strong>inner product</strong> is the more general term of <strong>a function</strong> which can apply to <strong>a wide range of <u>different vector spaces</u></strong>. </p><p>The <strong>dot product</strong> is the name given to the <strong>inner product</strong> on a <u><strong>finite dimensional Euclidean space</strong></u>.</p><p>输出可以是</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvov62sskj31jk04o41p.jpg" alt="比点积输出更多样" style="zoom:30%;" /><h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvoequwcvj30uw0lctn7.jpg" alt="点积公式的3个性质" style="zoom:50%;" /><h4 id="公式-和点积不同"><a href="#公式-和点积不同" class="headerlink" title="公式: 和点积不同"></a>公式: 和点积不同</h4><p>设A为[[a, b], [c, d]]</p><p>&lt;x, y&gt; = <strong>X.T · A · y</strong> = a x1y1 + c x2y1 + b x1y2 + d x2y2</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvojkio4jj30rs0e2afw.jpg" alt="展开公式" style="zoom:50%;" /><h5 id="Positive-definite"><a href="#Positive-definite" class="headerlink" title="Positive definite"></a>Positive definite</h5><p>直接检验 是否<strong>仅当</strong>x=0时，&lt;x,x&gt;=0</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvppvog8kj30yy02emxh.jpg" style="zoom:33%;" /><h5 id="Symmetric-lt-x-y-gt-lt-y-x-gt-看对角"><a href="#Symmetric-lt-x-y-gt-lt-y-x-gt-看对角" class="headerlink" title="Symmetric &lt;x, y&gt; = &lt;y, x&gt; 看对角"></a>Symmetric &lt;x, y&gt; = &lt;y, x&gt; 看对角</h5><p>因为<strong>对角</strong>同号，才能保证x2y1和x1y2是同号的；异号则不行</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvpn215bxj30ke0hsdgv.jpg" alt="异号则飞symmetric" style="zoom:33%;" /><h5 id="Bilinear"><a href="#Bilinear" class="headerlink" title="Bilinear"></a>Bilinear</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvprjtqxnj30fa01gdfx.jpg" alt="image-20201020130252879" style="zoom:50%;" /><h4 id="VS-dot-product-vector-space更复杂，会不一样"><a href="#VS-dot-product-vector-space更复杂，会不一样" class="headerlink" title="VS dot product: vector space更复杂，会不一样"></a>VS dot product: vector space更复杂，会不一样</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvoxw1lonj317a0hytja.jpg" alt="低维度(2D以下)和点积是一样的" style="zoom:30%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvp50nfnpj321o07en9e.jpg" alt="3D以上就可能输出为向量/矩阵了" style="zoom:33%;" /><p>(vector spaces 发生了变化)</p><p>结论：在<strong>复制Tensor的计算</strong>里，务必区分！！</p><h4 id="几何意义"><a href="#几何意义" class="headerlink" title="几何意义"></a><a href="https://blog.csdn.net/dcrmg/article/details/52416832" target="_blank" rel="noopener">几何意义</a></h4><h3 id="b-叉积-外积-类似矩阵乘法"><a href="#b-叉积-外积-类似矩阵乘法" class="headerlink" title="b. 叉积(外积): 类似矩阵乘法"></a>b. 叉积(外积): 类似矩阵乘法</h3><p>两个向量的叉乘，又叫向量积、外积、叉积，叉乘的运算结果<strong>是一个向量</strong>而不是一个标量。并且两个向量的叉积<strong>与这两个向量组成的坐标平面垂直</strong>。</p><p>对于向量a和向量b，叉乘公式为：</p><img src="https://img-blog.csdn.net/20160902230539163" style="zoom:75%;" /><img src="https://img-blog.csdn.net/20160902231520146" style="zoom:75%;" /><h4 id="几何意义-1"><a href="#几何意义-1" class="headerlink" title="几何意义"></a><a href="https://blog.csdn.net/dcrmg/article/details/52416832" target="_blank" rel="noopener">几何意义</a></h4><h3 id="顺带区分：基本运算-×÷"><a href="#顺带区分：基本运算-×÷" class="headerlink" title="顺带区分：基本运算 +-×÷"></a>顺带区分：基本运算 +-×÷</h3><blockquote><p>The product operator * when used on arrays or matrices indicates <strong>element-wise</strong> multiplications.</p></blockquote><p><strong>四则运算符号</strong>对于<strong>np.array</strong>，都是单纯的<strong><u>element-wise</u>的对位操作</strong>，不像点积——不相加</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghh5wqiu4cj30m607241f.jpg" style="zoom:43%;" /><h3 id="矩阵与矩阵-向量相乘-也可用"><a href="#矩阵与矩阵-向量相乘-也可用" class="headerlink" title="矩阵与矩阵/向量相乘 也可用@"></a><strong>矩阵与矩阵/向量</strong>相乘 <strong>也可用<u>@</u></strong></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gj5p36wqcij31ji094gn0.jpg" alt="也可用@！"></p><p>也自然要求：<del>一维向量</del>array a和b的<strong><del>行列数</del><u>维度</u>相同</strong></p><h2 id="8-条件概率和交集概率的区分"><a href="#8-条件概率和交集概率的区分" class="headerlink" title="8. 条件概率和交集概率的区分"></a>8. 条件概率和交集概率的区分</h2><h3 id="交集概率-分母是全集-所以默认忽略"><a href="#交集概率-分母是全集-所以默认忽略" class="headerlink" title="交集概率: 分母是全集(所以默认忽略)"></a>交集概率: 分母是<u>全集</u>(所以默认忽略)</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlntiq7ocj31dq0kak1t.jpg" style="zoom:30%;" /><h3 id="条件概率-分母是做条件的那个概率的所在集"><a href="#条件概率-分母是做条件的那个概率的所在集" class="headerlink" title="条件概率: 分母是做条件的那个概率的所在集"></a>条件概率: 分母是<u>做条件的那个概率</u>的所在集</h3><h4 id="分子用了交集概率，分母是做条件的概率的所在集"><a href="#分子用了交集概率，分母是做条件的概率的所在集" class="headerlink" title="分子用了交集概率，分母是做条件的概率的所在集"></a>分子用了交集概率，分母是<u>做条件的概率</u>的所在集</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlnqr41w2j31f20jiaj3.jpg" style="zoom:30%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlnppukfmj315w0imaha.jpg" style="zoom:30%;" /><h2 id="9-np-array的转置-至少是2D"><a href="#9-np-array的转置-至少是2D" class="headerlink" title="9. np.array的转置: 至少是2D"></a>9. np.array的转置: 至少是2D</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho372sm9hj31f70u079y.jpg" style="zoom:40%;" /><h2 id="10-贝叶斯学习—ML的概率表示"><a href="#10-贝叶斯学习—ML的概率表示" class="headerlink" title="10. 贝叶斯学习—ML的概率表示"></a>10. 贝叶斯学习—ML的概率表示</h2><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200917171456223.png" style="zoom:43%;" /><p>一般来说一个<u>巨大（通常为无限）的假设空间</u>，可已有<u>无数个假设</u>。</p><p>贝叶斯学习的本质是：左边直观解释是，在<strong>已有的(训练)数据</strong>条件下，得到各个h的概率；公式右边分子是，<strong><u>符合(fit)这些数据D分布</u>的，假设h的概率</strong>——学习的目标则是，确定左边那个 P(h|D) 的 argmax 函数，得到最优的h</p><p>为了简化，<strong>去掉分母 P(D) 的项，因为它不依赖于假设而是已有的数据。这个方法被称为最大后验（MAP）。</strong></p><p>现在我们应用下面的数学技巧：</p><ul><li>最大化原函数和最大化取对数的原函数的过程是相似的，即取对数不影响求最大值问题。</li><li>乘积的对数等于对数的和。</li><li>正量的最大化等同于负量的最小化。</li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitrm3yzidj30kr07274b.jpg" style="zoom:50%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitrkb5ka4j31ew0agtbl.jpg" alt="总结" style="zoom:43%;" /><p>底数为 2 的负对数项看起来是不是很熟悉？这都来自「信息论」</p><h2 id="11-图像化表达：ML算法3种-VS-贝叶斯深度学习-神经网络"><a href="#11-图像化表达：ML算法3种-VS-贝叶斯深度学习-神经网络" class="headerlink" title="11. 图像化表达：ML算法3种 VS 贝叶斯深度学习(神经网络)"></a>11. 图像化表达：ML算法3种 VS 贝叶斯深度学习(神经网络)</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifuko31hlj31mf0u0b29.jpg" alt="" style="zoom:67%;" /><h4 id="Bayesian-神经网络：让具体的标量参数，变为概率分布"><a href="#Bayesian-神经网络：让具体的标量参数，变为概率分布" class="headerlink" title="Bayesian 神经网络：让具体的标量参数，变为概率分布"></a>Bayesian 神经网络：让具体的标量参数，变为概率分布</h4><p>贝叶斯深度学习的核心思想：将<strong>神经网络的<u>权重w和b视作服从某分布的随机变量</u></strong>，而不是固定值。以及网络的前向传播，就是<strong><u>从权值分布中抽样</u></strong>然后计算。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifus5k4vuj30no15owp0.jpg" alt="Going Bayesian" style="zoom:30%;" /><p>进而，导致得到的<strong><u>model也不是唯一的</u></strong>，而是处于一个<strong><u>范围的所有</u></strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifuqihxwsj30sa0fawhh.jpg" style="zoom:40%;" /><h4 id="贝叶斯学习的意义"><a href="#贝叶斯学习的意义" class="headerlink" title="贝叶斯学习的意义"></a>贝叶斯学习的意义</h4><p><strong><u>提供不确定性</u>，非 softmax 生成的概率</strong>。虽然工业界<strong>一般需要确定性结果</strong>，所以有这种贝叶斯机器学习在工业界没有广泛应用的错觉，但是贝叶斯理论<strong>对于事件具有<u>很强的指导意义</u></strong></p><h2 id="12-统计学-vs-统计-机器-学习"><a href="#12-统计学-vs-统计-机器-学习" class="headerlink" title="12. 统计学 vs 统计(机器)学习"></a>12. 统计学 vs 统计(机器)学习</h2><p>link: <a href="https://zhuanlan.zhihu.com/p/61964658" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61964658</a></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitsm0lryej31d00u0ans.jpg" alt="区分" style="zoom:67%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitsff759nj30s40qw77x.jpg" alt="总结" style="zoom:40%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitsfzesikj30yc0c842g.jpg" alt="总结" style="zoom:40%;" /><h2 id="13-急切学习-Eager-和惰性学习-Lazy"><a href="#13-急切学习-Eager-和惰性学习-Lazy" class="headerlink" title="13. 急切学习(Eager)和惰性学习(Lazy)"></a>13. 急切学习(Eager)和惰性学习(Lazy)</h2><h3 id="以下内容均为转载"><a href="#以下内容均为转载" class="headerlink" title="(以下内容均为转载)"></a><em>(以下内容均为转载)</em></h3><p>急切学习方法也叫积极学习方法, 惰性学习方法也叫消极学习方法</p><h3 id="急切-积极学习方法"><a href="#急切-积极学习方法" class="headerlink" title="急切/积极学习方法"></a>急切/积极学习方法</h3><p>指在利用算法<strong>进行判断<u>之前</u></strong>,先利用训练集数据通过<strong>训练得到一个目标函数</strong>；在需要进行判断时，利用已经训练好的函数进行决策。</p><p>意义：这种方法是<strong>在开始的时候需要进行一些工作,到后期进行使用的时候会很方便.</strong></p><p><strong>例如</strong></p><p>以很好理解的决策树为例,通过决策树进行判断之前,先通过对<strong>训练集的训练建立起了一棵树</strong>。</p><p>比如很经典的利用决策树判断一各新发现的物种是否为哺乳动物的例子,首先根据已经已知的训练集进行训练,提炼出哺乳动物的各种特征,这就是一个建立模型的过程；<strong>模型建立好之后,再有新物种需要进行判断的时候</strong>,只需要按照训练好的模型对照新物种挨个对比,就能很容易的给出判断结果.</p><h3 id="惰性-消极学习方法"><a href="#惰性-消极学习方法" class="headerlink" title="惰性/消极学习方法"></a>惰性/消极学习方法</h3><p><strong>最开始的时候不会根据已有的样本(通过”训练”过程)创建目标函数</strong>,只是<strong>简单的把训练用的样本<u>储存</u>好</strong>；后期<strong>需要对新进入的样本进行判断的时候</strong>， 才开始<strong>分析新进入样本与已存在的训练样本之间的关系</strong>，并据此确定新入样本的目标函数值。</p><p><strong>例如</strong></p><p>典型算法<strong>KNN</strong>：KNN<strong>不会根据训练集主动学习或者拟合出一个函数</strong>，来对新进入的样本进行判断,而是单纯的记住训练集中所有的样本,并<strong>没有像上边决策树那样先对训练集数据进行训练，<u>得出一套规则</u></strong>。所以它实际上<strong>没有所谓的”训练”过程</strong>,而是在<strong>需要进行预测的时候从自己的训练集样本中查找</strong>与新进入样本最相似的样本，来获得预测结果</p><h3 id="对比：双方互补"><a href="#对比：双方互补" class="headerlink" title="对比：双方互补"></a>对比：双方互补</h3><p>积极学习方法：在训练时<strong>考虑到了训练集中所有数据,训练时间比较长</strong>；有新样本进入需要判断的时候<strong>决策时间短</strong>。但是也是由于这个原因,<strong><u>做增量拓展</u>的时候比较虐</strong>.</p><p>消极学习方法：几乎没有训练时间,在新样本进入做判断的时候<strong>计算开销大，时间长</strong>；但是，<strong><u>天生支持增量学习</u></strong>。</p><p>那什么是增量学习？</p><h2 id="14-增量学习-Incremental-Learning"><a href="#14-增量学习-Incremental-Learning" class="headerlink" title="14 增量学习(Incremental Learning)"></a>14 增量学习(Incremental Learning)</h2><p><strong>简单定义</strong>：作为机器学习的一种方法，现阶段得到广泛的关注。在其中，<strong>输入数据不断被用于扩展现有模型的知识，即进一步训练模型</strong>。它代表了一种<strong>动态</strong>的学习的技术。</p><h3 id="地位"><a href="#地位" class="headerlink" title="地位"></a>地位</h3><p>是<strong><u>数据挖掘算法走向实用化</u></strong>的关键技术之一</p><h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>对于<strong>传统的批量学习</strong>技术来说,如何从<strong>日益增加的新数据</strong>中得到有用信息是一个难题。随着数据规模的不断增加,对时间和空间的需求也会迅速增加,最终会导致学习的速度赶不上数据更新的速度。机器学习是一个解决此问题的有效方法。</p><p> 随着人工智能和机器学习的发展,人们开发了很多机器学习算法。这些算法<strong>大部分都是批量学习(Batch Learning)模式</strong>,即<strong><u>需要在训练之前所有训练样本一次性可得</u></strong>,<font color="#dd0000">学习完这些样本<strong><u>之后</u></strong>——<strong>学习过程就终止了,不再学习新的知识</strong></font>；然而在实际应用中,训练样本通常<strong><u>不可能一次全部得到,而是随着时间逐步得到</u></strong>,并且样本<strong>反映的信息也可能随着时间产生了变化</strong>。如果新样本到达后<u><strong>要重新学习</strong></u>全部数据,需要消耗大量时间和空间,因此批量的学习算法不能满足这种需求。</p><p>为了实现<strong>在线学习</strong>的需求,需要抛弃以前的学习结果,重新训练和学习,这对时间和空间的需求都很高。因此,迫切需要研究增量学习方法,可以<strong>渐进的进行知识更新</strong>,且能<strong>修正和加强以前的知识</strong>,使得更新后的知识能<strong>适应</strong>新增加的数据。</p><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p>增量学习是指一个学习系统能<strong><u>不断地从新样本中学习新的知识,并能保存大部分以前已经学习到的知识</u></strong>。增量学习非常<font color="#dd0000"><strong>类似于人类自身的学习模式</strong></font>。因为人在成长过程中,每天学习和接收新的事物,学习是逐步进行的,而且,对已经学习到的知识,<strong>人类一般是不会遗忘的</strong>。</p><p>与传统的数据分类技术相比，增量学习分类技术具有显著的优越性，这主要表现在两个方面：一方面由于其<strong>无需保存历史数据</strong>，从而减少存储空间的占用；另一方面，由于其在新的训练中<strong>充分<u>利用了历史的训练结果</u>，从而显著地减少了后续训练的时间</strong>。</p><p>增量学习技术是一种得到广泛应用的<strong><u>智能化数据挖掘与知识</u></strong>发现技术。其思想是当样本逐步积累时，学习精度也要随之提高。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>对于满足以下条件的学习方法可以定义为增量学习方法：</p><ul><li>可以学习<strong>新的</strong>信息中的有用信息</li><li><strong>不需要访问</strong>已经用于训练分类器的<strong>原始数据</strong></li><li>但是，对已经学习的知识具有<strong>记忆</strong></li><li>在面对新数据中包含的新类别时，可以有效地进行处理</li></ul><h2 id="15-No-free-lunch理论"><a href="#15-No-free-lunch理论" class="headerlink" title="15 No free lunch理论"></a>15 No free lunch理论</h2><h2 id="16-解析-闭式解-vs-数值解"><a href="#16-解析-闭式解-vs-数值解" class="headerlink" title="16 解析\闭式解 vs 数值解"></a>16 解析\闭式解 vs 数值解</h2><h3 id="解析解-Analytical-solution-、-闭式解-Closed-form-solution"><a href="#解析解-Analytical-solution-、-闭式解-Closed-form-solution" class="headerlink" title="解析解(Analytical solution)、 闭式解(Closed-form solution)"></a>解析解(Analytical solution)、 闭式解(Closed-form solution)</h3><p>就是根据严格的<strong>公式推导</strong>，给出任意的自变量就可以求出其因变量，也就是<strong>求解公式/函数，它适用于所有这类方程的求解</strong>，然后可以利用这些公式计算相应的问题。</p><p>所谓的解析解是一种包含分式、三角函数、指数、对数甚至无限级数等<strong>基本<u>函数</u>的解的形式</strong>。用来求得解析解的方法称为解析法(Analytical techniques)，解析法即是常见的微积分技巧，例如分离变量法等。</p><p>因为解析解是一个<strong>封闭形式(Closed-form) 的<u>函数</u></strong>，因此<strong>对任一自变量，皆可将其<u>带入</u>解析函数</strong>求得正确的因变量。因此，解析解也被称为封闭解。</p><h3 id="数值解-Numerical-solution"><a href="#数值解-Numerical-solution" class="headerlink" title="数值解(Numerical solution)"></a>数值解(Numerical solution)</h3><p>是采用某种<strong>计算方法</strong>，如<strong>有限元法， 数值逼近法，插值法</strong>等得到的解。</p><p>别人<strong>只能利用数值计算的<u>结果</u></strong>，而<strong>因为没有求出函数解析式f(x)，不能<u>给出自变量</u>并求出其输出值</strong>。当无法藉由微积分技巧<strong>求得解析解</strong>时，这时便只能利用数值分析的方式来求得其数值解了。</p><p>在数值分析的过程中，首先会将原方程<strong>加以简化</strong>，以利于后来的数值分析。例如，会先将微分符号改为差分（微分的离散形式）符号等，然后再用传统的代数方法将原方程改写成另一种方便求解的形式。这时的求解步骤就是将一自变量带入，求得因变量的近似解，因此利用此方法所求得的因变量为一个个离散的数值，不像解析解为一连续的分布，而且因为经过上述简化的操作，其正确性也不如解析法可靠。</p><p>简而言之，解析解就是<strong>给出解的函数形式</strong>，从解的<strong>表达式中</strong>就可以算出任何对应值；数值解就是用<strong>数值方法求出的解</strong>，给出自变量, 求出某个特定方程的具体的解。在计算机应用中，这些<strong>特殊函数因为<u>大多有现成的数值法实现</u></strong>，它们通常被看作<strong>常见运算或常见函数</strong>；而实际上，在计算机的<strong><u>计算过程中</u></strong>，多数都是用<a href="https://baike.baidu.com/item/数值法" target="_blank" rel="noopener">数值法</a>计算的，</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ML-数学-中的常见混淆-易错点和一般性结论&quot;&gt;&lt;a href=&quot;#ML-数学-中的常见混淆-易错点和一般性结论&quot; class=&quot;headerlink&quot; title=&quot;ML(数学)中的常见混淆, 易错点和一般性结论&quot;&gt;&lt;/a&gt;ML(数学)中的常见混淆, 易错点和一
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="积累" scheme="http://yoursite.com/tags/%E7%A7%AF%E7%B4%AF/"/>
    
      <category term="统计概率" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E6%A6%82%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>有趣\有思想的words积累</title>
    <link href="http://yoursite.com/2020/07/08/meaningful-words/"/>
    <id>http://yoursite.com/2020/07/08/meaningful-words/</id>
    <published>2020-07-08T16:25:23.000Z</published>
    <updated>2020-10-16T08:04:11.771Z</updated>
    
    <content type="html"><![CDATA[<p>积累下日常看到的，<strong>有意思/有思想</strong>的句子(截图)</p><h3 id="0-统计学派：贝叶斯-vs-频率"><a href="#0-统计学派：贝叶斯-vs-频率" class="headerlink" title="0. 统计学派：贝叶斯 vs 频率"></a>0. 统计学派：贝叶斯 vs 频率</h3><h4 id="频率派-古典统计学"><a href="#频率派-古典统计学" class="headerlink" title="频率派 =古典统计学"></a>频率派 =古典统计学</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjr8gme9y4j314y09mgp6.jpg" alt="image-20201016160151317"></p><p>缺点就是：光靠做高频率的实验，去<strong>逼近</strong>模型的真实参数, 有的时间成本太高，不可能1次次真的做</p><h3 id="贝叶斯方法"><a href="#贝叶斯方法" class="headerlink" title="贝叶斯方法"></a><strong>贝叶斯方法</strong></h3><p>被证明是非常 general 且强大的<strong>推理框架</strong>，一句话概括贝叶斯方法创始人Thomas Bayes的观点就是：</p><p><strong>任何时候，我对世界总有一个<u>主观的先验判断</u>，但是这个判断会随着世界的真实变化而随机<u>修正</u>，我对世界<u>永远保持开放的态度</u></strong>。</p><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20201013185802157.png" alt="频率派：好理解" style="zoom:40%;" /><p>但是，频率派<strong>不适合于很多复杂的情况——客观概率，无法轻易获得时</strong>，贝叶斯就出场了</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjnmz0vds1j313s0gyn3p.jpg" alt="贝叶斯：加入主观，且生活中用的更多！" style="zoom:50%;" /><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20201013132323821.png" alt="贝叶斯公式的白话理解" style="zoom:50%;" /><h3 id="1-推荐系统是双刃剑"><a href="#1-推荐系统是双刃剑" class="headerlink" title="1. 推荐系统是双刃剑"></a>1. 推荐系统是双刃剑</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggk13ffztdj310w03sq3k.jpg" alt="推荐系统下，当今互联网产品"></p><h3 id="2-习惯：解决万事开头难"><a href="#2-习惯：解决万事开头难" class="headerlink" title="2. 习惯：解决万事开头难"></a>2. 习惯：解决万事开头难</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gglqrm9ladj310k0880v8.jpg" alt="万事开头难：先做5分钟"></p><h3 id="3-习惯：言简意赅"><a href="#3-习惯：言简意赅" class="headerlink" title="3. 习惯：言简意赅"></a>3. 习惯：言简意赅</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gglqphbyucj311y07amym.jpg" alt="好习惯：压缩表达"></p><h3 id="4-理论vs实践"><a href="#4-理论vs实践" class="headerlink" title="4. 理论vs实践"></a>4. 理论vs实践</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggzr92fnejj310y0hijwy.jpg" alt="理论vs实践" style="zoom:50%;" /><h3 id="5-关于出身时机"><a href="#5-关于出身时机" class="headerlink" title="5. 关于出身时机~"></a>5. 关于出身时机~</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghiqjayfzrj30ny0363yp.jpg" alt="戏谑出身时机" style="zoom:50%;" /><h3 id="6-电子产品上的文字聊天，是离散碎片、非线性的"><a href="#6-电子产品上的文字聊天，是离散碎片、非线性的" class="headerlink" title="6. 电子产品上的文字聊天，是离散碎片、非线性的"></a>6. 电子产品上的文字聊天，是离散碎片、非线性的</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi25zo73thj30pg044js1.jpg" style="zoom:50%;" /><h3 id="7-童年…"><a href="#7-童年…" class="headerlink" title="7. 童年…"></a>7. 童年…</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi25zy33oyj30lf07xgn3.jpg" style="zoom:40%;" /><h3 id="8-善良or软弱"><a href="#8-善良or软弱" class="headerlink" title="8. 善良or软弱"></a>8. 善良or软弱</h3><p>在2020.10.14，大连理工一研三学生因博导方面压力过大，写下微博遗书后 释然自尽的评论区下….</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjouklv1xsj30qy06egnv.jpg" style="zoom:50%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;积累下日常看到的，&lt;strong&gt;有意思/有思想&lt;/strong&gt;的句子(截图)&lt;/p&gt;
&lt;h3 id=&quot;0-统计学派：贝叶斯-vs-频率&quot;&gt;&lt;a href=&quot;#0-统计学派：贝叶斯-vs-频率&quot; class=&quot;headerlink&quot; title=&quot;0. 统计学派：贝叶斯 
      
    
    </summary>
    
    
    
      <category term="feeling&amp;mind" scheme="http://yoursite.com/tags/feeling-mind/"/>
    
  </entry>
  
  <entry>
    <title>7、8月小记</title>
    <link href="http://yoursite.com/2020/07/01/7%E6%9C%88%E5%B0%8F%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/07/01/7%E6%9C%88%E5%B0%8F%E8%AE%B0/</id>
    <published>2020-07-01T15:47:28.000Z</published>
    <updated>2020-10-09T04:25:39.747Z</updated>
    
    <content type="html"><![CDATA[<h3 id="7-1-被宠物🐶咬却被家长骂反应慢"><a href="#7-1-被宠物🐶咬却被家长骂反应慢" class="headerlink" title="7.1 被宠物🐶咬却被家长骂反应慢"></a>7.1 被宠物🐶咬却被家长骂反应慢</h3><p>感觉：很不好，应该是父母长期怼我这种慵懒的生活方式积累不满的爆发；</p><p>为了避免风险，再小的口子也得打<strong>很折腾的持续一个月的</strong>5次疫苗；打完第一针，感觉有点不适的反应，加上前一天被骂的不爽，只能拿狗撒气了，不然还能拿父母吗？因为我会认为压抑着不爽，对于个人身心健康也不好…</p><h3 id="7-11-7-14-江苏重游-吃…"><a href="#7-11-7-14-江苏重游-吃…" class="headerlink" title="7.11-7.14 江苏重游(吃…)"></a>7.11-7.14 江苏重游(吃…)</h3><p>和万老板去玩了一圈，实际是吃吃吃+探索了没去过的店：南京大牌档这种江浙菜，大众的火锅店，苏州的江边城外烤鱼和同德兴面食和淮海路日本街的日料自助(撑死了但大饱眼福)，上海的popeyes北美一大炸鸡品牌</p><p>感觉：满足了味蕾——长期在nc没有太多美食多样性; 但因为太懒了，没做太多旅游景点的攻略——晚上的时间还是在玩农药…</p><h3 id="7-15-决心”快乐运动”by租了一个健身环"><a href="#7-15-决心”快乐运动”by租了一个健身环" class="headerlink" title="7.15 决心”快乐运动”by租了一个健身环"></a>7.15 决心”快乐运动”by租了一个健身环</h3><h3 id="7-16-卸载王者荣耀"><a href="#7-16-卸载王者荣耀" class="headerlink" title="7.16 卸载王者荣耀"></a>7.16 卸载王者荣耀</h3><p>感觉：晚上有点心思学习或者处理别的事情，也不熬夜了</p><p>希望是个好的开端，且能持续下去</p><h3 id="8-14-decline-NEU-offer-正式放弃美国"><a href="#8-14-decline-NEU-offer-正式放弃美国" class="headerlink" title="8.14 decline NEU offer, 正式放弃美国"></a>8.14 decline NEU offer, 正式放弃美国</h3><p>去年毕业季想偷懒，骗骗自己gap去找工体验下就业环境同时探索下研究生和未来的专业方向，就申请了英美学校的defer(真·备胎计划)，运气好都批准了。英国这边占位费一直挺高，一下子10%学费，等于要是最后不去那，真的是拿家里的钱开玩笑——就为了这段第一句那个所谓的理由。</p><p>但回过头来，我发现gap最忌讳的就是没有做好提前的计划和找工准备，比如要复习那些面试要考的题，要去哪里工作，什么途径投递，target公司有哪些，如果心仪的选不上planB公司又有哪些…对于我这种半吊子空有GPA的渣，这些甚至应该在回国之前就开始进行——然而我去年6-8月其实并没有准备好，心思依然很散，直到9、10月才开始。然后乍一看，大厂招聘要求挺高，就算是实习面试一般也是3轮或以上，以我这种没有深度实战经历说不出所以然的半吊子，当面完网易一个低端岗位(依然有算法题)失败后就不再考虑了。幸运的是，复习过程中一些基本的东西+专门看对应公司的面经，能达到苏州和上海的一些小厂要求，加上自己比较能BB，聊天挺愉快。最后去了上海做一些小后端的活儿，度过了19年最后2个多月，也是迷幻20年平静的前夕。</p><p>接着，1月疫情开始蔓延，但当时处于封锁消息的17天，武汉外的人并不知情。算是幸运，当时稀里糊涂抱着下半年换个工作方向的OS，辞了职，在1月初就回了老家休息，还送别了美国春季入学的朋友(至少避开了国内疫情)。然后疫情期间，在家嘛就自然而然咸鱼化，这种状态的我是无心也找不到新工作的(唉)…在此期间，3、4月看到海外的疫情开始蔓延，5、6月美国疫情和种族斗争的失控和7月中美关系恶化，此时不禁又感叹幸运：当时交了巨额占位费，现在是要发挥作用了。而一年前这个时候，还和同学们憧憬着相约在美国，去那学CS和工作当高薪不加班群体，而不是回国大概率当工作节奏快强度大的上班族社畜，现在是不得不食言了😷。</p><p>当然，20fall入学的留学生这半年都是网课了，因为国外还是有二次复发的风险。但总的来看，去年今日的gap决定+申请defer是让我避开了今年春季疫情的峰期，同时defer一手英国还让我不用今年再申请就有书读——但也意味着，我错过了和熟悉的同学一起读书的机会，而是要去陌生的环境面对陌生的人，而在英国开设PSW留学签证的情况下，是该计划留下试水海外职场还是抓紧宝贵的应届生身份直接回国。</p><p>即我这浑浑噩噩的过去365天，比起世界上很多人现在看来其实算有各种幸运了，个体在大局势和大灾难面前过于渺小；而在这世界迷幻的1年，感激活着，平安喜乐。虽然不是宗教信仰者，但我相信感应和巧合，何况过去发生了种种幸运的巧合让我手上还留有读研的权利，可能在上帝的剧本里我就该老实去英国完成学业。希望未来的日子，世界会好转，而我还没有荒废到不可恢复。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;7-1-被宠物🐶咬却被家长骂反应慢&quot;&gt;&lt;a href=&quot;#7-1-被宠物🐶咬却被家长骂反应慢&quot; class=&quot;headerlink&quot; title=&quot;7.1 被宠物🐶咬却被家长骂反应慢&quot;&gt;&lt;/a&gt;7.1 被宠物🐶咬却被家长骂反应慢&lt;/h3&gt;&lt;p&gt;感觉：很不好
      
    
    </summary>
    
    
    
      <category term="feeling&amp;mind" scheme="http://yoursite.com/tags/feeling-mind/"/>
    
      <category term="积累" scheme="http://yoursite.com/tags/%E7%A7%AF%E7%B4%AF/"/>
    
  </entry>
  
  <entry>
    <title>算法题的自我经验总结</title>
    <link href="http://yoursite.com/2020/07/01/leetcode-self-exp/"/>
    <id>http://yoursite.com/2020/07/01/leetcode-self-exp/</id>
    <published>2020-07-01T07:53:01.000Z</published>
    <updated>2020-10-09T04:25:17.138Z</updated>
    
    <content type="html"><![CDATA[<p>（注意是数据结构的巧用：<font color="#dd0000">提高效率/简化代码</font>的use case）</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giknzozs6tj31640f0dto.jpg" alt="刷题6步-注意第2步" style="zoom:50%;" /><h3 id="0-树的递归-初步—画草图找递归部分"><a href="#0-树的递归-初步—画草图找递归部分" class="headerlink" title="0. 树的递归 初步—画草图找递归部分"></a>0. 树的递归 初步—画<font color="#dd0000">草图</font>找递归部分</h3><p>在正式编码之前，思考时，我们知道递归是强调找到<strong>结构类似的子问题</strong>，且递归那几行代码就是<strong>针对这个子结构的</strong>——所以我们可以brainstorm解法中，<strong>在上面<u>第2步</u>，通过<font color="#dd0000">比划草图和<u>具体输入</u></font>，确定</strong>递归代码要<strong><font color="#dd0000">覆盖树的<u>几层</u></font></strong>，这样方便<strong>初步<u>比划</u>想出一个解法 + 单步运行验证它</strong>。</p><p>如下面这题</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gikgndvfutj30b208qwf1.jpg" style="zoom:50%;" /><p>思路：可以画出下面这种草图，确定自己<font color="#dd0000"><strong>要track的node</strong>具体 在每次递归是什么</font>，从而总结出递归结构的)</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gikgb3588wj30rw0hawn2.jpg" alt="红色相连要比较的，蓝色和绿色是第1、2次的递归结构" style="zoom:50%;" /><ul><li>tree level 1: 先判定 root == null；</li><li>tree level 2: 判断<strong>root.</strong>left和right树不为空(<strong>防止Nullpointer Exception</strong>); 都确定存在后，确定<strong>值相同否</strong></li><li>(tree level 3): 若相同，则递归判断 left right<strong>各自的</strong>子left, right树是否满足<strong><u>对称</u></strong>——<strong>通过草图，可以发现后续比较的是</strong>left.left 和 right.right，right.left 和 left.right，<strong>而不是像root node那样最简单的情况——自己是中轴 把对称的两个2对比即可</strong>！</li></ul><p>所以，初步认为，一次递归的<strong>比较、判定(返回boolean)部分</strong>会覆盖了tree的2层，而接着第3层会加入递归结构——而且继续往下递归，入参就是从level 1到level 2的<strong>root.</strong>left和right</p><h3 id="1-获取的数据是倒叙的，但要求他们正序输出"><a href="#1-获取的数据是倒叙的，但要求他们正序输出" class="headerlink" title="1. 获取的数据是倒叙的，但要求他们正序输出"></a>1. 获取的数据是倒叙的，但要求他们正序输出</h3><p>出处：Coursera算法 <strong>图的DFS in 搜索树</strong></p><h4 id="i-暴力解法用：数组"><a href="#i-暴力解法用：数组" class="headerlink" title="i. 暴力解法用：数组"></a>i. 暴力解法用：数组</h4><p>1by1存到数组里，然后想办法把<strong>数组reverse倒序</strong>，再输出</p><h4 id="ii-简化代码，使用数据结构：栈"><a href="#ii-简化代码，使用数据结构：栈" class="headerlink" title="ii. 简化代码，使用数据结构：栈"></a>ii. 简化代码，使用数据结构：<u>栈</u></h4><p>1by1 push到栈里，利用<strong>栈的Java@for each遍历</strong><font color="#dd0000"><strong>默认</strong>是从<strong>栈顶</strong>开始的</font>——所以遍历时，自动会从最后push的开始，达到<strong>自动正序</strong>的输出</p><p>实例应用：DFS深度优先算法搜索，或许源s到任何一个v的<strong>完整路径</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggbj8a6k3oj30qa0i8n16.jpg" style="zoom:33%;" /><h4 id="PS-如果是digraph的搜索，这种case又名reverse-postorder"><a href="#PS-如果是digraph的搜索，这种case又名reverse-postorder" class="headerlink" title="PS: 如果是digraph的搜索，这种case又名reverse postorder"></a>PS: 如果是digraph的搜索，这种case又名<u>reverse</u> postorder</h4><p>(postorder: 即先访问完<strong>所有子节点</strong>，再<strong>返回到父节点</strong>。而 graph搜索中<strong>DFS</strong>生成的正是postorder)</p><p>以下摘自stackoverflow</p><blockquote><p>Reverse postordering as the name suggests produces the exact <strong>opposite of postorder traversal.</strong></p><p><strong>Example</strong></p><p>if postorder traversals are D B C A and D C B A</p><p>the reverse postorder traversals are A C B D and A B C D</p><h5 id="How-to-obtain-reverse-postorder-traversal"><a href="#How-to-obtain-reverse-postorder-traversal" class="headerlink" title="How to obtain reverse postorder traversal"></a><strong>How to obtain reverse postorder traversal</strong></h5><p>One way is to run postorder traversal and push the nodes <strong><u>in a stack</u></strong> in postorder. Then <strong>pop out</strong> the nodes to get the reverse postorder.</p></blockquote><h3 id="2-图的搜索：”邻接表”的高效性"><a href="#2-图的搜索：”邻接表”的高效性" class="headerlink" title="2. 图的搜索：”邻接表”的高效性"></a>2. 图的搜索：”邻接表”的高效性</h3><p>出处：LeetCode207 图的<strong>拓扑排序</strong>(大前提：输入是 <strong>边缘列表(边的嵌套2Darray)</strong> 表示的图形，而不是 邻接矩阵 )</p><h4 id="若遍历时要访问所有邻点，请在一开始就构造图"><a href="#若遍历时要访问所有邻点，请在一开始就构造图" class="headerlink" title="若遍历时要访问所有邻点，请在一开始就构造图"></a>若遍历时要访问所有<u>邻点</u>，请在一开始就<u>构造图</u></h4><h4 id="i-不构造邻接表，暴力解法用：数组"><a href="#i-不构造邻接表，暴力解法用：数组" class="headerlink" title="i. 不构造邻接表，暴力解法用：数组"></a>i. 不构造邻接表，暴力解法用：数组</h4><p>弊端：BFS/DFS<strong>去找邻点</strong>时，<strong>每次都</strong>需要<strong>O(N)</strong>去<strong>遍历所有的边</strong>在<strong><u>边缘列表</u></strong>中</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggtzpuep1lj30im056gm1.jpg" alt="image-20200717151505568" style="zoom:50%;" /><h4 id="ii-用数据结构构造邻接表：hashtable-lt-Int-List-lt-Integer-gt-gt-或-List-lt-List-lt-Integer-gt-gt"><a href="#ii-用数据结构构造邻接表：hashtable-lt-Int-List-lt-Integer-gt-gt-或-List-lt-List-lt-Integer-gt-gt" class="headerlink" title="ii. 用数据结构构造邻接表：hashtable&lt;Int, List&lt;Integer&gt;&gt; 或 List&lt;List&lt;Integer&gt;&gt;"></a>ii. 用数据结构构造邻接表：hashtable&lt;Int, List&lt;Integer&gt;&gt; 或 List&lt;List&lt;Integer&gt;&gt;</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggtzhjqwddj30fj0gu0xi.jpg" alt="图的表示法：邻接表" style="zoom:50%;" /><h5 id="高效原因："><a href="#高效原因：" class="headerlink" title="高效原因："></a>高效原因：</h5><p>邻接表是最常用，<strong>最高效</strong>的表示法，<strong>性能上的好处</strong>是显而易见的</p><p>因为用了<strong>indexing</strong>和list线性存储, 可以<strong><u>一次性</u>获取到(而不是遍历哦)</strong>到<strong>所有邻接点in a list</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggtzl329saj30r207b40n.jpg" alt="image-20200717151030836" style="zoom:50%;" /><h3 id="3-树的层序遍历-所有子node一起递归时-—用队列Queue存结果"><a href="#3-树的层序遍历-所有子node一起递归时-—用队列Queue存结果" class="headerlink" title="3. 树的层序遍历(所有子node一起递归时)—用队列Queue存结果"></a>3. 树的<u>层序</u>遍历(所有子node<font color="#dd0000"><u>一起</u>递归时</font>)—用队列Queue存结果</h3><blockquote><p><strong>Level</strong> order traversal(一次性递归所有子node) uses a queue data structure to visit the nodes <strong>level by level</strong></p></blockquote><p><strong>不光是</strong>树的层序遍历，Queue适合存储:<strong>需要递归时，各自递归</strong>的<strong>“网状扩散处理”型</strong>的结果：比如递归节点是1-&gt;3，扩散完把3个新递归enqueue后，把1从队列首dequeue；3再-&gt;6同理…这种模式</p><h3 id="4-实现循环Queue用到的抽象"><a href="#4-实现循环Queue用到的抽象" class="headerlink" title="4. 实现循环Queue用到的抽象"></a>4. 实现循环Queue用到的抽象</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gheuhwcabsj31dw0ks0x9.jpg" alt="抽象：代码描述" style="zoom:40%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gheujpnv8oj30z00dsdho.jpg" alt="(不抽象的)具体形象表达" style="zoom:33%;" /><h3 id="5-java的深浅拷贝"><a href="#5-java的深浅拷贝" class="headerlink" title="5. java的深浅拷贝"></a>5. java的深浅拷贝</h3><h4 id="浅拷贝的特例：集合存的是immutable-Objects"><a href="#浅拷贝的特例：集合存的是immutable-Objects" class="headerlink" title="浅拷贝的特例：集合存的是immutable Objects"></a>浅拷贝的<font color="#dd0000">特例</font>：集合存的是immutable Objects</h4><h5 id="如：包装类，String类"><a href="#如：包装类，String类" class="headerlink" title="如：包装类，String类"></a>如：包装类，String类</h5><p>包装类型，即基本类型的包装类，由于自动装箱的缘故，复制后的集合与原集合所指向的<strong>并非指向同一个内存对象</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gikloyqteij31fq0iq16y.jpg" alt="如果集合存的是immutable对象，则可以看成”深拷贝“" style="zoom:50%;" /><h5 id="如：113题-路径总和-ii"><a href="#如：113题-路径总和-ii" class="headerlink" title="如：113题.路径总和-ii"></a>如：113题.路径总和-ii</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gikly698h0j31980tq453.jpg" style="zoom:43%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;（注意是数据结构的巧用：&lt;font color=&quot;#dd0000&quot;&gt;提高效率/简化代码&lt;/font&gt;的use case）&lt;/p&gt;
&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1giknzozs6tj31640f0dto
      
    
    </summary>
    
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="CS" scheme="http://yoursite.com/tags/CS/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
      <category term="积累" scheme="http://yoursite.com/tags/%E7%A7%AF%E7%B4%AF/"/>
    
  </entry>
  
  <entry>
    <title>NLP-text-feature-represent</title>
    <link href="http://yoursite.com/2020/06/10/NLP-text-feature-represent/"/>
    <id>http://yoursite.com/2020/06/10/NLP-text-feature-represent/</id>
    <published>2020-06-10T08:28:09.000Z</published>
    <updated>2020-08-05T08:15:33.430Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文本的表示和特征提取"><a href="#文本的表示和特征提取" class="headerlink" title="文本的表示和特征提取"></a>文本的表示和特征提取</h1><h2 id="文本表示-向量"><a href="#文本表示-向量" class="headerlink" title="文本表示(向量)"></a>文本表示(向量)</h2><h3 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h3><h4 id="one-hot"><a href="#one-hot" class="headerlink" title="one-hot"></a>one-hot</h4><p><strong>维度=|词典|</strong></p><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200610163125511.png" style="zoom:33%;" /><h3 id="句子"><a href="#句子" class="headerlink" title="句子"></a>句子</h3><h4 id="1-Sparse-boolean-representation-类似one-hot"><a href="#1-Sparse-boolean-representation-类似one-hot" class="headerlink" title="1. Sparse boolean representation(类似one-hot)"></a>1. <u>Sparse</u> boolean representation(类似one-hot)</h4><p><strong>维度=|词典|</strong></p><h5 id="不管词频"><a href="#不管词频" class="headerlink" title="不管词频"></a>不管词频</h5><p><strong>无论</strong>一个单词出现了<strong>几次</strong>，<strong>都设为1</strong>(true)</p><h5 id="明显缺点"><a href="#明显缺点" class="headerlink" title="明显缺点"></a>明显缺点</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghg07t9frvj31ay0m2aiz.jpg" alt="参数量 ∝ 单词总量V" style="zoom:33%;" /><h4 id="2-Count-based-representation-记词频"><a href="#2-Count-based-representation-记词频" class="headerlink" title="2. Count-based representation(记词频)"></a>2. Count-based representation(记词频)</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfna64u3v3j31g80aiaqo.jpg" alt="image-20200610163726467" style="zoom:33%;" /><h3 id="句子相似度"><a href="#句子相似度" class="headerlink" title="句子相似度"></a>句子相似度</h3><p><strong>欧氏距离</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfnfjibpxsj30j00980yn.jpg" style="zoom:33%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;文本的表示和特征提取&quot;&gt;&lt;a href=&quot;#文本的表示和特征提取&quot; class=&quot;headerlink&quot; title=&quot;文本的表示和特征提取&quot;&gt;&lt;/a&gt;文本的表示和特征提取&lt;/h1&gt;&lt;h2 id=&quot;文本表示-向量&quot;&gt;&lt;a href=&quot;#文本表示-向量&quot; class
      
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP-text-process</title>
    <link href="http://yoursite.com/2020/06/07/NLP-text-process/"/>
    <id>http://yoursite.com/2020/06/07/NLP-text-process/</id>
    <published>2020-06-07T07:06:46.000Z</published>
    <updated>2020-08-05T11:17:40.556Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP文本处理"><a href="#NLP文本处理" class="headerlink" title="NLP文本处理"></a>NLP文本处理</h1><h2 id="NLP-general-pipeline"><a href="#NLP-general-pipeline" class="headerlink" title="NLP general pipeline"></a>NLP general pipeline</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjqjhtaemj31kj0u0qte.jpg" alt="pipeline" style="zoom:30%;" /><h2 id="文本处理阶段"><a href="#文本处理阶段" class="headerlink" title="文本处理阶段"></a>文本处理阶段</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjqm2tae6j31go08gai2.jpg" alt="text process" style="zoom:30%;" /><h3 id="常见处理环节"><a href="#常见处理环节" class="headerlink" title="常见处理环节"></a>常见处理环节</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm2bbdbwoj30zg0eaaj2.jpg" style="zoom:33%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghg5ar7dblj30pi06kdgk.jpg" alt="预处理文本大致步骤" style="zoom:50%;" /><hr><h2 id="A-分词的常用底层算法"><a href="#A-分词的常用底层算法" class="headerlink" title="A. 分词的常用底层算法"></a>A. 分词的常用底层算法</h2><p>所有分词的工具，都要<strong><u>依赖于词库</u></strong></p><h3 id="1-基于匹配规则的方法"><a href="#1-基于匹配规则的方法" class="headerlink" title="1. 基于匹配规则的方法"></a>1. 基于<u>匹配</u>规则的方法</h3><h4 id="最大匹配算法-Max-Matching——贪心算法"><a href="#最大匹配算法-Max-Matching——贪心算法" class="headerlink" title="最大匹配算法 Max Matching——贪心算法"></a><u>最大</u>匹配算法 Max Matching——<font color="#dd0000"><u>贪心</u></font>算法</h4><h5 id="何为最大？-人为设的-每次截取的最大长度——经验决定，看词典和这种语言中词语常见的最大长度"><a href="#何为最大？-人为设的-每次截取的最大长度——经验决定，看词典和这种语言中词语常见的最大长度" class="headerlink" title="何为最大？(人为设的)每次截取的最大长度——经验决定，看词典和这种语言中词语常见的最大长度"></a>何为<u>最大</u>？(人为设的)每次截取的最大长度——<u>经验</u>决定，看词典和这种语言中<u>词语常见的最大长度</u></h5><ul><li><u>前向</u>最大匹配(forward-max matching)</li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjs4728c6j31gi0t07wh.jpg" style="zoom:30%;" /><ul><li><u>后向</u>最大匹配(backward-max matching): 句子的<strong>遍历方向，从右到左</strong>和前向<strong>相反</strong>而已<ul><li>和前向的效果90%大多数情况，结果一样</li><li>不一样的情况：后向可能会导致一些分词的琐碎</li></ul></li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjs9uae0oj31cg0nudwn.jpg" style="zoom:30%;" /><ul><li>双向：结合了前向和后向，双向遍历</li></ul><h5 id="该算法的缺点"><a href="#该算法的缺点" class="headerlink" title="该算法的缺点"></a>该算法的缺点</h5><ul><li><p>贪心只能找局部最优，未必最优 </p></li><li><p>效率，依赖于max_length</p></li><li><p><font color="#dd0000">容易歧义，因为算法不支持考虑<strong><u>语义</u></strong></font>，<strong>不智能</strong>。导致机器学习的：<strong>纯粹是单词</strong>，没有任何<strong>句法</strong>甚至<strong>语义</strong></p><ul><li><p>比如一个case：在词典中，一个词是另一个词的子集，而最大匹配会<strong>保留那个更长的词</strong>(<strong>匹配到</strong>该更长的词，则提前结束遍历)</p><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200607161233659.png" style="zoom:35%;" /></li></ul></li></ul><hr><p>那么，什么样的算法工具，能考虑到语义？想想<strong>语言模型</strong>(机器翻译也用到了)</p><p>语言模型，可以evaluate一句话<font color="#dd0000"><strong>语义的正确性</strong></font>——基于<strong>已有的词频的统计</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjt7ifbxlj310m0doak1.jpg" alt="人工的词频统计" style="zoom:33%;" /><p>所以，对于分词结果，计算其语义合理性</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjt8qkr98j319209cn88.jpg" style="zoom:33%;" /><h3 id="2-基于概率统计的方法——考虑语义"><a href="#2-基于概率统计的方法——考虑语义" class="headerlink" title="2. 基于概率统计的方法——考虑语义"></a>2. 基于概率统计的方法——考虑语义</h3><h4 id="by语言模型-机器翻译也用到了"><a href="#by语言模型-机器翻译也用到了" class="headerlink" title="by语言模型(机器翻译也用到了)"></a>by<u>语言模型</u>(<u>机器翻译</u>也用到了)</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjux5rp84j31760u07wh.jpg" style="zoom:30%;" /><h5 id="可能的问题和解决-underflow"><a href="#可能的问题和解决-underflow" class="headerlink" title="可能的问题和解决: underflow"></a>可能的问题和解决: underflow</h5><p>问题：词频统计的概率太小，小数一旦再乘积，导致underflow的溢出</p><p>解决：<strong>数学处理技巧</strong>，给概率加上log<strong>转化为<u>累加</u></strong>，利用log的<strong><u>递增型</u></strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjtgdx5m3j316y0f6ws9.jpg" alt="log转化" style="zoom:33%;" /><h5 id="缺点和解决"><a href="#缺点和解决" class="headerlink" title="缺点和解决"></a>缺点和解决</h5><h5 id="缺点：如果要考虑所有可能，复杂度太高"><a href="#缺点：如果要考虑所有可能，复杂度太高" class="headerlink" title="缺点：如果要考虑所有可能，复杂度太高"></a>缺点：如果要考虑所有可能，复杂度太高</h5><h5 id="解决：维特比算法"><a href="#解决：维特比算法" class="headerlink" title="解决：维特比算法"></a>解决：维特比算法</h5><p>(问题和解决方法，都和我<a href="https://kennyng-19.github.io/Kenny_Ng.github.io/2020/01/29/NLP-intro/" target="_blank" rel="noopener">NLP-intro</a>文中的机器翻译case的<strong>穷举法下</strong>的情况，<strong>是一样的</strong>)</p><h4 id="用维特比算法来优化"><a href="#用维特比算法来优化" class="headerlink" title="用维特比算法来优化"></a>用维特比算法来优化</h4><p>维特比<strong>基于DP动态规划</strong></p><p>//TODO ……..</p><h2 id="B-拼写纠错"><a href="#B-拼写纠错" class="headerlink" title="B. 拼写纠错"></a>B. 拼写纠错</h2><h3 id="编辑距离-edit-dis"><a href="#编辑距离-edit-dis" class="headerlink" title="编辑距离(edit dis.)"></a>编辑距离(edit dis.)</h3><h4 id="3种编辑操作"><a href="#3种编辑操作" class="headerlink" title="3种编辑操作"></a>3种编辑操作</h4><p>replace, add和delete。</p><h4 id="需要操作的数量-edit-dis"><a href="#需要操作的数量-edit-dis" class="headerlink" title="需要操作的数量=edit dis"></a>需要操作的数量=edit dis</h4><h3 id="算法目标：找到编辑距离较小的候选words"><a href="#算法目标：找到编辑距离较小的候选words" class="headerlink" title="算法目标：找到编辑距离较小的候选words"></a>算法目标：找到<u>编辑距离较小</u>的候选words</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm4rhshbsj31fo0ra7f0.jpg" style="zoom:30%;" /><h4 id="❌低效率的暴力搜索法"><a href="#❌低效率的暴力搜索法" class="headerlink" title="❌低效率的暴力搜索法"></a>❌低效率的暴力<u>搜索</u>法</h4><p>遍历词典的<strong>所有词</strong>，逐一比较</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm4ot9eayj31ma0qodvp.jpg" style="zoom:27%;" /><h4 id="✅升级版-生成-距离较小的字符串"><a href="#✅升级版-生成-距离较小的字符串" class="headerlink" title="✅升级版: 生成 距离较小的字符串"></a>✅升级版: <u>生成</u> 距离较小的字符串</h4><h5 id="step1-生成大量words"><a href="#step1-生成大量words" class="headerlink" title="step1: 生成大量words"></a>step1: 生成大量words</h5><p><strong>当然，生成</strong>也要用到3种常规<strong>编辑操作</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm4vjv2trj30bq0lotet.jpg" style="zoom:33%;" /><h6 id="问题-为什么只生成dis为1，2的？"><a href="#问题-为什么只生成dis为1，2的？" class="headerlink" title="问题: 为什么只生成dis为1，2的？"></a>问题: 为什么只生成dis为1，2的？</h6><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm4widswij309s05idh7.jpg" style="zoom:25%;" /><p>因为经验主义，<strong>大多数情况</strong>，<strong>人为的</strong>单词拼写错误，只是1、2个字母的错误</p><h5 id="Step2-过滤筛选"><a href="#Step2-过滤筛选" class="headerlink" title="Step2: 过滤筛选"></a>Step2: 过滤筛选</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm544piarj30ry0a8q6z.jpg" style="zoom:50%;" /><h6 id="问题定义，转为条件概率问题"><a href="#问题定义，转为条件概率问题" class="headerlink" title="问题定义，转为条件概率问题"></a>问题定义，转为条件概率问题</h6><p>c，为<strong>现有词典里的<font color="#dd0000">正确的</font>单词</strong></p><p>现在要，求出每一个<strong>生成的word，s</strong> 其<font color="#dd0000"><strong>对应<u>所有</u>可能的c中</strong></font>概率最大的那个c ——即为<strong>“最正确”</strong>的</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm55fyha3j3152090wmp.jpg" style="zoom:33%;" /><p>基于贝叶斯定理，</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfmgp0y1e1j319e0daan3.jpg" style="zoom:37%;" /><p>再细看，理解<strong>P(s|c)和P(c)</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfmgqyuhkdj30wk0660yy.jpg" style="zoom:50%;" /><p>理解P(s|c)，<strong>基于统计</strong>：人写错单词c为s的概率</p><p>举例: 人把apple写成其他形式s，如app, appl的概率</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfmjmfoqeqj30bg0g479w.jpg" style="zoom:50%;" /><p>理解P(c)：即<strong>基于统计的词频</strong>，单词c(在语料库)出现的概率</p><h2 id="C-移除停用词Stop-words"><a href="#C-移除停用词Stop-words" class="headerlink" title="C. 移除停用词Stop words"></a>C. 移除停用词Stop words</h2><h3 id="移除对象"><a href="#移除对象" class="headerlink" title="移除对象"></a>移除对象</h3><p>通常移除的是<strong>停用词(对意义理解影响很小的)、出现频率很低(noise, outlier偏离主语义的)</strong>的词</p><p>注意：虽然有的停用词很常见，但也要<strong>考虑应用场景</strong>考虑<strong>是否移除</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfn8q0eqdxj30rg058tcj.jpg" alt="image-20200610154719443" style="zoom:33%;" /><p>比如：”好，很好“这种停用词，在<u>sentiment analysis</u>中就<u>不应该被移除</u>——反而可能需要<strong>修改</strong><u>停用词库</u></p><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghg591mrcpj31ko0sutgr.jpg" style="zoom:33%;" /><h3 id="注意-在情感分析中，stem的词可只留着前缀！"><a href="#注意-在情感分析中，stem的词可只留着前缀！" class="headerlink" title="注意: 在情感分析中，stem的词可只留着前缀！"></a>注意: 在情感分析中，stem的词可只留着前缀！</h3><p>因为<strong>分类</strong>问题，只需要知道什么词占对应情感类别<strong>的比例/程度</strong>即可</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghg5d0rmhjj31ie0au40q.jpg" alt="stem后 词并不是正确拼写的" style="zoom:43%;" /><h2 id="D-词的标准化-英文文本"><a href="#D-词的标准化-英文文本" class="headerlink" title="D. 词的标准化(英文文本)"></a>D. 词的标准化(英文文本)</h2><h3 id="标准化的对象"><a href="#标准化的对象" class="headerlink" title="标准化的对象"></a>标准化的对象</h3><p>英文中常用的词语的多种形式(单复数 时态等)，在<strong>语义理解</strong>上该统一</p><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200610160111641.png" style="zoom:35%;" /><h3 id="常用技术"><a href="#常用技术" class="headerlink" title="常用技术"></a>常用技术</h3><h4 id="Stemming-基本还原成原形"><a href="#Stemming-基本还原成原形" class="headerlink" title="Stemming: 基本还原成原形"></a>Stemming: 基本还原成原形</h4><blockquote><p>Stemming is the process of converting a word <strong>to its most general form, or stem</strong>. This helps in reducing the size of our vocabulary.</p></blockquote><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghg0nir63dj31340gm43m.jpg" style="zoom:33%;" /><h4 id="Lemmazation-更严格"><a href="#Lemmazation-更严格" class="headerlink" title="Lemmazation(更严格)"></a>Lemmazation(更严格)</h4><h3 id="stemmer底层原理实例"><a href="#stemmer底层原理实例" class="headerlink" title="stemmer底层原理实例"></a>stemmer底层原理实例</h3><p>需要<u>语言学家</u>(懂词根词缀的)来制定！</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfn93dbta0j31pw0po7wh.jpg" style="zoom:33%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;NLP文本处理&quot;&gt;&lt;a href=&quot;#NLP文本处理&quot; class=&quot;headerlink&quot; title=&quot;NLP文本处理&quot;&gt;&lt;/a&gt;NLP文本处理&lt;/h1&gt;&lt;h2 id=&quot;NLP-general-pipeline&quot;&gt;&lt;a href=&quot;#NLP-general-p
      
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv05模型集成</title>
    <link href="http://yoursite.com/2020/06/02/datawhale-cv05%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/"/>
    <id>http://yoursite.com/2020/06/02/datawhale-cv05%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/</id>
    <published>2020-06-02T14:35:07.000Z</published>
    <updated>2020-06-02T15:30:01.125Z</updated>
    
    <content type="html"><![CDATA[<p>本章是本次赛题学习的最后一章，将会讲解如何使用集成学习提高预测精度</p><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><p>本章讲解的知识点包括</p><ul><li>学习集成学习<strong>方法</strong>，以及交叉验证情况下的模型集成</li><li>学会使用<strong>深度学习</strong>模型的集成学习和结果后处理思路。</li></ul><h2 id="5-模型集成"><a href="#5-模型集成" class="headerlink" title="5 模型集成"></a>5 模型集成</h2><h3 id="5-1-集成学习方法"><a href="#5-1-集成学习方法" class="headerlink" title="5.1 集成学习方法"></a>5.1 集成学习方法</h3><p>在机器学习中的集成学习可以在一定程度上提高预测精度，常见的集成学习方法有Stacking、Bagging和Boosting，同时这些集成学习方法与具体验证集划分联系紧密。</p><p>由于深度学习模型一般需要较长的训练周期，如果硬件设备不允许建议选取留出法，如果需要追求精度可以使用交叉验证的方法。</p><p>下面假设构建了10折交叉验证，训练得到10个CNN模型。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task05/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.png" alt="集成学习"></p><p>那么在10个CNN模型可以使用如下方式进行集成：</p><ol><li><p>对预测的结果的<u>概率值</u><strong>进行平均</strong>，然后解码为具体字符；</p></li><li><p>or 对预测的字符进行<strong>投票</strong>，得到最终字符</p></li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>训练了<strong>10个CNN</strong>，<strong>训练时间</strong>较长</p><h3 id="5-2-深度学习中的集成学习"><a href="#5-2-深度学习中的集成学习" class="headerlink" title="5.2 深度学习中的集成学习"></a>5.2 深度学习中的集成学习</h3><p>此外在深度学习中本身还有一些集成学习思路的做法，值得借鉴学习：</p><h4 id="5-2-1-Dropout"><a href="#5-2-1-Dropout" class="headerlink" title="5.2.1 Dropout"></a>5.2.1 Dropout</h4><p>Dropout可以作为<font color="#dd0000"><strong>训练</strong></font>深度神经网络的一种技巧。在每个<font color="#dd0000"><strong>训练</strong>批次</font>中，通过<font color="#dd0000"><strong>随机让一部分(按照输入的比例)的节点</strong>停止工作</font>，但同时在<font color="#dd0000"><strong>预测</strong>的过程中</font>让<font color="#dd0000"><strong>所有的</strong>节点</font>都其作用。</p><h5 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h5><p>经常出现在在<strong>先有的CNN网络</strong>中，可以有效的<strong>缓解模型过拟合</strong>的情况，也可以在预测时<strong>增加模型的精度</strong></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><p>加入Dropout后的代码如下: 即nn<strong>.Dropout(0.25)</strong>这一行，(按照输入的比例为0.25)</p><pre class=" language-Python"><code class="language-Python"># 定义模型class SVHN_Model1(nn.Module):    def __init__(self):        super(SVHN_Model1, self).__init__()        # CNN提取特征模块        self.cnn = nn.Sequential(            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),            nn.Dropout(0.25), //            nn.MaxPool2d(2),            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),             nn.Dropout(0.25), //            nn.MaxPool2d(2),        )        #         self.fc1 = nn.Linear(32*3*7, 11)        self.fc2 = nn.Linear(32*3*7, 11)        self.fc3 = nn.Linear(32*3*7, 11)        self.fc4 = nn.Linear(32*3*7, 11)        self.fc5 = nn.Linear(32*3*7, 11)        self.fc6 = nn.Linear(32*3*7, 11)    def forward(self, img):          //...</code></pre><h4 id="5-2-2-Snapshot"><a href="#5-2-2-Snapshot" class="headerlink" title="5.2.2 Snapshot"></a>5.2.2 Snapshot</h4><p>本章的开头5.2已经提到训练了<strong>10个CNN</strong>则可以将多个模型的预测结果<strong>进行平均</strong>。但是加入只训练了<strong><u>一个</u></strong>CNN模型，如何做模型集成呢？</p><p>在论文Snapshot Ensembles中，作者提出使用cyclical learning rate进行训练模型，并<font color="#dd0000">保存<strong>精度比较好的</strong>一些checkopint，最后<strong>将多个checkpoint</strong>进行<strong>模型集成</strong></font>。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task05/Snapshot.png" alt="Snapshot"></p><h5 id="优-缺点"><a href="#优-缺点" class="headerlink" title="优/缺点"></a>优/缺点</h5><p>在Snapshot论文中作者通过使用表明，此种方法</p><ul><li><p>优点：可以在一定程度上<strong>提高模型精度</strong>，</p></li><li><p>缺点：但<strong>需要更长的训练时间</strong>；由于在cyclical learning rate中<strong>学习率</strong>有周期性变大和减少的行为，因此CNN模型很有可能在<strong>跳出局部最优进入<u>另一个局部最优</u></strong>。</p></li></ul><h4 id="5-2-3-TTA测试集数据扩增"><a href="#5-2-3-TTA测试集数据扩增" class="headerlink" title="5.2.3 TTA测试集数据扩增"></a>5.2.3 TTA<font color="#dd0000"><strong><u>测试集</u></strong></font>数据扩增</h4><p><font color="#dd0000"><strong><u>测试集</u></strong></font>数据扩增（Test Time Augmentation，简称TTA）也是常用的集成学习技巧。</p><p>数据扩增<font color="#dd0000">不仅可以在训练时候用，而且可以同样在预测时候进行数据扩增</font>，对同一个样本预测三次，然后对三次结果进行平均。</p><table><thead><tr><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/23.png" alt="IMG"></td><td><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/23_1.png" alt="IMG"></td><td><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/23_2.png" alt="IMG"></td></tr></tbody></table><h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>test_loader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> tta<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>   test_pred_tta <span class="token operator">=</span> None   <span class="token comment" spellcheck="true"># TTA 次数(对测试集做数据扩增)</span>   <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>tta<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">//</span>       test_pred <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>       <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>           <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>input<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>               c0<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> c5 <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>               output <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>c0<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c1<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                  c2<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c3<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                  c4<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c5<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>               test_pred<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>       test_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>test_pred<span class="token punctuation">)</span>       <span class="token keyword">if</span> test_pred_tta <span class="token keyword">is</span> None<span class="token punctuation">:</span>           test_pred_tta <span class="token operator">=</span> test_pred       <span class="token keyword">else</span><span class="token punctuation">:</span>           test_pred_tta <span class="token operator">+=</span> test_pred   <span class="token keyword">return</span> test_pred_tta</code></pre><h2 id="6-结果后的额外处理"><a href="#6-结果后的额外处理" class="headerlink" title="6. 结果后的额外处理"></a>6. 结果后的额外处理</h2><p>在不同的任务中可能会有不同的解决方案，不同思路的模型<strong>不仅可以互相借鉴</strong>，同时也可以<strong>修正</strong>最终的预测结果。</p><p>在本次赛题中，可以从以下几个思路对预测结果进行后处理：</p><ul><li>统计图片中每个位置字符出现的<strong>频率</strong>，使用<strong><u>规则</u>修正结果</strong>；</li><li>单独训练一个<strong>字符长度预测模型</strong>，用来预测图片中<strong>字符个数，并修正结果</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本章是本次赛题学习的最后一章，将会讲解如何使用集成学习提高预测精度&lt;/p&gt;
&lt;h2 id=&quot;学习目标&quot;&gt;&lt;a href=&quot;#学习目标&quot; class=&quot;headerlink&quot; title=&quot;学习目标&quot;&gt;&lt;/a&gt;学习目标&lt;/h2&gt;&lt;p&gt;本章讲解的知识点包括&lt;/p&gt;
&lt;ul&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-04模型训练与验证</title>
    <link href="http://yoursite.com/2020/05/30/datawhale-cv04%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2020/05/30/datawhale-cv04%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/</id>
    <published>2020-05-30T10:56:39.000Z</published>
    <updated>2020-05-30T15:28:12.625Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型训练验证与调优"><a href="#模型训练验证与调优" class="headerlink" title="模型训练验证与调优"></a>模型训练验证与调优</h1><p>在上一章节我们构建了一个简单的CNN进行训练，但这些还远远不够。一个成熟合格的深度学习训练流程至少具备以下功能：</p><ul><li>在训练集上进行训练，并<strong>在验证集上进行验证；</strong></li><li>(模型可以保存最优的权重，并读取权重)</li><li>记录下训练集和验证集的精度，便于调参。</li></ul><h2 id="4-模型训练与验证"><a href="#4-模型训练与验证" class="headerlink" title="4 模型训练与验证"></a>4 模型训练与验证</h2><p>为此本章将从构建验证集、模型训练和验证、模型保存与加载和模型调参几个部分讲解，在部分小节中将会结合Pytorch代码进行讲解。</p><h2 id="4-1-学习目标"><a href="#4-1-学习目标" class="headerlink" title="4.1 学习目标"></a>4.1 学习目标</h2><ul><li>理解验证集的作用，并使用训练集和验证集完成训练</li><li>学会使用Pytorch环境下的模型读取和加载，并了解调参流程</li></ul><h2 id="4-2-why构造验证集"><a href="#4-2-why构造验证集" class="headerlink" title="4.2 why构造验证集"></a>4.2 why构造验证集</h2><p>在机器学习模型（特别是深度学习模型）的训练过程中，模型是<strong>非常容易过拟合的</strong>——模型在不断的训练过程中训练<strong>误差会逐渐降低</strong>，但<font color="#dd0000"><strong>测试误差的走势</strong></font>则<strong>不一定</strong>。</p><p>在训练过程中，模型只能利用训练数据来进行训练，<strong>并不能接触到测试集上的样本</strong>。因此模型如果将训练集学的<font color="#dd0000">过好，模型就会<strong>记住</strong><u>训练样本</u>的<strong>细节</strong>，导致模型<strong>在测试集的泛化效果较差</strong></font>，这种现象称为过拟合（Overfitting）。</p><p>过拟合的大致特征，如图所示：随着模型复杂度和模型训练轮数的增加，CNN模型在训练集上的误差会降低，但在测试集上的误差会逐渐降低，然后逐渐升高。</p><p>[<img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task04/loss.png" alt="IMG">](<a href="https://github.com/datawhalechina/team-learning/blob/master/03" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/blob/master/03</a> 计算机视觉/计算机视觉实践（街景字符编码识别）/IMG/Task04/loss.png)</p><h3 id="导致过拟合的最常见原因"><a href="#导致过拟合的最常见原因" class="headerlink" title="导致过拟合的最常见原因"></a>导致过拟合的最常见原因</h3><ul><li>最为常见的情况是<strong>模型复杂度（Model Complexity ）太高</strong>，导致模型学习了数据的方方面面——包括<strong>过于细枝末节</strong>的规律。</li></ul><p>解决上述问题最好的解决方法：构建一个与测试集<font color="#dd0000"><strong>尽可能分布一致</strong></font>的验证集，在训练过程中不断验证模型在验证集上的精度，并以此控制模型的训练。</p><h3 id="何为数据的分布？"><a href="#何为数据的分布？" class="headerlink" title="何为数据的分布？"></a>何为数据的分布？</h3><p>这里讨论的<strong>数据的分布</strong>一般指的是<font color="#dd0000"><strong>与标签相关的统计分布</strong></font>：比如在分类任务中“分布”指的是标签的<strong>类别分布</strong>，训练集-验证集-测试集的<strong>类别分布情况应该大体一致</strong>；如果标签是带有时序信息，则验证集和测试集的时间间隔应该保持一致。</p><h3 id="验证集主要的2个作用：验证精度和调整超参数；"><a href="#验证集主要的2个作用：验证精度和调整超参数；" class="headerlink" title="验证集主要的2个作用：验证精度和调整超参数；"></a>验证集主要的2个作用：<font color="#dd0000">验证</font>精度和调整<font color="#dd0000">超参数</font>；</h3><p>在给定赛题后，赛题方会<u>给定训练集和测试集<strong>两部分</strong></u>数据。参赛者需要在训练集上面构建模型，并在测试集上面验证模型的泛化能力。</p><p>在不提供验证集时，参赛选手可以自己<strong>在本地划分出一个验证集出来</strong>，进行本地验证。训练集、验证集和测试集分别有不同的作用：</p><ul><li><h4 id="训练集（Train-Set）：模型用于训练和调整模型参数；"><a href="#训练集（Train-Set）：模型用于训练和调整模型参数；" class="headerlink" title="训练集（Train Set）：模型用于训练和调整模型参数；"></a>训练集（Train Set）：模型用于训练和调整模型参数；</h4></li><li><h4 id="验证集（Validation-Set）：用来①验证模型训练后的精度和②调整模型超参数；"><a href="#验证集（Validation-Set）：用来①验证模型训练后的精度和②调整模型超参数；" class="headerlink" title="验证集（Validation Set）：用来①验证模型训练后的精度和②调整模型超参数；"></a>验证集（Validation Set）：用来①<font color="#dd0000">验证</font>模型<u>训练后</u>的精度和②调整模型<font color="#dd0000">超参数</font>；</h4></li><li><h4 id="测试集（Test-Set）：验证模型的泛化能力。"><a href="#测试集（Test-Set）：验证模型的泛化能力。" class="headerlink" title="测试集（Test Set）：验证模型的泛化能力。"></a>测试集（Test Set）：验证模型的泛化能力。</h4></li></ul><p>因为训练集和验证集是分开的，所以模型在验证集上面的精度<strong>在一定程度上可以反映模型的泛化能力</strong>（前提就是，和测试集分布的一致性）——所以划分时，需要<strong>注意验证集的分布应该与测试集尽量保持一致</strong>。</p><h2 id="4-3-如何划分-生成验证集"><a href="#4-3-如何划分-生成验证集" class="headerlink" title="4.3 如何划分 生成验证集"></a>4.3 如何划分 生成验证集</h2><p>既然验证集这么重要，那么如何划分本地验证集呢：<font color="#dd0000"><strong>从训练集中拆分</strong></font>一部分得到验证集。</p><p>具体的有如下几种方式：</p><h4 id="1-留出法（Hold-Out）——最简单直接的"><a href="#1-留出法（Hold-Out）——最简单直接的" class="headerlink" title="1. 留出法（Hold-Out）——最简单直接的"></a>1. 留出法（Hold-Out）——最简单直接的</h4><p>直接将训练集划分成两部分，新的训练集和验证集。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfapeqhp0gj30r00ee0t0.jpg" alt=""></p><ul><li><p>这种划分方式的优点是最为直接简单</p></li><li><p>缺点是只得到了一份验证集，有可能导致模型在验证集上过拟合。</p></li><li><p>场景: <strong>数据量比较大</strong>的情况—随机划分，划分得均匀程度的概率更大</p></li></ul><h4 id="2-交叉验证法（Cross-Validation，CV）"><a href="#2-交叉验证法（Cross-Validation，CV）" class="headerlink" title="2. 交叉验证法（Cross Validation，CV）"></a>2. 交叉验证法（Cross Validation，CV）</h4><p>将训练集划分成K份，将其中的K-1份作为训练集，剩余的1份作为验证集，循环K训练。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfapfbvkqkj30qg0huaaj.jpg" alt=""></p><p>这种划分方式是<strong>所有的训练集<font color="#dd0000">都做过一次验证集</font></strong>，最终模型验证精度是K份平均得到。</p><ul><li>优点: 验证集<font color="#dd0000"><strong>精度比较可靠（所有训练数据都做过验证集）</strong></font>，训练K次可以得到K个有多样性差异的模型；</li><li>缺点: 需要训练K次，加大了训练的量（epoch）</li><li>场景：不适合<strong>数据量很大</strong>的情况——那样划分的频率过高</li></ul><h4 id="3-自助采样法（BootStrap）-略"><a href="#3-自助采样法（BootStrap）-略" class="headerlink" title="3. 自助采样法（BootStrap）(略)"></a>3. 自助采样法（BootStrap）(略)</h4><p>通过<strong>有放回</strong>的<strong>采样方式</strong>得到新的训练集和验证集，每次的训练集和验证集都是有区别的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfapje5ph0j30q40ew74p.jpg" alt=""></p><ul><li>场景：数据量较小</li></ul><h4 id="实际使用时的选择"><a href="#实际使用时的选择" class="headerlink" title="实际使用时的选择"></a>实际使用时的选择</h4><p>这些划分方法是<strong>从数据划分方式的角度</strong>来讲的，在现有的数据比赛中<strong>一般采用</strong>的划分方法是<strong><font color="#dd0000">留出法和交叉验证法</font></strong></p><h2 id="5-模型调优流程"><a href="#5-模型调优流程" class="headerlink" title="5. 模型调优流程"></a>5. 模型调优流程</h2><p>深度学习原理少但实践性非常强，基本上很多的模型的验证只能通过训练来完成。同时深度学习有<strong>众多的网络结构和超参数</strong>，因此需要反复尝试需要较多的训练时间。而如何有效的训练深度学习模型逐渐成为了一门学问。</p><h3 id="5-1-训练技巧"><a href="#5-1-训练技巧" class="headerlink" title="5.1 训练技巧"></a>5.1 训练技巧</h3><p>深度学习有众多的训练技巧，本节挑选了常见的一些技巧来讲解，并针对本次赛题进行具体分析。</p><p>与传统的机器学习模型不同，深度学习模型的精度与模型的<strong>复杂度、数据量、正则化、数据扩增等因素</strong>直接相关。</p><p>所以当深度学习模型处于不同的阶段（欠拟合、过拟合和完美拟合）的情况下，大家可以知道可以什么角度来继续优化模型。</p><p>在参加本次比赛的过程中，我建议大家以如下逻辑完成：</p><ul><li>1.初步构建简单的CNN模型，不用特别复杂，跑通训练、验证和预测的流程；</li><li>2.此时简单CNN模型的损失会比较大，尝试<strong>增加模型复杂度</strong>，并观察验证集精度；</li><li>3.在增加模型复杂度的<strong>同时增加数据扩增方法</strong>，<font color="#dd0000">直至<strong>验证集精度不变</strong></font>。</li></ul><p>对应如下图</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task04/%E8%B0%83%E5%8F%82%E6%B5%81%E7%A8%8B.png" alt="IMG"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;模型训练验证与调优&quot;&gt;&lt;a href=&quot;#模型训练验证与调优&quot; class=&quot;headerlink&quot; title=&quot;模型训练验证与调优&quot;&gt;&lt;/a&gt;模型训练验证与调优&lt;/h1&gt;&lt;p&gt;在上一章节我们构建了一个简单的CNN进行训练，但这些还远远不够。一个成熟合格的深度学
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Hexo指令的使用和经历的坑</title>
    <link href="http://yoursite.com/2020/05/26/hexo%E6%8C%87%E4%BB%A4%E4%BD%BF%E7%94%A8%E7%BB%8F%E5%8E%86/"/>
    <id>http://yoursite.com/2020/05/26/hexo%E6%8C%87%E4%BB%A4%E4%BD%BF%E7%94%A8%E7%BB%8F%E5%8E%86/</id>
    <published>2020-05-26T04:52:21.000Z</published>
    <updated>2020-05-25T17:04:51.031Z</updated>
    
    <content type="html"><![CDATA[<p>（为了防止自己忘记，而万一有下一次，所以当日记记下）</p><p><a href="https://hexo.io/zh-cn/docs/commands.html" target="_blank" rel="noopener">官网中文解说</a></p><h2 id="亲历-用Hexo命令解决搭建博客的坑"><a href="#亲历-用Hexo命令解决搭建博客的坑" class="headerlink" title="亲历: 用Hexo命令解决搭建博客的坑"></a>亲历: 用Hexo命令解决搭建博客的<font color="#dd0000">坑</font></h2><h3 id="1-hexo-clean"><a href="#1-hexo-clean" class="headerlink" title="1. hexo clean"></a>1. hexo clean</h3><p>清除缓存文件 (<code>db.json</code>) 和<strong>已生成的静态文件 (<code>public</code>)</strong>——就是hexo d<strong>推送到GitHub并显示到前端的<font color="#dd0000">网页静态源码</font></strong>。</p><p><strong>Case</strong>: 在某些情况（尤其是<strong>编辑过主题后</strong>）， 如果发现您<strong>对站点的<font color="#dd0000">更改无论如何也不生效</font>——因为<code>hexo g</code>并<u>不会</u>将全本地的<font color="#dd0000">生成的静态文件</font>和<font color="#dd0000">现有的<code>public</code></font>路径的文件<font color="#dd0000">一一对比，而是部分对比</font>，来更新后者</strong>——此时，可能需要运行该命令<strong>重新生成网页源码</strong>。</p><h3 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h3><p>to be coninue…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;（为了防止自己忘记，而万一有下一次，所以当日记记下）&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hexo.io/zh-cn/docs/commands.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网中文解说&lt;/a&gt;&lt;/p&gt;
&lt;h2 id
      
    
    </summary>
    
    
    
      <category term="走过的坑" scheme="http://yoursite.com/tags/%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-03字符识别模型</title>
    <link href="http://yoursite.com/2020/05/25/datawhale-cv03%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/05/25/datawhale-cv03%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-05-25T17:24:13.000Z</published>
    <updated>2020-05-26T16:08:30.975Z</updated>
    
    <content type="html"><![CDATA[<p>前面的章节，我们讲解了赛题的背景知识和赛题数据的读取。 本文的任务是: 基于对赛题理解本章将构建一个定长多字符分类模型, 构建一个<strong>CNN类</strong>的定长字符识别模型。</p><h2 id="3-字符识别模型"><a href="#3-字符识别模型" class="headerlink" title="3 字符识别模型"></a>3 字符识别模型</h2><p>本章将会讲解卷积神经网络（Convolutional Neural Network, CNN）的常见层，并从头搭建一个字符识别模型。</p><h3 id="3-1-学习目标"><a href="#3-1-学习目标" class="headerlink" title="3.1 学习目标"></a>3.1 学习目标</h3><ul><li>学习CNN基础和原理</li><li>使用Pytorch框架构建CNN模型，并完成训练</li></ul><h3 id="3-2-CNN介绍"><a href="#3-2-CNN介绍" class="headerlink" title="3.2 CNN介绍"></a>3.2 CNN介绍</h3><p>卷积神经网络（简称CNN）是一类特殊的人工神经网络，是深度学习中重要的一个分支。</p><h4 id="为什么CV方面用CNN？"><a href="#为什么CV方面用CNN？" class="headerlink" title="为什么CV方面用CNN？"></a>为什么CV方面用CNN？</h4><p>CNN在很多领域都表现优异，精度和速度比传统计算学习算法高很多。<strong>特别是在计算机视觉领域</strong>，CNN是解决图像分类、图像检索、物体检测和语义分割的主流模型。</p><h4 id="常见结构组成"><a href="#常见结构组成" class="headerlink" title="常见结构组成"></a>常见结构组成</h4><p>CNN每一层由众多的卷积核组成，每个卷积核对输入的像素进行<strong>卷积操作</strong>，得到下一次的输入。随着网络层的增加卷积核会逐渐扩大感受野，并缩减图像的尺寸。</p><p>CNN是一种<u><strong>层次</strong></u>模型: </p><h5 id="1-Input"><a href="#1-Input" class="headerlink" title="1. Input"></a>1. Input</h5><p>输入的是原始的<strong>像素数据</strong>。</p><h5 id="2-中间处理计算"><a href="#2-中间处理计算" class="headerlink" title="2. 中间处理计算"></a>2. 中间处理计算</h5><p>CNN通过<strong>卷积（convolution）、池化（pooling）、<u>非线性</u>激活函数（non-linear activation function）和全连接层（fully connected layer）</strong>构成。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task03/%E5%8D%B7%E7%A7%AF.png" alt="卷积过程"></p><h5 id="3-Output"><a href="#3-Output" class="headerlink" title="3. Output"></a>3. Output</h5><p>通过多次卷积和池化，CNN的<strong>最后一层将输入的图像像素映射为具体的输出</strong>：如在分类任务中会转换为不同类别的<strong>概率输出</strong>，</p><h5 id="4-学习过程-优化参数-——反向传播"><a href="#4-学习过程-优化参数-——反向传播" class="headerlink" title="4. 学习过程(优化参数)——反向传播"></a>4. 学习过程(优化参数)——反向传播</h5><p>然后计算真实标签与CNN模型的输出的预测结果的<strong>差异</strong>，并通过<strong>反向传播更新每层的参数</strong>；在更新完成<strong>后再次前向传播</strong>，如此反复直到训练完成 。</p><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task03/Le_CNN.png" alt="经典的字符识别模型"></p><h3 id="3-3-模型的优势特点"><a href="#3-3-模型的优势特点" class="headerlink" title="3.3 模型的优势特点"></a>3.3 模型的优势特点</h3><p>传统机器学习相比，CNN, 或者说<font color="#dd0000"><strong>深度学习模型</strong>(各种<strong><u>深度神经网络</u></strong>)</font>，具有一种<strong><font color="#dd0000">端到端（End to End）的优势</font></strong>：模型训练的过程中是<strong><font color="#dd0000">直接</font></strong>从<u>输入</u>图像像素到<u>最终的输出</u>分类结果——<font color="#dd0000">并<strong>不涉及</strong>到具体的<strong>特征提取</strong>和构建模型的过程</font>，也不需要<strong>人工的</strong>参与。</p><h3 id="3-4-实战：Pytorch构建CNN模型"><a href="#3-4-实战：Pytorch构建CNN模型" class="headerlink" title="3.4 实战：Pytorch构建CNN模型"></a>3.4 实战：Pytorch构建CNN模型</h3><p>在Pytorch中构建CNN模型非常简单，<strong>只需要定义</strong>好模型的<strong>参数</strong>和<strong>正向传播函数</strong>即可，<font color="#dd0000">Pytorch会根据正向传播<strong>自动计算反向传播</strong></font>！！</p><h4 id="1-定义模型"><a href="#1-定义模型" class="headerlink" title="1. 定义模型"></a>1. 定义模型</h4><pre class=" language-Python"><code class="language-Python"># 定义模型: 这个CNN模型包括两个卷积层，最后并联6个全连接层进行分类class SVHN_Model1(nn.Module):    # 构造器：1. 只需要定义好模型参数    def __init__(self):        super(SVHN_Model1, self).__init__()        # CNN提取特征模块        self.cnn = nn.Sequential(            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),              nn.MaxPool2d(2),            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),            nn.ReLU(),             nn.MaxPool2d(2),        )        #         self.fc1 = nn.Linear(32*3*7, 11)        self.fc2 = nn.Linear(32*3*7, 11)        self.fc3 = nn.Linear(32*3*7, 11)        self.fc4 = nn.Linear(32*3*7, 11)        self.fc5 = nn.Linear(32*3*7, 11)        self.fc6 = nn.Linear(32*3*7, 11)        # 2. 只需要定义好模型正向传播即可，Pytorch会根据正向传播自动计算反向传播。    def forward(self, img):                feat = self.cnn(img)        feat = feat.view(feat.shape[0], -1)        c1 = self.fc1(feat)        c2 = self.fc2(feat)        c3 = self.fc3(feat)        c4 = self.fc4(feat)        c5 = self.fc5(feat)        c6 = self.fc6(feat)        return c1, c2, c3, c4, c5, c6# 构造一个模型对象model = SVHN_Model1()</code></pre><h4 id="2-使用模型"><a href="#2-使用模型" class="headerlink" title="2. 使用模型"></a>2. 使用模型</h4><p>在训练之前，需要定义好</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 损失函数</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 优化器</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.005</span><span class="token punctuation">)</span></code></pre><p>然后就是训练： 过程包括，Pytorch<strong>自动计算</strong>反向传播，让模型真正的实现<font color="#dd0000">“<strong>自我学习</strong>”</font></p><pre class=" language-python"><code class="language-python">loss_plot<span class="token punctuation">,</span> c0_plot <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 迭代10个Epoch</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>        c0<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> c5 <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>c0<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c1<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c2<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c3<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c4<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> \                criterion<span class="token punctuation">(</span>c5<span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        loss <span class="token operator">/=</span> <span class="token number">6</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss_plot<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        c0_plot<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>c0<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1.0</span> <span class="token operator">/</span> c0<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span></code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面的章节，我们讲解了赛题的背景知识和赛题数据的读取。 本文的任务是: 基于对赛题理解本章将构建一个定长多字符分类模型, 构建一个&lt;strong&gt;CNN类&lt;/strong&gt;的定长字符识别模型。&lt;/p&gt;
&lt;h2 id=&quot;3-字符识别模型&quot;&gt;&lt;a href=&quot;#3-字符识别模型
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-02数据读取与数据扩增</title>
    <link href="http://yoursite.com/2020/05/23/datawhale-cv02/"/>
    <id>http://yoursite.com/2020/05/23/datawhale-cv02/</id>
    <published>2020-05-23T04:39:17.000Z</published>
    <updated>2020-05-23T13:52:38.634Z</updated>
    
    <content type="html"><![CDATA[<p>在上一章节，官方提供了三种不同的解决方案。从本章开始 将逐渐的学习使用<strong>【定长字符识别】思路</strong>来构建模型，讲解赛题的解决方案和相应知识点。</p><h2 id="2-数据读取与数据扩增"><a href="#2-数据读取与数据扩增" class="headerlink" title="2 数据读取与数据扩增"></a>2 数据读取与数据扩增</h2><h3 id="2-1-学习目标"><a href="#2-1-学习目标" class="headerlink" title="2.1 学习目标"></a>2.1 学习目标</h3><p>本章主要内容为<strong>图像数据读取、数据扩增方法</strong>和<strong>实战Pytorch读取赛题数据</strong>三个部分组成。</p><ul><li>学习Python和Pytorch中图像读取</li><li>学会扩增方法和实战Pytorch读取赛题数据</li></ul><h3 id="2-2-图像读取"><a href="#2-2-图像读取" class="headerlink" title="2.2 图像读取"></a>2.2 图像读取</h3><p>在识别之前，首先需要完成<strong>对数据的读取操作</strong>。在Python中有很多库可以完成数据读取的操作，比较常见的有<strong>Pillow和OpenCV</strong>。</p><h4 id="2-2-1-Pillow"><a href="#2-2-1-Pillow" class="headerlink" title="2.2.1 Pillow"></a>2.2.1 Pillow</h4><p>Pillow是Python图像<strong>处理函式库(PIL）</strong>的一个分支。Pillow提供了常见的图像读取和处理的操作。</p><h4 id="2-2-2-OpenCV"><a href="#2-2-2-OpenCV" class="headerlink" title="2.2.2 OpenCV"></a>2.2.2 OpenCV</h4><p>OpenCV是一个跨平台的<strong>计算机视觉库</strong>。OpenCV发展的非常早，拥有<strong>众多的计算机视觉、数字图像处理和机器视觉等功能</strong>。OpenCV在功能上<font color="#dd0000"><strong>比Pillow更加强大很多，但学习成本也高很多</strong></font>。</p><h3 id="2-3-数据扩增"><a href="#2-3-数据扩增" class="headerlink" title="2.3 数据扩增"></a>2.3 数据扩增</h3><p>现在回到赛题街道字符识别任务中。在赛题中我们需要对的图像进行字符识别，因此需要我们完成的数据的读取操作，同时也需要完成<strong>数据扩增（Data Augmentation）操作</strong>。</p><h4 id="2-3-1-基本介绍"><a href="#2-3-1-基本介绍" class="headerlink" title="2.3.1 基本介绍"></a>2.3.1 基本介绍</h4><p>在深度学习中数据扩增方法非常重要，数据扩增可以增加训练集的样本，同时也可以有效缓解模型过拟合的情况，也可以给模型带来的更强的泛化能力。</p><p>已知，在深度学习模型的训练过程中，数据扩增是<font color="#dd0000">必不可少的环节</font>。</p><ul><li><h4 id="数据扩增为什么有用？"><a href="#数据扩增为什么有用？" class="headerlink" title="数据扩增为什么有用？"></a><font color="#dd0000">数据扩增为什么有用？</font></h4></li></ul><ol><li><p>现有深度学习的参数非常多，一般的模型可训练的<font color="#dd0000"><strong>参数量基本上都是万到百万级别，而训练集样本的数量很难有这么多</strong></font>。</p></li><li><p>其次数据扩增可以<font color="#dd0000"> <strong>扩展样本空间</strong></font>：假设现在的分类模型需要对汽车进行分类</p><p>如果<strong>不使用任何数据扩增方法</strong>，深度学习模型会<strong>从汽车车头的角度❌</strong>来进行判别，<strong>而不是汽车具体的区别✅</strong>。</p></li></ol><p><img src="https://github.com/datawhalechina/team-learning/raw/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%A1%97%E6%99%AF%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%86%E5%88%AB%EF%BC%89/IMG/Task02/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9Ecar.png" alt="汽车分类"></p><h4 id="2-3-2-常见的数据扩增方法"><a href="#2-3-2-常见的数据扩增方法" class="headerlink" title="2.3.2 常见的数据扩增方法"></a>2.3.2 常见的数据扩增方法</h4><ul><li><h4 id="有哪些数据扩增方法？"><a href="#有哪些数据扩增方法？" class="headerlink" title="有哪些数据扩增方法？"></a>有哪些数据扩增方法？</h4></li></ul><p>数据扩增方法有很多：<font color="#dd0000">从<strong>颜色空间、尺度空间到样本空间</strong>，同时根据不同任务数据扩增都有相应的区别</font>。</p><blockquote><p>对于图像分类，数据扩增一般<font color="#dd0000"><strong>不会改变标签</strong></font>;(即本比赛的<strong>需求</strong>场景)</p><p>对于物体检测，数据扩增会改变物体坐标位置；</p><p>对于图像分割，数据扩增会改变像素标签。</p></blockquote><p><strong>Note</strong>: 在本次赛题中，赛题任务是需要对图像中的字符进行识别，因此对于<strong><font color="#dd0000">字符图片并不能进行翻转操作</font>。比如字符6经过水平翻转就变成了字符9</strong>，<font color="#dd0000"><strong>会改变字符原本的含义</strong></font></p><ul><li><h4 id="具体常用的方法和数据扩增库"><a href="#具体常用的方法和数据扩增库" class="headerlink" title="具体常用的方法和数据扩增库"></a><strong>具体</strong>常用的方法和数据扩增<u>库</u></h4></li></ul><p>在常见的数据扩增方法中，一般会从<strong>图像颜色、尺寸、形态、空间和像素等角度</strong>进行变换。当然不同的数据扩增方法可以自由进行组合，得到更加丰富的数据扩增方法。</p><p>以<strong><a href="https://pytorch.org/docs/stable/torchvision/index.html" target="_blank" rel="noopener">torchvision</a></strong>(pytorch官方提供的数据扩增库，提供了基本的数据数据扩增方法，可以<u>无缝与torch进行集成</u>；但<u>数据扩增方法种类较少，且速度中等</u>)为例，常见的数据扩增方法（API）包括：</p><pre class=" language-python"><code class="language-python"><span class="token operator">-</span> transforms<span class="token punctuation">.</span>CenterCrop 对图片中心进行裁剪<span class="token operator">-</span> transforms<span class="token punctuation">.</span>ColorJitter 对图像颜色的对比度、饱和度和零度进行变换<span class="token operator">-</span> transforms<span class="token punctuation">.</span>FiveCrop 对图像四个角和中心进行裁剪得到五分图像<span class="token operator">-</span> transforms<span class="token punctuation">.</span>Grayscale 对图像进行灰度变换<span class="token operator">-</span> transforms<span class="token punctuation">.</span>Pad 使用固定值进行像素填充<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomAffine 随机仿射变换<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomCrop 随机区域裁剪<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomHorizontalFlip 随机水平翻转<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomRotation 随机旋转<span class="token operator">-</span> transforms<span class="token punctuation">.</span>RandomVerticalFlip 随机垂直翻转</code></pre><p>除了torchvision，还有速度<strong>更快的第三方扩增库</strong>供选择：</p><ol><li><p><a href="https://github.com/aleju/imgaug" target="_blank" rel="noopener">imgaug</a> 提供了多样的数据扩增方法，且组合起来非常方便，速度较快；</p></li><li><p><a href="https://albumentations.readthedocs.io" target="_blank" rel="noopener">albumentations</a> 提供了多样的数据扩增方法，对图像分类、语义分割、物体检测和关键点检测都支持，速度较快。</p></li></ol><h2 id="2-4-Pytorch读取数据"><a href="#2-4-Pytorch读取数据" class="headerlink" title="2.4 Pytorch读取数据"></a>2.4 Pytorch读取数据</h2><p>由于本次赛题我们使用Pytorch框架讲解具体的解决方案，接下来将是解决赛题的<strong>第一步</strong>使用<strong>Pytorch读取赛题数据</strong>。</p><h4 id="2-4-1-一些定义-写代码前想好大致逻辑"><a href="#2-4-1-一些定义-写代码前想好大致逻辑" class="headerlink" title="2.4.1 一些定义(写代码前想好大致逻辑)"></a>2.4.1 一些定义(写代码前想好大致逻辑)</h4><p>首先，<strong>区分</strong>Dataset和DataLoder这两个<u>数据处理的常用术语</u> 和 <strong>解释</strong>有了Dataset为什么还要有DataLoder？</p><p>其实这两个是两个不同的概念，是为了<strong>实现不同的功能</strong>。</p><ul><li>Dataset：对<font color="#dd0000"><strong>数据集的封装</strong></font>，提供索引方式的对数据样本进行读取</li><li>DataLoder：对<font color="#dd0000"><strong>Dataset进行封装</strong></font>，提供批量读取的迭代读取</li></ul><p>而<font color="#dd0000"><strong>在Pytorch中的数据读取逻辑</strong></font>， 数据①先<strong>通过Dataset进行封装</strong>，②再<u><strong>通过DataLoder进行并行读取</strong></u>。</p><h4 id="2-4-2-代码"><a href="#2-4-2-代码" class="headerlink" title="2.4.2 代码"></a>2.4.2 代码</h4><h5 id="Step①定义对数据集封装的Dataset-详情，见注释"><a href="#Step①定义对数据集封装的Dataset-详情，见注释" class="headerlink" title="Step①定义对数据集封装的Dataset(详情，见注释)"></a>Step①定义对数据集封装的Dataset(详情，见注释)</h5><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 省略各种import </span><span class="token keyword">class</span> <span class="token class-name">SVHNDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># constructor</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img_path<span class="token punctuation">,</span> img_label<span class="token punctuation">,</span> transform<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>img_path <span class="token operator">=</span> img_path        self<span class="token punctuation">.</span>img_label <span class="token operator">=</span> img_label         <span class="token keyword">if</span> transform <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>transform <span class="token operator">=</span> None    <span class="token comment" spellcheck="true"># getter: 因为Dataset是提供【索引方式】的对数据样本进行读取</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        img <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_path<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 原始SVHN中类别10为数字0</span>        lbl <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_label<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int<span class="token punctuation">)</span>        lbl <span class="token operator">=</span> list<span class="token punctuation">(</span>lbl<span class="token punctuation">)</span>  <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">5</span> <span class="token operator">-</span> len<span class="token punctuation">(</span>lbl<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> img<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>lbl<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_path<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取: 图片数据和label的路径</span>train_path <span class="token operator">=</span> glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span><span class="token string">'../input/train/*.png'</span><span class="token punctuation">)</span>train_path<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>train_json <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>open<span class="token punctuation">(</span><span class="token string">'../input/train.json'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>train_label <span class="token operator">=</span> <span class="token punctuation">[</span>train_json<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> train_json<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 定义数据集实例</span>data <span class="token operator">=</span> SVHNDataset<span class="token punctuation">(</span>train_path<span class="token punctuation">,</span> train_label<span class="token punctuation">,</span>          transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>              <span class="token comment" spellcheck="true"># 缩放到固定尺寸</span>              transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token comment" spellcheck="true">########################## 数据扩增 ##########################</span>              <span class="token comment" spellcheck="true"># 随机颜色变换</span>              transforms<span class="token punctuation">.</span>ColorJitter<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 加入随机旋转</span>              transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 将图片转换为pytorch 的tesntor</span>              transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 对图像像素进行归一化</span>                            transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span><span class="token number">0.456</span><span class="token punctuation">,</span><span class="token number">0.406</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span><span class="token number">0.224</span><span class="token punctuation">,</span><span class="token number">0.225</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h5 id="Step②定义对Dataset封装的DataLoader"><a href="#Step②定义对Dataset封装的DataLoader" class="headerlink" title="Step②定义对Dataset封装的DataLoader"></a>Step②定义对Dataset封装的DataLoader</h5><p>加入DataLoder：数据按照<strong>批次(batch_size=10)</strong>获取，每批次调用Dataset读取单个样本进行拼接。</p><p>此时data的格式为：<code>torch.Size([10, 3, 64, 128]), torch.Size([10, 6])</code>。</p><p>前者为图像文件，为batchsize * chanel * height * width次序；后者为字符标签。</p><pre class=" language-python"><code class="language-python">train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>data<span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 封装上面的dataset即可</span>    batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 每批样本个数</span>    shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 是否打乱顺序</span>    num_workers<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 读取的线程个数</span><span class="token punctuation">)</span><span class="token keyword">for</span> data <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 后续操作...</span></code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一章节，官方提供了三种不同的解决方案。从本章开始 将逐渐的学习使用&lt;strong&gt;【定长字符识别】思路&lt;/strong&gt;来构建模型，讲解赛题的解决方案和相应知识点。&lt;/p&gt;
&lt;h2 id=&quot;2-数据读取与数据扩增&quot;&gt;&lt;a href=&quot;#2-数据读取与数据扩增&quot; clas
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>datawhale-cv训练营-01赛题研究</title>
    <link href="http://yoursite.com/2020/05/18/datawhale-cv01/"/>
    <id>http://yoursite.com/2020/05/18/datawhale-cv01/</id>
    <published>2020-05-18T16:53:16.000Z</published>
    <updated>2020-05-23T10:45:24.082Z</updated>
    
    <content type="html"><![CDATA[<p>这次<strong>基础</strong>赛事, 是Datawhale与天池联合发起的零基础<strong>入门系列</strong>赛事 <a href="https://tianchi.aliyun.com/competition/entrance/531795/introduction" target="_blank" rel="noopener">赛事地址</a></p><h1 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h1><ol><li><strong>总结</strong>基本了解比赛规则</li><li><strong>总结</strong>解题思路</li><li>数据下载和<strong>理解</strong></li></ol><h1 id="1-规则"><a href="#1-规则" class="headerlink" title="1.规则"></a>1.规则</h1><p>本赛题需要选手识别图片中所有的字符。<strong>评测指标</strong>：准确率，Score=编码识别正确的数量/测试集图片数量</p><p>为了降低比赛难度，我们提供了训练集、验证集中所有字符的<strong>位置框</strong>（在<strong>阿里天池</strong>上下载）。</p><p><strong>注意</strong>: 按照比赛规则，所有的参赛选手，<strong>只能使用比赛给定的数据集完成训练(不要使用SVHN原始数据集进行训练</strong>）</p><h4 id="使用的Python模块"><a href="#使用的Python模块" class="headerlink" title="使用的Python模块"></a>使用的Python模块</h4><p>大概介绍一下，这里可能需要用到的主要模块。</p><blockquote><p>numpy ：提供了python对多维数组对象的支持：ndarray，具有矢量运算能力，快速、节省空间。numpy支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。</p><p>torch：神经网络界的 Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算. 所以神经网络的话, 当然是用 Torch 的 tensor 形式数据最好</p><p>torchvision：torchvision包是服务于pytorch深度学习框架的,用来生成图片,视频数据集,和一些流行的模型类和预训练模型。我认为这个是最关键的模块</p><p>OpenCV（import时候是cv2）：一款强大的跨平台的计算机视觉库，使用它能完成我们对于图像和视频处理的很多功能。它以电信号的方式加以捕捉、记录、处理、储存、传送与重现的各种技术。这里主要是用来对图片的处理</p><p>json：这个就是json的读写库，处理json文件的</p></blockquote><h1 id="2-数据理解"><a href="#2-数据理解" class="headerlink" title="2. 数据理解"></a>2. 数据理解</h1><h4 id="数据集初步观察"><a href="#数据集初步观察" class="headerlink" title="数据集初步观察"></a>数据集初步观察</h4><p>分.json的label位置信息，和原图集合</p><p><strong>数据读取</strong>： json文件包含的位置信息，除了便于正式的训练，还可以用于数据观察——直接作用在原图集，<strong>查看已给的位置信息的分割效果</strong></p><p>样例代码: 数据读取，在此我们给出JSON中标签的读取方式</p><pre><code>import jsontrain_json = json.load(open(&#39;../input/train.json&#39;))# 数据标注处理def parse_json(d):   arr = np.array([       d[&#39;top&#39;], d[&#39;height&#39;], d[&#39;left&#39;],  d[&#39;width&#39;], d[&#39;label&#39;]   ])   arr = arr.astype(int)   return arrimg = cv2.imread(&#39;../input/train/000000.png&#39;)arr = parse_json(train_json[&#39;000000.png&#39;])plt.figure(figsize=(10, 10))plt.subplot(1, arr.shape[1]+1, 1)plt.imshow(img)plt.xticks([]); plt.yticks([])for idx in range(arr.shape[1]):   plt.subplot(1, arr.shape[1]+1, idx+2)   plt.imshow(img[arr[0, idx]:arr[0, idx]+arr[1, idx],arr[2, idx]:arr[2, idx]+arr[3, idx]])   plt.title(arr[4, idx])   plt.xticks([]); plt.yticks([])</code></pre><p>输出</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez59nbvuij30z30u0gtm.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez59nbvuij30z30u0gtm.jpg" alt="img"></a></p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez5ab60sij30yw0oaqc0.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gez5ab60sij30yw0oaqc0.jpg" alt="img"></a></p><h1 id="3-潜在的疑难杂症"><a href="#3-潜在的疑难杂症" class="headerlink" title="3. 潜在的疑难杂症"></a>3. 潜在的疑难杂症</h1><h4 id="预处理细节"><a href="#预处理细节" class="headerlink" title="预处理细节"></a>预处理细节</h4><p>数据集存在原图片<strong>大小不统一</strong>，这个只需要用pytorch的transforms处理即可</p><h4 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h4><h5 id="a-确定待识别数字在图中的位置"><a href="#a-确定待识别数字在图中的位置" class="headerlink" title="a.确定待识别数字在图中的位置"></a>a.确定待识别数字在图中的位置</h5><p>(使用比赛简化后的数据，则该问题并不存在了)</p><p>在简化数据集之前的难点是：模型要能找到待识别数字的<strong>位置</strong>。但是既然处理后的数据集，<strong>位置信息全都提供了</strong>，那么这个问题就容易很多——<strong>单纯的识别数字信息</strong>。数字的位数问题可以通过简单的算法来解决，就像MNIST数据集一样。</p><p><a href="https://crazy-winds.github.io/images/cv1/0-1.png" target="_blank" rel="noopener"><img src="https://crazy-winds.github.io/images/cv1/0-1.png" alt="图1"></a>图1</p><p>（json格式存储label和位置信息）</p><p><a href="https://crazy-winds.github.io/images/cv1/0-2.png" target="_blank" rel="noopener"><img src="https://crazy-winds.github.io/images/cv1/0-2.png" alt="图2"></a>图2</p><h5 id="b-确定待识别数字的个数"><a href="#b-确定待识别数字的个数" class="headerlink" title="b.确定待识别数字的个数"></a>b.确定待识别数字的个数</h5><p>即每幅图的数字个数可能均不相同，如何统一的解决(找到一种general的方法)</p><p>将在解题思路部分详细展开</p><h1 id="4-大致解题思路"><a href="#4-大致解题思路" class="headerlink" title="4. 大致解题思路"></a>4. 大致解题思路</h1><ol><li>简单入门思路：定长字符识别。将不定长字符转化为定长处理，不足部分用<strong>填充占位符</strong>为代替</li></ol><p>（<strong>适合新手</strong>也<strong>适合此题给的处理后的数据集</strong>：赛题数据集中大部分图像中字符个数为2-4个，最多的字符 个数为<strong>6个</strong>）</p><p><a href="https://crazy-winds.github.io/images/cv1/0-3.png" target="_blank" rel="noopener"><img src="https://crazy-winds.github.io/images/cv1/0-3.png" alt="图3"></a>图3</p><ol><li>专业字符识别思路：按照<strong>不定长字符</strong>处理</li></ol><p>在字符识别研究中，有<strong>特定的方法</strong>来解决此种不定长的字符识别问题：如<strong>典型的有CRNN字符识别模型</strong>。</p><p>因为本次赛题中给定的图像数据都<strong>比较规整，可以视为一个单词或者一个句子</strong> 喂进CRNN模型。</p><ol><li>专业分类思路：检测位置再识别数字</li></ol><p>在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用<strong>物体检测</strong>的思路完成。</p><p>可参考物体检测模型：<strong>SSD或者YOLO</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这次&lt;strong&gt;基础&lt;/strong&gt;赛事, 是Datawhale与天池联合发起的零基础&lt;strong&gt;入门系列&lt;/strong&gt;赛事 &lt;a href=&quot;https://tianchi.aliyun.com/competition/entrance/531795/int
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>NLP-intro</title>
    <link href="http://yoursite.com/2020/01/29/NLP-intro/"/>
    <id>http://yoursite.com/2020/01/29/NLP-intro/</id>
    <published>2020-01-29T05:30:38.000Z</published>
    <updated>2020-07-23T08:04:14.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-NLP-Overview"><a href="#0-NLP-Overview" class="headerlink" title="0. NLP Overview"></a>0. NLP Overview</h2><h3 id="a-按照研究方向-语言的I-O"><a href="#a-按照研究方向-语言的I-O" class="headerlink" title="a. 按照研究方向~语言的I/O"></a>a. 按照<u>研究</u>方向~语言的I/O</h3><p>2个大方向对应了，语言的<strong>“一进一出”</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf99ljg3ooj30v80hswh7.jpg" style="zoom:35%;" /><h3 id="b-按照现实中的业务大致分成2大类"><a href="#b-按照现实中的业务大致分成2大类" class="headerlink" title="b. 按照现实中的业务大致分成2大类"></a>b. 按照<u><font color="#dd0000">现实中的业务</font></u>大致分成2大类</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0xhkrae0j31440jwna9.jpg" alt="2大类常见业务" style="zoom:35%;" /><h4 id="文本生成任务-文本序列→文本序列"><a href="#文本生成任务-文本序列→文本序列" class="headerlink" title="文本生成任务: 文本序列→文本序列"></a>文本生成任务: <strong>文本序列→文本序列</strong></h4><p>比如机器翻译，文本风格迁移等，</p><h4 id="类别识别任务-文本序列→类别"><a href="#类别识别任务-文本序列→类别" class="headerlink" title="类别识别任务: 文本序列→类别"></a>类别识别任务: 文本序列→类别</h4><p>比如情感分类，实体命名识别，主题分类，槽位填充等。</p><h5 id="输入：多个序列"><a href="#输入：多个序列" class="headerlink" title="输入：多个序列"></a>输入：多个序列</h5><p>多个序列要怎么办呢？大概两种可能的解法。</p><p>一是把两个序列，分别用<strong>两个模型</strong>去做编码。再把它们编码后的嵌入，丢给另一个整合的模块，去得到最终的输出。有时，我们也会在两个模型之间加 Attention，确保二者的编码内容能互相意识。</p><p>二是，近年来比较流行的做法是直接<strong>把两个句子连接起来，中间加一个特殊的字符</strong>，如 <u><strong>BERT</strong></u> 里面的 <SEP>，来提示模型去意识到这是两个句子的分隔符(接起来的序列丢给模型后可以预测下游任务)</p><h4 id="其他-这2大类的变体"><a href="#其他-这2大类的变体" class="headerlink" title="其他: 这2大类的变体"></a>其他: 这2大类的变体</h4><ul><li><p><strong>Word Segmentations 分词</strong></p><p>英文词汇有<strong>空格符</strong>分割词的边界，但有的语言，如<strong>中文，却没有类似的方式来区分</strong>，所以需要<strong>额外的分词处理</strong>。在一个句子中找出词的边界有时并不是一个简单的问题，所以也需要模型来做。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0xppt5gbj311r0u0wpj.jpg" style="zoom:25%;" /></li><li><p><strong>POS Tagging 词性标注</strong>: Part of Speech <strong>词性</strong>分析(词性是很重要的特征)</p><p> 需要标记出一个句子中的每个词的词性是什么。对应输入一个序列，输出序列每个位置的<strong>类别任务</strong>。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0xotv23zj31020jsthy.jpg" style="zoom:33%;" /></li><li><p><strong>Pre-Processing work</strong> 自然语言理解的<strong>前处理</strong>的任务</p><ul><li><p><strong>Parsing 句法分析</strong></p><p>给定句子，根据<u>词性</u>生成<strong>树状结构</strong> by CYK算法</p></li><li><p><strong>Dependency Parsing</strong>(词的)依存分析 </p><p>生成<strong>graph</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0y22yxs4j31200ncwll.jpg" style="zoom:33%;" /></li><li><p><strong>Coreference Resolution 指代消解</strong></p><p>模型需要把输入文章中<strong>指代同样东西的部分，找出来</strong>。比如当He 和 Paul Allen 指的就是同一个人。</p></li></ul></li><li><p><strong>Summarization 生成摘要</strong></p><p>分成2种</p><ul><li><p>过去常用的是<strong>抽取式摘要 Extractive</strong></p><p>把一篇文档看作是许多句子的组成的序列，模型需要从中找出最能熔炼文章大意的句子提取出来作为输出。它相当于是对每个句子做一个二分类，来决定它要不要放入摘要中。但仅仅把每个句子分开来考虑是不够的。我们需要模型输入整篇文章后，再决定哪个句子更重要。这个序列的基本单位是一个句子的表征</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0xx1vlv0j314a0rkqcu.jpg" style="zoom:33%;" /></li><li><p>近年流行的<strong>生成式摘要 Abstractive</strong></p><p>模型的输入是一段长文本，输出是短文本。输出的短文本往往会与输出的长文本<strong>有很多共用的词汇</strong>。这就需要模型在生成的过程中有, 把文章中重要词汇拷贝出来再放到输出中的, 复制能力，比如 Pointer Network。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0xy8wzhhj313y0pgaio.jpg" style="zoom:33%;" /></li></ul></li><li><p><strong>NER：Named Entity Recognition 命名实体识别</strong>——<strong><u>抽取</u></strong>语句中我<strong>重点关注</strong>的<strong>名词</strong></p><p>并没有非常清楚的定义：它取决于我们<strong>对哪些事情关注</strong>，随着领域的不同，有所差异，取决于我们的<strong>具体应用场景</strong>。一般的实体包括人名、组织和地名等等。</p><p>比如想让机器读大量医学相关的文献，希望它自动知道有什么药物可以治疗新冠状肺炎。<u>这些药物的名字，就是实体</u>。它输入的是一个序列，输出的是序列上每个位置的类别。它就和词性标注、槽位填充一样。</p><p>NER常见的两个问题: <strong>名字一样但指的是不同的东西</strong>，有多个标签需要实体消歧；<strong>不一样的名字指的却是相同的东西</strong>，需要实体归一化。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0y4ykt6uj312m0regxp.jpg" style="zoom:33%;" /></li><li><p><strong>Relation Extraction</strong>(重要): 关系抽取</p><p>假如已知如何从文本中获得实体，接下来还需要知道<strong>它们之间的关系</strong>。</p><p>关系抽取的输入：<strong>序列和抽取出的实体</strong>，输出是<strong>两两实体之间的关系</strong>。本质是一个<strong>分类任务</strong></p><p>比如，输入：哈利波特和霍格沃茨，输出：关系，是后者的学生</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0yb0p0i0j314g0t8nd9.jpg" style="zoom:33%;" /></li><li><p><strong>Question Answering</strong> 问答系统/搜索引擎</p></li><li><p><strong>Chating</strong> 对话机器人</p><p>分成两种，闲聊和任务导向型。</p><ul><li><p><strong>Chatbox</strong> 闲聊机器人：基本上都是在尬聊，有一堆问题待解决，比如角色一致性，多轮会话，对上下文保有记忆等。</p></li><li><p><strong>Task-oriented</strong> 任务导向的对话机器人：能<strong>协助人完成某件事</strong></p><p>比如订机票，调闹钟，问天气等。我们需要一个模型，<strong>把过去已经有的历史对话，统统都输入到一个模型中</strong>，这个模型<strong>可以输出一个序列</strong>当作现在机器的<strong>回复</strong></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0ym6grvaj313a0peauo.jpg" style="zoom:33%;" /><ul><li><p>我们会把这个模型<strong>再细分成很多模块</strong>，而不会是端对端的</p><p>这些模块通常包括：<strong>自然语言理解NLU，行动策略管理，自然语言生成NLG</strong></p><ul><li><p>自然语言理解负责根据上下文去<strong>理解当前用户的意图</strong>，</p><p>方便</p></li><li><p>行动策略(policy)管理<strong>选出下一步候选的行为</strong>。如执行系统操作，澄清还是补全信息，</p><p>确定好行动之后，</p></li><li><p>自然语言生成模块会生成出对齐行动的回复</p></li><li><p>(除了之前三个模块，再加上语音助手的<u>语音识别 ASR 和 语音合成 TTS</u> 就成了完整的对话系统。)</p></li></ul><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gh0yqezmcfj314w0rktk0.jpg" style="zoom:33%;" /></li></ul></li></ul></li></ul><h3 id="c-技术的4个-实现难度递增的-维度"><a href="#c-技术的4个-实现难度递增的-维度" class="headerlink" title="c. 技术的4个(实现难度递增的)维度"></a>c. 技术的4个(实现难度递增的)维度</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gff3scbnfmj31980pynao.jpg" alt="从下往上难度递增" style="zoom:33%;" /><h2 id="1-case-机器翻译"><a href="#1-case-机器翻译" class="headerlink" title="1. case-机器翻译"></a>1. case-机器翻译</h2><h3 id="思路a-暴力法"><a href="#思路a-暴力法" class="headerlink" title="思路a: 暴力法"></a>思路a: 暴力法</h3><h4 id="原理：全排列组合分词后的句子，用语言模型从中筛选出语义最合适的"><a href="#原理：全排列组合分词后的句子，用语言模型从中筛选出语义最合适的" class="headerlink" title="原理：全排列组合分词后的句子，用语言模型从中筛选出语义最合适的"></a>原理：<strong>全排列组合</strong>分词后的句子，用语言模型从中筛选出语义最合适的</h4><p>第一步通过<strong>翻译模型TM</strong>进行<strong>分词</strong></p><p>第二步映射词语，因为语法的存在而进行<u>映射后词语的排列组合</u></p><p>第三步通过<strong>语言模型LM</strong>选出排列组合中最合适的语句</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf99soj3vmj317w0kuq8d.jpg" alt="">缺点: 排列组合的复杂度太高</p><p>解决: <strong>维特比Viterbi</strong>算法——<strong>同时考虑TM</strong>分词+<strong>LM筛选</strong>这分开2个步骤</p><h3 id="思路b-维特比Viterbi算法"><a href="#思路b-维特比Viterbi算法" class="headerlink" title="思路b: 维特比Viterbi算法"></a>思路b: <strong>维特比Viterbi</strong>算法</h3><h4 id="原理：贝叶斯TM和LM"><a href="#原理：贝叶斯TM和LM" class="headerlink" title="原理：贝叶斯TM和LM"></a>原理：贝叶斯TM和LM</h4><p><strong>串联TM</strong>分词+<strong>LM筛选</strong>这分开的2个步骤</p><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9i9qd3wlj31680f20vt.jpg" alt="流程图"></p><p>各个部分的作用</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9iup3gr2j30x20icwjg.jpg" alt="">好处：降低了复杂度，<strong>避免了原来随机排列组合生成句子</strong>的<strong>NP-hard的指数级</strong>复杂度</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9iao2h3fj309802e749.jpg" alt=""></p><h5 id="1-语言模型LM"><a href="#1-语言模型LM" class="headerlink" title="1. 语言模型LM"></a>1. 语言模型LM</h5><h5 id="作用：生成句子-即输出"><a href="#作用：生成句子-即输出" class="headerlink" title="作用：生成句子(即输出)"></a>作用：生成句子(即输出)</h5><h5 id="分类：按照当前生成的语言，对之前词语的”记忆“程度"><a href="#分类：按照当前生成的语言，对之前词语的”记忆“程度" class="headerlink" title="分类：按照当前生成的语言，对之前词语的”记忆“程度"></a>分类：按照当前生成的语言，对之前词语的”记忆“程度</h5><p>这种<strong>连续的条件概率</strong>——又称，<strong>联合分布</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9i640a7bj316e0aojut.jpg" alt=""></p><p><strong>Unigram</strong> model, <strong>Bingram</strong> model …. n-gram model</p><p>至于每个Prop, 则源于<strong>已有的<u>统计频率</u>结果</strong></p><h6 id="性能标准"><a href="#性能标准" class="headerlink" title="性能标准"></a>性能标准</h6><p>生成之后同时看<strong>句子质量</strong>和<strong>对应概率(score)</strong>，句子<strong>错误少</strong>的<strong>概率应该越大</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf9iknu2qsj30yw06idh3.jpg" alt=""></p><h5 id="2-翻译模型TM"><a href="#2-翻译模型TM" class="headerlink" title="2. 翻译模型TM"></a>2. 翻译模型TM</h5><p>…</p><h2 id="2-case-基于检索的智能问答系统"><a href="#2-case-基于检索的智能问答系统" class="headerlink" title="2. case-基于检索的智能问答系统"></a>2. case-基于检索的智能问答系统</h2><h3 id="数据-输入-语料库"><a href="#数据-输入-语料库" class="headerlink" title="数据/输入: 语料库"></a>数据/输入: 语料库</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfinf6trm9j311w0kuke0.jpg" alt="写好的问与答结对"></p><h3 id="NLP-general-pipeline"><a href="#NLP-general-pipeline" class="headerlink" title="NLP general pipeline"></a>NLP general pipeline</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjqjhtaemj31kj0u0qte.jpg" alt="image-20200607150135228"></p><h4 id="QAsys的流程"><a href="#QAsys的流程" class="headerlink" title="QAsys的流程"></a>QAsys的流程</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfir8926c2j31ip0u0e81.jpg" alt="image-20200606183943480"></p><h4 id="解决：如何搜索出最相似问题"><a href="#解决：如何搜索出最相似问题" class="headerlink" title="解决：如何搜索出最相似问题"></a>解决：如何搜索出最相似问题</h4><h5 id="相似度的衡量？"><a href="#相似度的衡量？" class="headerlink" title="相似度的衡量？"></a>相似度的衡量？</h5><ul><li>❌<strong>逐字</strong>比较：正则/<strong>规则</strong>AI——<strong>对匹配要求很高很高</strong>, 所以需要考虑<strong><u>尽可能多</u></strong>的输入，过于繁琐<ul><li>唯一使用场景：<strong><u>没有数据时</u></strong>，语料库很空</li></ul></li><li>✅<strong>概率</strong>比较：字符串的<strong>“相似性”计算</strong></li></ul><h4 id="阶段一-文本处理"><a href="#阶段一-文本处理" class="headerlink" title="阶段一 文本处理"></a>阶段一 <a href="https://kennyng-19.github.io/Kenny_Ng.github.io/2019/08/07/NLP-text-process/" target="_blank" rel="noopener">文本处理</a></h4><p>常用技术</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm2bbdbwoj30zg0eaaj2.jpg" alt="image-20200609152001651" style="zoom:50%;" />]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-NLP-Overview&quot;&gt;&lt;a href=&quot;#0-NLP-Overview&quot; class=&quot;headerlink&quot; title=&quot;0. NLP Overview&quot;&gt;&lt;/a&gt;0. NLP Overview&lt;/h2&gt;&lt;h3 id=&quot;a-按照研究方向-语言的I-O
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>CS学科中English terms的中文大意</title>
    <link href="http://yoursite.com/2019/09/28/CS-English-terms/"/>
    <id>http://yoursite.com/2019/09/28/CS-English-terms/</id>
    <published>2019-09-28T07:23:26.000Z</published>
    <updated>2020-07-08T16:19:47.562Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-client-program-实现应用的程序-Java"><a href="#1-client-program-实现应用的程序-Java" class="headerlink" title="1. client program 实现应用的程序(Java)"></a>1. client program 实现应用的程序(Java)</h3><p>我见到的其出处：Coursera普林斯顿《算法》课程</p><p>（此client并非C/S的客户端哦）</p><p>A <strong>client</strong> is a <strong>program</strong> that <strong><u>uses</u></strong> a data type API. （And an implementation is the code that implements the data type specified in an API）</p><p>数据结构类/<strong>已有的java类</strong>，本身不能直接用于应用；而client程序，基于已有的类，实现了<strong>XX应用</strong>的<strong>功能类</strong>——可以给<strong>有需要XX应用的client调用方</strong>直接使用, 且一般会<u>支持java 命令行的<strong>args传参</strong></u></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg81a2k36yj309e08u3z4.jpg" alt="字典的应用程序、索引的应用程序" style="zoom:33%;" /><p>如上图，命名都是<strong>XX client</strong>，字典的应用程序、索引的应用程序</p><h3 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h3><p>我见到的其出处：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-client-program-实现应用的程序-Java&quot;&gt;&lt;a href=&quot;#1-client-program-实现应用的程序-Java&quot; class=&quot;headerlink&quot; title=&quot;1. client program 实现应用的程序(Java)&quot;&gt;&lt;
      
    
    </summary>
    
    
    
      <category term="CS" scheme="http://yoursite.com/tags/CS/"/>
    
      <category term="English" scheme="http://yoursite.com/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch一些小细节补充</title>
    <link href="http://yoursite.com/2019/08/26/sum_pytorch_details/"/>
    <id>http://yoursite.com/2019/08/26/sum_pytorch_details/</id>
    <published>2019-08-26T08:32:47.000Z</published>
    <updated>2020-05-26T09:05:12.724Z</updated>
    
    <content type="html"><![CDATA[<h1 id="补充Pytorch中的易被忽视但得注意的细节"><a href="#补充Pytorch中的易被忽视但得注意的细节" class="headerlink" title="补充Pytorch中的易被忽视但得注意的细节"></a>补充Pytorch中的易被忽视但得注意的细节</h1><h4 id="1-定义Dataloader的num-workers参数"><a href="#1-定义Dataloader的num-workers参数" class="headerlink" title="1. 定义Dataloader的num_workers参数"></a>1. 定义Dataloader的num_workers参数</h4><p><strong>Q</strong>: </p><p>在给Dataloader设置worker数量（<code>num_worker</code>）时，到底<strong>设置多少合适</strong>？这个worker到底怎么工作的？<br><strong>如果将<code>num_worker</code>设为0（也是默认值），就没有worker了吗？</strong></p><p><strong>A</strong>: <code>num_workers</code>的经验设置值<strong>是<font color="#dd0000">自己电脑/服务器的CPU核心数</font></strong>，(比如我的Macbook Pro16寸是 6核)如果CPU很强+配的<strong>RAM是充足的</strong>，就可以设置为<strong>略微≥核心数</strong>。</p><ol><li><p>参数的<strong>作用</strong>： 官方注释为：</p><blockquote><p> how many <strong>subprocesses</strong> to use for data loading.</p><p> <code>0</code> means that the data will be loaded <strong>in the main process.</strong> (default: <code>0</code>)</p></blockquote><p>作用：denotes the <strong><font color="#dd0000">number of processes that generate batches in parallel</font></strong> to generate your data <font color="#dd0000"><strong>on multiple cores</strong> in real time</font></p><p>详细解释：每每轮到dataloader加载数据时：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>start_epoch<span class="token punctuation">,</span> end_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span><span class="token punctuation">:</span></code></pre><p>dataloader<strong>一次性创建<code>num_worker</code>个worker</strong>，（也可以说dataloader一次性创建<code>num_worker</code>个工作进程，worker也是普通的工作进程），并用<code>batch_sampler</code>将指定batch分配给指定worker，worker将它负责的batch加载进RAM。</p><p>然后，dataloader从RAM中找本轮迭代要用的batch，如果找到了，就使用。如果没找到，就要<code>num_worker</code>个worker继续加载batch到内存，直到dataloader在RAM中找到目标batch。一般情况下都是能找到的，因为<code>batch_sampler</code>指定batch时当然优先指定本轮要用的batch。</p></li><li><p><strong>不同赋值时各自的意义</strong>：<code>num_worker</code>设置得大，好处是<strong>寻batch速度快</strong>，因为下一轮迭代的batch很可能在上一轮/上上一轮…迭代时已经加载好了。坏处是<strong>内存开销大</strong>，也加重了CPU负担（worker加载数据到RAM的进程是CPU复制的嘛）。</p><p>如果设为0，意味着每一轮迭代时，dataloader<strong>不再有自主加载数据到RAM</strong>这一步骤（因为没有worker了），而是在RAM中找batch，找不到时再加载相应的batch。缺点当然是<strong>速度更慢</strong>。</p></li></ol><h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;补充Pytorch中的易被忽视但得注意的细节&quot;&gt;&lt;a href=&quot;#补充Pytorch中的易被忽视但得注意的细节&quot; class=&quot;headerlink&quot; title=&quot;补充Pytorch中的易被忽视但得注意的细节&quot;&gt;&lt;/a&gt;补充Pytorch中的易被忽视但得注意的
      
    
    </summary>
    
    
    
      <category term="ML" scheme="http://yoursite.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>(转)算法复杂度-intro(推导分治算法的master theorem)</title>
    <link href="http://yoursite.com/2018/02/06/algo-complexity-intro/"/>
    <id>http://yoursite.com/2018/02/06/algo-complexity-intro/</id>
    <published>2018-02-06T08:10:13.000Z</published>
    <updated>2020-06-06T10:45:18.215Z</updated>
    
    <content type="html"><![CDATA[<p>在实现算法的时候，通常会从两方面考虑算法的复杂度，即时间复杂度和空间复杂度。</p><p>顾名思义，时间复杂度用于度量算法的计算工作量；空间复杂度用于度量算法占用的内存空间。</p><p>由于算法测试结果非常<strong>依赖测试环境，且受数据规模的影响，</strong>因此需要一种粗略的评估方法，即时间、空间复杂度分析方法。</p><h2 id="渐进时间复杂度"><a href="#渐进时间复杂度" class="headerlink" title="渐进时间复杂度"></a>渐进时间复杂度</h2><p>理论上来说，时间复杂度是算法运算所消耗的时间，但是对于一个算法来说，评估运行时间是很难的，因为针对不同大小的输入数据，算法处理所要消耗的时间是不同的，因此通常关注的是时间频度，即算法运行计算操作的次数，记为 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> ，其中n称为问题的规模。同样，因为n是一个变量，n发生变化时，时间频度 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 也在发生变化，我们称时间复杂度的极限情形称为算法的“渐近时间复杂度”，记为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。这种表示方法称作<strong>大 O 时间复杂度表示法</strong>。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示<strong>代码执行时间随数据规模增长的变化趋势</strong>，因为我们没办法用通常的评估方式来评估所有算法的时间复杂度，所以通常使用渐进时间复杂度表示算法的时间复杂度，下文中的时间复杂度均表示渐进时间复杂度。</p><p>我们在这里放一个python算法的例子来解释一下：</p><pre class=" language-python3"><code class="language-python3">def f(n):    a,b=0,0#运行消耗t0时间    for i in range(n):#运行一次平均消耗t1时间        a = a + rand()#运行一次平均消耗t2时间    for j in range(n):#运行一次平均消耗t3时间        b = b + rand()#运行一次平均消耗t4时间</code></pre><p>在这个例子中，我们分别计算 <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 函数的时间复杂度与空间复杂度。根据代码上执行的平均时间假设，计算出来执行 <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 的时间为 <img src="https://www.zhihu.com/equation?tex=T%28n%29%3Dt_0%2B%28t_1%2Bt_2%29%2An%2B%28t_3%2Bt_4%29%2An%3Dt_0%2B%28t_1%2Bt_2%2Bt_3%2Bt_4%29%2An" alt="[公式]"> ；而函数中申请了两个变量a,b，占用内存空间为2。上述 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 是我们对函数f(n)进行的准确时间复杂度的计算。但实际情况中，输入规模n是影响算法执行时间的因素之一。在n固定的情况下，不同的输入序列也会影响其执行时间。当n值非常大的时候， <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 函数中常数项t0以及n的系数 <img src="https://www.zhihu.com/equation?tex=%28t_1%2Bt_2%2Bt_3%2Bt_4%29" alt="[公式]"> 对n的影响也可以忽略不计了，因此这里函数 <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 的时间复杂度我们可以表示为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。我们再来看空间复杂度，跟时间复杂度表示类似，也可以用极限的方式来表示空间复杂度（但是貌似没有渐进空间复杂度的说法），因为这里只声明了2个变量，因此空间复杂度也是常数阶的，因此这里空间复杂度计算为 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]"> </p><p><strong>规律总结：</strong></p><p><strong>所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。</strong></p><p><img src="https://www.zhihu.com/equation?tex=T%28n%29%3DO%28f%28n%29%29" alt="[公式]"></p><p>其中f(n) 表示每行代码执行的次数总和。公式中的 O，表示代码的执行时间 <img src="https://www.zhihu.com/equation?tex=T%28n%29" alt="[公式]"> 与 <img src="https://www.zhihu.com/equation?tex=+f%28n%29+" alt="[公式]"> 表达式成正比。</p><h3 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h3><p>因为我们计算的是极限状态下的时间复杂度，因此存在两种特性：</p><p>1.按照函数数量级角度来说，相对增长低的项对相对增长高的项产生的影响很小，可忽略不计。</p><p>2.最高项系数对最高项的影响也很小，因此也可以忽略不计。针对第1点，常见的时间复杂度有：常数阶：常数阶： <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]"> , 对数阶： <img src="https://www.zhihu.com/equation?tex=O%28log_2+n%29" alt="[公式]"> , 线性阶： <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> , k次方阶： <img src="https://www.zhihu.com/equation?tex=O%28n%5EK%29" alt="[公式]"> ,指数阶： <img src="https://www.zhihu.com/equation?tex=O%282%5En%29" alt="[公式]"> 。</p><p>根据上述两种特性，总结时间复杂度的计算方法：</p><p>1.<strong>加法法则-总复杂度等于量级最大的那段代码的复杂度：</strong>计算时只取相对增长最高的项，去掉低阶项，并去掉最高项的系数（比如 <img src="https://www.zhihu.com/equation?tex=O%282n%29" alt="[公式]"> 只需要表示为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> ）；</p><p>2.<strong>只关注循环执行次数最多的一段代码</strong>：大 O 这种复杂度表示方法只是表示一种变化趋势。通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了；</p><p>3.<strong>乘法法则-嵌套代码的复杂度等于嵌套内外代码复杂度的乘积：</strong>如果 <img src="https://www.zhihu.com/equation?tex=T_1%28n%29%3DO%28f%28n%29%29%2CT_2%28n%29%3DO%28f%28n%29%29" alt="[公式]"> ,那么 <img src="https://www.zhihu.com/equation?tex=T%28n%29%3DT_1%28n%29%E2%88%97T_2%28n%29%3DO%28f%28n%29%29%E2%88%97O%28g%28n%29%29%3DO%28f%28n%29%E2%88%97g%28n%29%29." alt="[公式]"> 也就是说假设<img src="https://www.zhihu.com/equation?tex=+T_1%28n%29+%3D+O%28n%29%EF%BC%8CT_2%28n%29+%3D+O%28n%5E2%29" alt="[公式]"> ，则 <img src="https://www.zhihu.com/equation?tex=T_1%28n%29+%2A+T_2%28n%29+%3D+O%28n%5E3%29" alt="[公式]"></p><p>4.<strong>针对常数阶，取时间复杂度为O(1)</strong>。</p><h2 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h2><p>空间复杂度指的是算法在内存上临时占用的空间，包括程序代码所占用的空间，输入数据占用的空间和变量占用的空间。在递归运算时，由于递归计算是需要使用堆栈的，所以需要考虑堆栈操作占用的内存空间大小。空间复杂度的计算也遵循渐进原则，即参考时间复杂度与空间复杂度计算方法项。</p><h3 id="计算方法-1"><a href="#计算方法-1" class="headerlink" title="计算方法"></a>计算方法</h3><p>1.通常只考虑参数表中为形参分配的存储空间和为函数体中定义的局部变量分配的存储空间（比如变量 <img src="https://www.zhihu.com/equation?tex=a%3D0" alt="[公式]"> 在算法中空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]"> ； <img src="https://www.zhihu.com/equation?tex=list_a%3D%5B0%2C1%2C....%2Cn%5D" alt="[公式]"> 的空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> ； <img src="https://www.zhihu.com/equation?tex=set%28list_a%29" alt="[公式]"> 的空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="[公式]">) )。</p><p>2.递归函数情况下，空间复杂度等于一次递归调用分配的临时存储空间的大小乘被调用的次数。</p><p>我们这里以递归方法实现的斐波那契数列为例：</p><pre class=" language-python3"><code class="language-python3">def fib(n):    if n < 3:         return 1    else:        return fib(n-2)+fib(n-1)</code></pre><p>斐波那契数列的序列依次为 <img src="https://www.zhihu.com/equation?tex=1%2C1%2C2%2C3%2C5%2C8%2C13....." alt="[公式]"> 特点是，当数列的长度大于等于3时，数列中任意位置的元素值等于该元素前两位元素之和。</p><p>1.计算时间复杂度： <img src="https://www.zhihu.com/equation?tex=O%282%5En%29" alt="[公式]"> 。计算方法：通过归纳证明的方法，我们尝试计算数列中第8个元素的值的递归调用次数,为了方便观察，我把外层括号替换为了大括号。 <img src="https://www.zhihu.com/equation?tex=fib%288%29%3Dfib%287%29%2Bfib%286%29+%3D%7Bfib%286%29%2Bfib%285%29%7D%2B%7Bfib%285%29%2Bfib%284%29%7D+" alt="[公式]"> <img src="https://www.zhihu.com/equation?tex=%3D%28%7Bfib%285%29%2Bfib%284%29%7D%2B%7Bfib%284%29%2Bfib%283%29%7D%29%2B%28%7Bfib%284%29%2Bfib%283%29%7D%2B%7Bfib%283%29%2Bfib%282%29%7D%29+%3D....." alt="[公式]"> 这里太多我就不一一写出了。不难发现，每次调用递归时，递归调用次数都是以程序中调用递归次数即2的指数形式增长的。第一层递归时，调用了2次 <img src="https://www.zhihu.com/equation?tex=fib%28n%29" alt="[公式]"> 函数；第二层递归时，第一层的2次递归调用分别又要调用2次，即调用了 <img src="https://www.zhihu.com/equation?tex=2%5E2" alt="[公式]"> 次；第三层递归调用了 <img src="https://www.zhihu.com/equation?tex=2%5E3" alt="[公式]"> 次，以此规律，不难算出时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%282%5En%29" alt="[公式]"></p><p>2.计算空间复杂度： <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。计算方法：我们同样使用归纳证明的方法，尝试推导数列中第6个元素值的内存占用情况。调用函数 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> ,此时因为有形参n传递，在栈中为n申请内存资源，我们为了方便，以 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> 表示栈中元素。此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> ;我们根据函数内的递归调用关系，为了计算 <img src="https://www.zhihu.com/equation?tex=fib%286%29" alt="[公式]"> ,我们需要 <img src="https://www.zhihu.com/equation?tex=fib%285%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%284%29" alt="[公式]"> 的值，此时发生形参传递，栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29" alt="[公式]"> ;为了计算 <img src="https://www.zhihu.com/equation?tex=fib%284%29" alt="[公式]"> ，我们需要 <img src="https://www.zhihu.com/equation?tex=fib%283%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 的值，此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29%2Cfib%283%29%2Cfib%282%29" alt="[公式]"> ，但是由于 <img src="https://www.zhihu.com/equation?tex=fib%282%29%3D1" alt="[公式]"> ，此时 <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 函数计算完成， <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 出栈，此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29%2Cfib%283%29" alt="[公式]"> 。为了计算 <img src="https://www.zhihu.com/equation?tex=fib%283%29" alt="[公式]"> ，需要 <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%281%29" alt="[公式]"> 的值，此时栈中有 <img src="https://www.zhihu.com/equation?tex=fib%286%29%2Cfib%285%29%2Cfib%284%29%2Cfib%283%29%2Cfib%282%29%2Cfib%281%29" alt="[公式]"> 。但是 <img src="https://www.zhihu.com/equation?tex=fib%282%29%3D1%2Cfib%281%29%3D1" alt="[公式]"> ,计算完成， <img src="https://www.zhihu.com/equation?tex=fib%282%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=fib%281%29" alt="[公式]"> 出栈。此时得到fib(3)的值为2，fib(3)出栈；由此出栈顺序， <img src="https://www.zhihu.com/equation?tex=fib%284%29%2Cfib%285%29%2Cfib%286%29" alt="[公式]"> 也会随计算完成出栈。不难发现，在此次递归计算的过程中，内存中最多消耗了6个内存资源，由归纳证明法得出 <img src="https://www.zhihu.com/equation?tex=fib%28n%29" alt="[公式]"> 的空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 。</p><h2 id="Master-Theorem-主方法"><a href="#Master-Theorem-主方法" class="headerlink" title="Master Theorem 主方法"></a>Master Theorem 主方法</h2><p>Master Theorem是为了计算<strong><font color="#dd0000">含有递归调用</strong>的<strong>分治</strong>算法</font>的时间复杂度的。因为算法中如果含有递归调用算法的情况下<strong>使用归纳证明的方法</strong>，计算时间复杂度是相当困难的，因此需要利用<strong>已有的定理Master Theorem</strong>来帮我们计算复杂情况下的算法的时间复杂度</p><h3 id="定理的定义"><a href="#定理的定义" class="headerlink" title="定理的定义"></a>定理的定义</h3><p><img src="https://pic1.zhimg.com/80/v2-d05e32c23bdbbd70ef3dea46eb47db58_1440w.jpg" alt="img"></p><p>Master Theorem的一般形式是T(n) = a T(n / b) + f(n)， a &gt;= 1, b &gt; 1。递归项f(n)理解为一个高度为log_b n 的a叉树， 这样总时间频次为 (a ^ log_b n) - 1， 右边的f(n)假设为 nc 那么我们对比一下这两项就会发现 T(n)的复杂度主要取决于 log_b a 与 f(n) 的大小。（log_b a表示以b为底，a的对数）因此我总结了使用Master Theorem的三种case的简单判断：</p><p>1.计算 <img src="https://www.zhihu.com/equation?tex=log_b+a" alt="[公式]"> 的值，比较 n^(log_b a)与f(n)的大小。</p><p>2.若n^(log_b a)&gt;f(n),时间复杂度为O(n^(log_b a)) （case 1）</p><p>3.若n^(log_b a)&lt;f(n),时间复杂度为O(f(n)) （case 3）</p><p>4.若n^(log_b a)=f(n),时间复杂度为O(n^(log_b a)*(log n)^k+1) （case 2） (其中k值为f(n)中如果有log对数项时，该对数项的指数为k，例如，如果f(n)=log n ，k=1；f(n)=n,k=0)</p><p>可能公式理解起来有点困难，举几个例子来加深理解：</p><p><img src="https://pic4.zhimg.com/80/v2-40b032e934608df08aabbbd79d1b18cb_1440w.jpg" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><p>常规套路，就是<strong>比较n^(log_b a)与f(n)</strong>的值了，比较出来就可以套用上述公式</p></li><li><p>但是一定要注意公式中的限制条件，如a必须为常数项等。</p></li></ul><p>Reference:</p><p>《数据结构与算法之美》</p><p><a href="http://people.csail.mit.edu/thies/6.046-web/master.pdf" target="_blank" rel="noopener">The Master theorem</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在实现算法的时候，通常会从两方面考虑算法的复杂度，即时间复杂度和空间复杂度。&lt;/p&gt;
&lt;p&gt;顾名思义，时间复杂度用于度量算法的计算工作量；空间复杂度用于度量算法占用的内存空间。&lt;/p&gt;
&lt;p&gt;由于算法测试结果非常&lt;strong&gt;依赖测试环境，且受数据规模的影响，&lt;/stro
      
    
    </summary>
    
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
