<!DOCTYPE HTML>
<html lang="zh-CNs">


<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="ML,DL的混淆点/易错点, 池中之物">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>ML,DL的混淆点/易错点 | 池中之物</title>
    <link rel="icon" type="image/png" href="/Kenny_Ng.github.io/favicon.png">

    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/css/my.css">

    <script src="/Kenny_Ng.github.io/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/Kenny_Ng.github.io/atom.xml" title="池中之物" type="application/atom+xml">
<link rel="stylesheet" href="/Kenny_Ng.github.io/css/prism-tomorrow.css" type="text/css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Kenny_Ng.github.io/" class="waves-effect waves-light">
                    
                    <img src="/Kenny_Ng.github.io/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">池中之物</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Kenny_Ng.github.io/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Kenny_Ng.github.io/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Kenny_Ng.github.io/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Kenny_Ng.github.io/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Kenny_Ng.github.io/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Kenny_Ng.github.io/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>Contact</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Kenny_Ng.github.io/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Kenny_Ng.github.io/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">池中之物</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Kenny_Ng.github.io/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Kenny_Ng.github.io/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Kenny_Ng.github.io/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Kenny_Ng.github.io/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Kenny_Ng.github.io/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			About
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Kenny_Ng.github.io/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Kenny_Ng.github.io/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/Kenny_Ng.github.io/medias/featureimages/7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">ML,DL的混淆点/易错点</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/Kenny_Ng.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Kenny_Ng.github.io/tags/ML/">
                                <span class="chip bg-color">ML</span>
                            </a>
                        
                            <a href="/Kenny_Ng.github.io/tags/math/">
                                <span class="chip bg-color">math</span>
                            </a>
                        
                            <a href="/Kenny_Ng.github.io/tags/%E7%A7%AF%E7%B4%AF/">
                                <span class="chip bg-color">积累</span>
                            </a>
                        
                            <a href="/Kenny_Ng.github.io/tags/%E7%BB%9F%E8%AE%A1%E6%A6%82%E7%8E%87/">
                                <span class="chip bg-color">统计概率</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-07-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2020-11-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    6.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    22 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="ML-数学-中的常见混淆-易错点和一般性结论"><a href="#ML-数学-中的常见混淆-易错点和一般性结论" class="headerlink" title="ML(数学)中的常见混淆, 易错点和一般性结论"></a>ML(数学)中的常见混淆, 易错点和一般性结论</h1><h2 id="0-似然？"><a href="#0-似然？" class="headerlink" title="0. 似然？"></a>0. 似然？</h2><p>似然和概率在统计学中是经常见到的两个术语，有时候这两个概念是一个意思，有时候却有很大区别。这里梳理下这两个术语所代表的具体含义。</p>
<h4 id="本文中数学符号及含义"><a href="#本文中数学符号及含义" class="headerlink" title="本文中数学符号及含义"></a>本文中数学符号及含义</h4><table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>O</td>
<td>观测值</td>
</tr>
<tr>
<td>θ</td>
<td>随机过程中的参数</td>
</tr>
<tr>
<td><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifp45h7myj303c02q745.jpg" style="zoom:25%;" /></td>
<td>参数的估计</td>
</tr>
<tr>
<td>P(O|θ)</td>
<td>概率</td>
</tr>
<tr>
<td>L(θ|O)</td>
<td><strong>似然函数</strong></td>
</tr>
</tbody></table>
<h4 id="wiki中关于“似然”和“概率”的解释"><a href="#wiki中关于“似然”和“概率”的解释" class="headerlink" title="wiki中关于“似然”和“概率”的解释"></a>wiki中关于“似然”和“概率”的解释</h4><ul>
<li>在频率推论中，似然函数（常常简称为似然）是一个在<strong>给定了数据</strong>后<strong>关于模型参数的</strong>函数。<strong>在非正式情况下，“似然”通常被用作“概率”的同义词。</strong></li>
<li>在<strong><u>数理统计</u></strong>中，两个术语<strong><u>则有不同的意思</u></strong>：<ul>
<li>“概率”描述了<strong>给定模型参数后</strong>，输出<strong>结果的合理性(可能性大小)</strong>，而不涉及任何观察到的数据。</li>
<li>“似然”则描述了<strong>给定了特定观测结果</strong>后，描述模型<strong>参数是否合理</strong></li>
</ul>
</li>
</ul>
<h4 id="一些基本的概念"><a href="#一些基本的概念" class="headerlink" title="一些基本的概念"></a>一些基本的概念</h4><p>估计（estimating）是一种手段，一种对<strong>模型参数</strong>进行<strong>推测(inference)</strong>的手段，说白了，就是利用训练数据X，对模型进行calibration(校正)。在这之前，首先<strong>需要有一个<u>种类确定</u>的模型</strong>，推测的目标是模型中<strong>待辨识的</strong>参数。</p>
<h4 id="极大似然估计公式"><a href="#极大似然估计公式" class="headerlink" title="极大似然估计公式"></a>极大似然估计公式</h4><p>先看似然函数的定义，它是给定联合样本值x下关于(未知)参数θ的函数，人话说就是：<strong>利用已知的样本结果 x</strong>，反推最有可能（最大概率）<strong><u>导致</u>这样结果的参数</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl6k5alhoej30ou05k75h.jpg" alt="ML Estimation公式"></p>
<ul>
<li>这里的小<strong>x</strong>指联合样本随机变量X取到的值，即X=x；</li>
<li>联和样本集<strong>X</strong>={x1 、x2 、…、xn}，<strong>MLE是默认假设<u>样本之间都是相对独立的</u>，注意这个假设很重要！所以上面才能连乘</strong></li>
<li>这里的θ是指未知参数，属于参数空间；</li>
<li>这里的f(x|θ)是一个<strong>概率密度函数</strong>，特别地表示(给定)θ下，关于联合样本值x的<strong>联合密度函数</strong>。</li>
</ul>
<p>所以从定义上，似然函数和密度函数是完全不同的两个<strong>数学对象</strong>：前者是关于θ的函数，后者是关x的函数。所以这里的等号=，应理解为<strong><font color="#dd0000">函数值形式</font>的相等</strong>，而<font color="#dd0000">不是两个函数本身是同一函数</font>。所以这个式子的<strong>严格书写方式</strong>是<img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%3B+%5Ctheta%29" alt="">)(分号示把参数隔开) 即<strong>θ在右端只当作参数</strong>。</p>
<h4 id="概率角度看ML-频率派-vs-贝叶斯派"><a href="#概率角度看ML-频率派-vs-贝叶斯派" class="headerlink" title="概率角度看ML: 频率派 vs 贝叶斯派"></a>概率角度看ML: 频率派 vs 贝叶斯派</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjo3vb3pqgj31450u016u.jpg" alt="概率角度分析两派的机器学习" style="zoom:50%;" />

<p><strong>频率派，统计机器学习: MLE 极大似然估计</strong>：L(θ) best θ = f(X|θ), <strong>f为θ fit数据的概率函数</strong>！<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjo3zx4hv4j30e20agwie.jpg" alt="MLE 极大后验估计" style="zoom:50%;"/></p>
<p>given data, 寻找<strong>fit程度最大的概率分布/θ</strong></p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjie9ys7s0j31qk0tekhj.jpg" alt="最大似然估计" style="zoom:30%;" />



<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjie8c3kjpj315m0bedi7.jpg" alt="在寻找最优分布能fit/产生这些given data" style="zoom:33%;" />





<p><strong>贝叶斯派机器学习</strong>：<strong>MAP，极大后验估计</strong>，<strong>对概率公式积分——核心求的是<u>积分</u></strong></p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjo41bewn3j30to0lw4ef.jpg" alt="MAP 考虑了prior的极大似然估计" style="zoom:40%;" />





<h4 id="对比本质-“似然”和“概率”是站在两个角度看待问题"><a href="#对比本质-“似然”和“概率”是站在两个角度看待问题" class="headerlink" title="对比本质: “似然”和“概率”是站在两个角度看待问题"></a>对比本质: “似然”和“概率”是站在<u>两个角度</u>看待问题</h4><h5 id="例如"><a href="#例如" class="headerlink" title="例如"></a>例如</h5><ul>
<li>概率</li>
</ul>
<blockquote>
<p>抛一枚均匀的硬币，拋20次，问15次拋得正面的可能性有多大？ 这里的可能性就是”概率”，均匀的硬币就是给定参数θ=0.5，“拋20次15次正面”是观测值O。求概率P(H=15|θ=0.5)=？的概率。</p>
</blockquote>
<ul>
<li><strong>“似然”描述了给定了特定观测值后，描述<u>模型参数</u>是否合理。</strong></li>
</ul>
<blockquote>
<p>拋一枚硬币，拋20次，结果15次正面向上，问其为均匀的可能性？ 这里的可能性就是”似然”</p>
<p>“拋20次15次正面”为观测值O为已知，参数θ并不知道，求L(θ|H=15)=P(H=15|θ=0.5)(prior和evidence都是常数) 的<strong>最大化下的θ 值</strong></p>
</blockquote>
<p>例B. 对于统计<strong>概率分布</strong>的似然</p>
<h4 id="注意：计算题中的似然"><a href="#注意：计算题中的似然" class="headerlink" title="注意：计算题中的似然"></a>注意：计算题中的似然</h4><p>贝叶斯公式中，求似然概率值时——就是<u>分子本身的概率公式/分布函数</u>，而已经given的分母, 与其无关！</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gji415ep10j30fa03udhm.jpg" alt="若有分母一定时，直觉上可得的分子概率，那就是似然值！"></p>
<h4 id="离散随机变量的“似然”与“概率”"><a href="#离散随机变量的“似然”与“概率”" class="headerlink" title="离散随机变量的“似然”与“概率”"></a>离散随机变量的“似然”与“概率”</h4><h4 id="连续型随机变量的“似然”与“概率”"><a href="#连续型随机变量的“似然”与“概率”" class="headerlink" title="连续型随机变量的“似然”与“概率”"></a>连续型随机变量的“似然”与“概率”</h4><h2 id="1-均值化？归一化？傻傻分不清"><a href="#1-均值化？归一化？傻傻分不清" class="headerlink" title="1. 均值化？归一化？傻傻分不清"></a>1. 均值化？归一化？傻傻分不清</h2><p>“标准化”和”归一化”这两个中文词要指代<strong>四种</strong>Feature scaling(特征缩放)方法, 实质是<font color="#dd0000"><strong>一种线性变换</strong>: <strong>对向量 X 按照比例α压缩, 再进行平移</strong></font>。线性变换有很多良好的性质，这些性质<strong>决定了对数据改变后<u>不会造成“失效”，反而能提高数据的表现</u></strong>，这些性质是归一化/标准化的前提, 详情见<a href="https://www.zhihu.com/question/20455227/answer/370658612" target="_blank" rel="noopener">特征工程中的「归一化」有什么作用？- 知乎</a> 。</p>
<p>先说<strong><u>数学领域</u>的normalization <u>标准化</u></strong>: 泛指将一组数，<strong><u>除以它们的sum</u></strong>, <strong>得到各自占比且总和为1</strong>——常见的就是求<strong>概率值</strong>或<strong>概率分布</strong></p>
<p>再到了<strong><u>ML方面处理feature</u></strong>时，具体这四种常用的分别是：</p>
<ol>
<li><p><strong>归一化</strong> Rescaling (<strong>min-max normalization</strong>)  ，<img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-min%28x%29%7D%7Bmax%28x%29-min%28x%29%7D" alt="[归一化]"> </p>
</li>
<li><p><strong>均值归一化</strong> mean normalization  <img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-mean%28x%29%7D%7Bmax%28x%29-min%28x%29%7D+" alt="[均值归一化]"> </p>
<p>先说<strong>x - mean</strong>：这叫<strong>centralize中心化</strong>。因为 sum = mean * N, 而处理是每一项减去mean就是总共减去了mean * N，这样可以保证<strong>处理后的sum为0</strong>！</p>
</li>
</ol>
<p>   归一化性质总结：把数据变成(0,1)或者（1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1<strong>范围</strong>之内处理，更加便捷快速</p>
<p>   目的关键词：归一化的缩放，顾名思义，”归一”——<strong>“拍扁”压缩到<font color="#dd0000">区间</font>（仅由极值决定）</strong></p>
<ol start="3">
<li><p><strong>标准化 Standardization</strong>(<strong><u>Z-score</u> normalization</strong>)  <img src="https://www.zhihu.com/equation?tex=+x%5E%7B%27%7D+%3D+%5Cfrac%7Bx-mean%28x%29%7D%7B%5Csigma%7D" alt="[标准化]"> </p>
<p>标准化总结：<strong>能伸能缩</strong>，当数据较为集中时， α更小，于是数据在标准化后就会<strong>更加分散</strong>。如果数据本身分布很广，那么 α 较大，数据就会被<strong>更加集中</strong>，到更小的范围内。</p>
<p>目的关键词：标准化的缩放<strong>是更加“弹性”和“动态”的能伸能缩，和<font color="#dd0000">整体样本的分布</font>有很大的关系，每个点都在贡献缩放，通过方差（variance）体现出来</strong>。</p>
<p>补充说明：</p>
<ul>
<li><p>在Batch Normalization(BN)出现之前是很有必要的，因为这样<strong>拉到均值附近，学习的时候更加容易</strong>，毕竟<strong>激活函数是以均值为中心的，学习到这个位置才能将不同的类分开</strong>。</p>
</li>
<li><p>但是BN出现之后，这个操作就完全没必要了。因为每次卷积后都有BN操作，BN就是把数据拉到0均值1方差的分布，而且这个均值和方差是动态统计的，不是只在原始输入上统计，因此更加准确。</p>
</li>
</ul>
</li>
<li><p>中心化：x’ = x - μ</p>
<p>平均值为0，对标准差无要求</p>
</li>
<li><p>Scaling to unit length  <img src="https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+%5Cfrac%7Bx%7D%7B%7C%7Cx%7C%7C%7D" alt=""> </p>
</li>
</ol>
<p>什么时候用归一化？什么时候用标准化？</p>
<ul>
<li><p>归一化： 输出范围在0-1之间</p>
</li>
<li><p>标准化：输出范围是<strong>负无穷到正无穷</strong>，灵活</p>
<p> （1）如果对输出结果<strong><u>范围</u>有要求</strong>，用归一化。<br>           如果数据较为稳定，<strong>不存在极端的最大最小值</strong>，用归一化。<br> （2）如果数据存在<strong>异常值和较多噪音</strong>，用<strong>标准化</strong>，可以间接通过中心化避免异常值和极端值的影响。</p>
</li>
</ul>
<p><strong>一般来说，我个人建议<u><font color="#dd0000">优先使用标准化</font></u>。</strong>在对特征形式有要求时再尝试别的方法，如归一化或者更加复杂的方法。很多方法都可以将输出调整到0-1，如果我们对于数据的分布有假设的话，更加有效方法是使用相对应的概率密度函数来转换</p>
<h2 id="2-理清ML-flow各个部分的关系"><a href="#2-理清ML-flow各个部分的关系" class="headerlink" title="2. 理清ML flow各个部分的关系"></a>2. 理清ML flow各个部分的关系</h2><p>从上到下是 应用层→数据底层：</p>
<ol>
<li><p>最优模型(hypothesis)，作为ML flow的最红产物，本质是为<strong>实际业务</strong>服务的——最终用于输入实际数据，输出需要的结果</p>
</li>
<li><p>cost/error func是为<strong>寻找最优模型</strong>服务的，作为metric, 找出<strong>最优参数</strong>——是最优化算法去<strong>优化</strong>的对象，(输出是错误metric, 仅用于评估), 但实际为模型服务的是是取到极值的最优参数——正则化项也在其中(既然是为了调参)</p>
</li>
<li><p>学习/优化算法，是为<strong>cost func</strong>服务的——<strong>通过数学方法让metric快速取到极值</strong></p>
</li>
</ol>
<h2 id="3-训练的目的是输出最佳参数"><a href="#3-训练的目的是输出最佳参数" class="headerlink" title="3. 训练的目的是输出最佳参数"></a>3. 训练的目的是输出最佳参数</h2><p>用训练数据的训练过程，根本目的是<strong>输出最佳的参数(即带着该参数的hypothesis)，而非<u>最低的错误率——错误率只是判定的指标</u></strong>。所以相应的<strong>简化训练过程/提高效率的技巧</strong>，如PCA降维，只能用在训练集(但因为用了PCA，同一个数据降维的mapping, Ureduce 也需要对验证/测试集，使用！)</p>
<h2 id="4-无监督学习的所有输入数据都是unlabeled"><a href="#4-无监督学习的所有输入数据都是unlabeled" class="headerlink" title="4. 无监督学习的所有输入数据都是unlabeled?"></a>4. 无监督学习的所有输入数据<u>都是unlabeled</u>?</h2><p>无监督学习，只是说 模型<strong>上线后，用于实际业务</strong>时，输入数据是unlabeled——在模型<strong>训练/验证/测试</strong>过程中，<strong>也是要求</strong>最优参数的(如，异常检测的异常阈值)，当然<strong>可以使用labeled data</strong></p>
<h2 id="5-数据增强器对于不同数据集"><a href="#5-数据增强器对于不同数据集" class="headerlink" title="5. 数据增强器对于不同数据集"></a>5. 数据增强<u>器</u>对于<u>不同数据集</u></h2><p>已知给training set构造了ImageDataGenerator,  我们应该构造a <u><strong>separate</strong></u> generator for <strong>valid and test sets</strong></p>
<h3 id="Why用构造不同的generator"><a href="#Why用构造不同的generator" class="headerlink" title="Why用构造不同的generator?"></a>Why用构造不同的generator?</h3><p><strong>Why can’t we use the same generator as for the training data?</strong></p>
<p>LOk back at the generator we wrote for the training data.</p>
<ul>
<li>It normalizes each image <strong>per batch</strong>, meaning that it uses <strong>batch statistics</strong>.</li>
<li>We should not do this with the test and validation data, since in a real life scenario we <strong>don’t process incoming images <u>a batch</u> at a time</strong> (we process <strong><u>one image</u> at a time</strong>).</li>
<li>Knowing the average per batch of test data would effectively give our model an advantage.<ul>
<li>The model should <strong><font color="#dd0000">not have any information about the test data</font></strong>.</li>
</ul>
</li>
</ul>
<h3 id="但又该如何构造val-test-set的数据增强generator呢？"><a href="#但又该如何构造val-test-set的数据增强generator呢？" class="headerlink" title="但又该如何构造val/test set的数据增强generator呢？"></a>但又该如何构造val/test set的数据增强generator呢？</h3><p>What we need to do is <strong>normalize</strong> incoming test data using the <font color="#dd0000">statistics <strong>computed from the training set</strong></font>.</p>
<p>There is one technical note. </p>
<ul>
<li><p>Ideally, we would want to compute our <strong>sample mean and standard deviation</strong> using the <strong>entire training set</strong>. However, since this is <strong>extremely large,</strong> that would be very time consuming.</p>
</li>
<li><p>In the interest of time, we’ll <strong><font color="#dd0000">take a random sample of the dataset</font></strong> and do the calcualtion.</p>
</li>
</ul>
<h2 id="6-ml理论中常见的超平面概念"><a href="#6-ml理论中常见的超平面概念" class="headerlink" title="6. ml理论中常见的超平面概念"></a>6. ml理论中常见的超平面概念</h2><p>超平面一般化的广义叫法：</p>
<p>超平面是【分解平面】的一般化：</p>
<ul>
<li><p>在一维的平面中，它是点 </p>
</li>
<li><p>在二维的平面中，它是线 </p>
<ul>
<li><p><font color="#dd0000"><strong>为什么</strong></font>是平面？因为<strong>normal vector法向量和plane是一一对应的</strong>，等价！而实际正是normal vector在分割</p>
<p>A plane would be this magenta line into two-dimensional space, and it <strong>actually represents all the possible vectors that would be sitting on that line(之后都用plane).</strong> In other words, they would be parallel to the plane, such as this blue vector or this orange vector. </p>
<p>You can define a plane with a single vector. This magenta vector is <strong>perpendicular to the plane</strong>, and it’s called the <strong><u>normal vector</u> to that plane</strong>. So normal vector is perpendicular to <strong>any vectors that lie on the plane</strong>. </p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqf03pey5j30ui0ki0vw.jpg" style="zoom:33%;" />

<ul>
<li><p>关于分割：实际就是normal vector在发挥作用（而该vector对应它所垂直的plane！)</p>
<p>如何在数学上而不是几何视觉上，计算分割的结果we are able to see <strong>visually</strong> when the vector is <strong>on one side of the plane</strong> or the other, but how do you <strong>do this <u>mathematically</u></strong>?  把目标向量和normal vector，<strong>做dot product</strong>，正负号表明在同侧/异侧——实际是<strong>俩向量，夹角的cos</strong>在决定！！</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghqf6a99ekj31d60ig7c3.jpg" alt="2D plane，数学上是这样起到“分割”作用的" style="zoom:43%;" />
</li>
</ul>
</li>
</ul>
</li>
<li><p>在三维的平面中，它是面 </p>
</li>
<li><p>…</p>
</li>
<li><p>在更高的维度中，我们称之为超平面** <br>所以，<strong>广义上</strong>，1D直线、2D平面，都可叫超平面</p>
</li>
</ul>
<h2 id="7-区分点积-内积-、叉积-外积-、矩阵乘法"><a href="#7-区分点积-内积-、叉积-外积-、矩阵乘法" class="headerlink" title="7. 区分点积(内积)、叉积(外积)、矩阵乘法"></a>7. 区分点积(内积)、叉积(外积)、矩阵乘法</h2><h3 id="a-内积-点积-结果是数-标量"><a href="#a-内积-点积-结果是数-标量" class="headerlink" title="a. 内积(点积): 结果是数(标量)"></a>a. 内积(点积): 结果是数(标量)</h3><p>向量的点乘,也叫向量的内积、数量积，对两个向量执行点乘运算，就是对这两个向量对位①<strong>一一相乘</strong>之后 ②<strong>再求和</strong>的操作，点乘的结果是一个标量。</p>
<p>对于向量a和向量b，要求一维向量a和向量b的<strong>行列数相同</strong>，点积公式为：</p>
<p><img src="https://img-blog.csdn.net/20160902214456788" alt=""></p>
<p>可以看出，计算结果是个标量！</p>
<h4 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h4><p>in order to compute <strong>angles, lengths and distances</strong> of vectors.</p>
<h4 id="实现：numpy-dot"><a href="#实现：numpy-dot" class="headerlink" title="实现：numpy.dot"></a>实现：numpy.dot</h4><p>注：虽然数学公式很清楚，但该np.dot要求了2个输入向量的维度，同<strong>矩阵乘法</strong>一样：<strong>第一个v的行数 = 第二个v的列数</strong>。</p>
<p>如：2个等长的向量v1, v2相乘——写成np.dot(v1, v2<strong>.T</strong>)</p>
<h4 id="内积-点积的-generalisation"><a href="#内积-点积的-generalisation" class="headerlink" title="内积: 点积的 generalisation"></a>内积: 点积的 generalisation</h4><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>An <strong>inner product</strong> is the more general term of <strong>a function</strong> which can apply to <strong>a wide range of <u>different vector spaces</u></strong>. </p>
<p>The <strong>dot product</strong> is the name given to the <strong>inner product</strong> on a <u><strong>finite dimensional Euclidean space</strong></u>.</p>
<p>输出可以是</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvov62sskj31jk04o41p.jpg" alt="比点积输出更多样" style="zoom:30%;" />

<h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvoequwcvj30uw0lctn7.jpg" alt="点积公式的3个性质" style="zoom:50%;" />



<h4 id="公式-和点积不同"><a href="#公式-和点积不同" class="headerlink" title="公式: 和点积不同"></a>公式: 和点积不同</h4><p>设A为[[a, b], [c, d]]</p>
<p>&lt;x, y&gt; = <strong>X.T · A · y</strong> = a x1y1 + c x2y1 + b x1y2 + d x2y2</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvojkio4jj30rs0e2afw.jpg" alt="展开公式" style="zoom:50%;" />

<h5 id="Positive-definite"><a href="#Positive-definite" class="headerlink" title="Positive definite"></a>Positive definite</h5><p>直接检验 是否<strong>仅当</strong>x=0时，&lt;x,x&gt;=0</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvppvog8kj30yy02emxh.jpg" style="zoom:33%;" />

<h5 id="Symmetric-lt-x-y-gt-lt-y-x-gt-看对角"><a href="#Symmetric-lt-x-y-gt-lt-y-x-gt-看对角" class="headerlink" title="Symmetric &lt;x, y&gt; = &lt;y, x&gt; 看对角"></a>Symmetric &lt;x, y&gt; = &lt;y, x&gt; 看对角</h5><p>因为<strong>对角</strong>同号，才能保证x2y1和x1y2是同号的；异号则不行</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvpn215bxj30ke0hsdgv.jpg" alt="异号则飞symmetric" style="zoom:33%;" />

<h5 id="Bilinear"><a href="#Bilinear" class="headerlink" title="Bilinear"></a>Bilinear</h5><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvprjtqxnj30fa01gdfx.jpg" alt="image-20201020130252879" style="zoom:50%;" />

<h4 id="VS-dot-product-vector-space更复杂，会不一样"><a href="#VS-dot-product-vector-space更复杂，会不一样" class="headerlink" title="VS dot product: vector space更复杂，会不一样"></a>VS dot product: vector space更复杂，会不一样</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvoxw1lonj317a0hytja.jpg" alt="低维度(2D以下)和点积是一样的" style="zoom:30%;" />

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gjvp50nfnpj321o07en9e.jpg" alt="3D以上就可能输出为向量/矩阵了" style="zoom:33%;" />



<p>(vector spaces 发生了变化)</p>
<p>结论：在<strong>复制Tensor的计算</strong>里，务必区分！！</p>
<h4 id="几何意义"><a href="#几何意义" class="headerlink" title="几何意义"></a><a href="https://blog.csdn.net/dcrmg/article/details/52416832" target="_blank" rel="noopener">几何意义</a></h4><h3 id="b-叉积-外积-类似矩阵乘法"><a href="#b-叉积-外积-类似矩阵乘法" class="headerlink" title="b. 叉积(外积): 类似矩阵乘法"></a>b. 叉积(外积): 类似矩阵乘法</h3><p>两个向量的叉乘，又叫向量积、外积、叉积，叉乘的运算结果<strong>是一个向量</strong>而不是一个标量。并且两个向量的叉积<strong>与这两个向量组成的坐标平面垂直</strong>。</p>
<p>对于向量a和向量b，叉乘公式为：</p>
<img src="https://img-blog.csdn.net/20160902230539163" style="zoom:75%;" />

<img src="https://img-blog.csdn.net/20160902231520146" style="zoom:75%;" />



<h4 id="几何意义-1"><a href="#几何意义-1" class="headerlink" title="几何意义"></a><a href="https://blog.csdn.net/dcrmg/article/details/52416832" target="_blank" rel="noopener">几何意义</a></h4><h3 id="顺带区分：基本运算-×÷"><a href="#顺带区分：基本运算-×÷" class="headerlink" title="顺带区分：基本运算 +-×÷"></a>顺带区分：基本运算 +-×÷</h3><blockquote>
<p>The product operator * when used on arrays or matrices indicates <strong>element-wise</strong> multiplications.</p>
</blockquote>
<p><strong>四则运算符号</strong>对于<strong>np.array</strong>，都是单纯的<strong><u>element-wise</u>的对位操作</strong>，不像点积——不相加</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghh5wqiu4cj30m607241f.jpg" style="zoom:43%;" />

<h3 id="矩阵与矩阵-向量相乘-也可用"><a href="#矩阵与矩阵-向量相乘-也可用" class="headerlink" title="矩阵与矩阵/向量相乘 也可用@"></a><strong>矩阵与矩阵/向量</strong>相乘 <strong>也可用<u>@</u></strong></h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gj5p36wqcij31ji094gn0.jpg" alt="也可用@！"></p>
<p>也自然要求：<del>一维向量</del>array a和b的<strong><del>行列数</del><u>维度</u>相同</strong></p>
<h2 id="8-条件概率-表达约束-、联和概率的区分"><a href="#8-条件概率-表达约束-、联和概率的区分" class="headerlink" title="8. 条件概率(表达约束)、联和概率的区分"></a>8. 条件概率(表达约束)、联和概率的区分</h2><h3 id="联和概率-分母是全集-所以默认忽略"><a href="#联和概率-分母是全集-所以默认忽略" class="headerlink" title="联和概率: 分母是全集(所以默认忽略)"></a>联和概率: 分母是<u>全集</u>(所以默认忽略)</h3><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlntiq7ocj31dq0kak1t.jpg" style="zoom:30%;" />



<h3 id="条件概率-分母是做条件的那个概率的所在集"><a href="#条件概率-分母是做条件的那个概率的所在集" class="headerlink" title="条件概率: 分母是做条件的那个概率的所在集"></a>条件概率: 分母是<u>做条件的那个概率</u>的所在集</h3><p>条概y|x 总是给人一种<strong>“先有x后有y”</strong>的感觉，但其实x,y<strong>并不是先有鸡后有蛋的关系</strong>——x,y应该理解<strong><font color="red">为y受到x的约束</font>，会随着x的改变而y的取值也改变</strong>。条件所表达的意义<strong>在于约束，而非顺序</strong></p>
<h4 id="分子用了交集概率，分母是做条件的概率的所在集"><a href="#分子用了交集概率，分母是做条件的概率的所在集" class="headerlink" title="分子用了交集概率，分母是做条件的概率的所在集"></a>分子用了交集概率，分母是<u>做条件的概率</u>的所在集</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlnqr41w2j31f20jiaj3.jpg" style="zoom:30%;" />

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghlnppukfmj315w0imaha.jpg" style="zoom:30%;" />



<h2 id="9-np-array的转置-至少是2D"><a href="#9-np-array的转置-至少是2D" class="headerlink" title="9. np.array的转置: 至少是2D"></a>9. np.array的转置: 至少是2D</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gho372sm9hj31f70u079y.jpg" style="zoom:40%;" />



<h2 id="10-贝叶斯学习—ML的概率表示"><a href="#10-贝叶斯学习—ML的概率表示" class="headerlink" title="10. 贝叶斯学习—ML的概率表示"></a>10. 贝叶斯学习—ML的概率表示</h2><img src="/Users/kenny/Library/Application Support/typora-user-images/image-20200917171456223.png" style="zoom:43%;" />





<p>一般来说一个<u>巨大（通常为无限）的假设空间</u>，可已有<u>无数个假设</u>。</p>
<p>贝叶斯学习的本质是：左边直观解释是，在<strong>已有的(训练)数据</strong>条件下，得到各个h的概率；公式右边分子是，<strong><u>符合(fit)这些数据D分布</u>的，假设h的概率</strong>——学习的目标则是，确定左边那个 P(h|D) 的 argmax 函数，得到最优的h</p>
<p>为了简化，<strong>去掉分母 P(D) 的项，因为它不依赖于假设而是已有的数据。这个方法被称为最大后验（MAP）。</strong></p>
<p>现在我们应用下面的数学技巧：</p>
<ul>
<li>最大化原函数和最大化取对数的原函数的过程是相似的，即取对数不影响求最大值问题。</li>
<li>乘积的对数等于对数的和。</li>
<li>正量的最大化等同于负量的最小化。</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitrm3yzidj30kr07274b.jpg" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitrkb5ka4j31ew0agtbl.jpg" alt="总结" style="zoom:43%;" />

<p>底数为 2 的负对数项看起来是不是很熟悉？这都来自「信息论」</p>
<h2 id="11-图像化表达：ML算法3种-VS-贝叶斯深度学习-神经网络"><a href="#11-图像化表达：ML算法3种-VS-贝叶斯深度学习-神经网络" class="headerlink" title="11. 图像化表达：ML算法3种 VS 贝叶斯深度学习(神经网络)"></a>11. 图像化表达：ML算法3种 VS 贝叶斯深度学习(神经网络)</h2><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifuko31hlj31mf0u0b29.jpg" alt="" style="zoom:67%;" />



<h4 id="Bayesian-神经网络：让具体的标量参数，变为概率分布"><a href="#Bayesian-神经网络：让具体的标量参数，变为概率分布" class="headerlink" title="Bayesian 神经网络：让具体的标量参数，变为概率分布"></a>Bayesian 神经网络：让具体的标量参数，变为概率分布</h4><p>贝叶斯深度学习的核心思想：将<strong>神经网络的<u>权重w和b视作服从某分布的随机变量</u></strong>，而不是固定值。以及网络的前向传播，就是<strong><u>从权值分布中抽样</u></strong>然后计算。</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifus5k4vuj30no15owp0.jpg" alt="Going Bayesian" style="zoom:30%;" />

<p>进而，导致得到的<strong><u>model也不是唯一的</u></strong>，而是处于一个<strong><u>范围的所有</u></strong></p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gifuqihxwsj30sa0fawhh.jpg" style="zoom:40%;" />

<h4 id="贝叶斯学习的意义"><a href="#贝叶斯学习的意义" class="headerlink" title="贝叶斯学习的意义"></a>贝叶斯学习的意义</h4><p><strong><u>提供不确定性</u>，非 softmax 生成的概率</strong>。虽然工业界<strong>一般需要确定性结果</strong>，所以有这种贝叶斯机器学习在工业界没有广泛应用的错觉，但是贝叶斯理论<strong>对于事件具有<u>很强的指导意义</u></strong></p>
<h2 id="12-统计学-vs-统计-机器-学习"><a href="#12-统计学-vs-统计-机器-学习" class="headerlink" title="12. 统计学 vs 统计(机器)学习"></a>12. 统计学 vs 统计(机器)学习</h2><p>link: <a href="https://zhuanlan.zhihu.com/p/61964658" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61964658</a></p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitsm0lryej31d00u0ans.jpg" alt="区分" style="zoom:67%;" />

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitsff759nj30s40qw77x.jpg" alt="总结" style="zoom:40%;" />



<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gitsfzesikj30yc0c842g.jpg" alt="总结" style="zoom:40%;" />



<h2 id="13-急切学习-Eager-和惰性学习-Lazy"><a href="#13-急切学习-Eager-和惰性学习-Lazy" class="headerlink" title="13. 急切学习(Eager)和惰性学习(Lazy)"></a>13. 急切学习(Eager)和惰性学习(Lazy)</h2><h3 id="以下内容均为转载"><a href="#以下内容均为转载" class="headerlink" title="(以下内容均为转载)"></a><em>(以下内容均为转载)</em></h3><p>急切学习方法也叫积极学习方法, 惰性学习方法也叫消极学习方法</p>
<h3 id="急切-积极学习方法"><a href="#急切-积极学习方法" class="headerlink" title="急切/积极学习方法"></a>急切/积极学习方法</h3><p>指在利用算法<strong>进行判断<u>之前</u></strong>,先利用训练集数据通过<strong>训练得到一个目标函数</strong>；在需要进行判断时，利用已经训练好的函数进行决策。</p>
<p>意义：这种方法是<strong>在开始的时候需要进行一些工作,到后期进行使用的时候会很方便.</strong></p>
<p><strong>例如</strong></p>
<p>以很好理解的决策树为例,通过决策树进行判断之前,先通过对<strong>训练集的训练建立起了一棵树</strong>。</p>
<p>比如很经典的利用决策树判断一各新发现的物种是否为哺乳动物的例子,首先根据已经已知的训练集进行训练,提炼出哺乳动物的各种特征,这就是一个建立模型的过程；<strong>模型建立好之后,再有新物种需要进行判断的时候</strong>,只需要按照训练好的模型对照新物种挨个对比,就能很容易的给出判断结果.</p>
<h3 id="惰性-消极学习方法"><a href="#惰性-消极学习方法" class="headerlink" title="惰性/消极学习方法"></a>惰性/消极学习方法</h3><p><strong>最开始的时候不会根据已有的样本(通过”训练”过程)创建目标函数</strong>,只是<strong>简单的把训练用的样本<u>储存</u>好</strong>；后期<strong>需要对新进入的样本进行判断的时候</strong>， 才开始<strong>分析新进入样本与已存在的训练样本之间的关系</strong>，并据此确定新入样本的目标函数值。</p>
<p><strong>例如</strong></p>
<p>典型算法<strong>KNN</strong>：KNN<strong>不会根据训练集主动学习或者拟合出一个函数</strong>，来对新进入的样本进行判断,而是单纯的记住训练集中所有的样本,并<strong>没有像上边决策树那样先对训练集数据进行训练，<u>得出一套规则</u></strong>。所以它实际上<strong>没有所谓的”训练”过程</strong>,而是在<strong>需要进行预测的时候从自己的训练集样本中查找</strong>与新进入样本最相似的样本，来获得预测结果</p>
<h3 id="对比：双方互补"><a href="#对比：双方互补" class="headerlink" title="对比：双方互补"></a>对比：双方互补</h3><p>积极学习方法：在训练时<strong>考虑到了训练集中所有数据,训练时间比较长</strong>；有新样本进入需要判断的时候<strong>决策时间短</strong>。但是也是由于这个原因,<strong><u>做增量拓展</u>的时候比较虐</strong>.</p>
<p>消极学习方法：几乎没有训练时间,在新样本进入做判断的时候<strong>计算开销大，时间长</strong>；但是，<strong><u>天生支持增量学习</u></strong>。</p>
<p>那什么是增量学习？</p>
<h2 id="14-增量学习-Incremental-Learning"><a href="#14-增量学习-Incremental-Learning" class="headerlink" title="14 增量学习(Incremental Learning)"></a>14 增量学习(Incremental Learning)</h2><p><strong>简单定义</strong>：作为机器学习的一种方法，现阶段得到广泛的关注。在其中，<strong>输入数据不断被用于扩展现有模型的知识，即进一步训练模型</strong>。它代表了一种<strong>动态</strong>的学习的技术。</p>
<h3 id="地位"><a href="#地位" class="headerlink" title="地位"></a>地位</h3><p>是<strong><u>数据挖掘算法走向实用化</u></strong>的关键技术之一</p>
<h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>对于<strong>传统的批量学习</strong>技术来说,如何从<strong>日益增加的新数据</strong>中得到有用信息是一个难题。随着数据规模的不断增加,对时间和空间的需求也会迅速增加,最终会导致学习的速度赶不上数据更新的速度。机器学习是一个解决此问题的有效方法。</p>
<p> 随着人工智能和机器学习的发展,人们开发了很多机器学习算法。这些算法<strong>大部分都是批量学习(Batch Learning)模式</strong>,即<strong><u>需要在训练之前所有训练样本一次性可得</u></strong>,<font color="#dd0000">学习完这些样本<strong><u>之后</u></strong>——<strong>学习过程就终止了,不再学习新的知识</strong></font>；然而在实际应用中,训练样本通常<strong><u>不可能一次全部得到,而是随着时间逐步得到</u></strong>,并且样本<strong>反映的信息也可能随着时间产生了变化</strong>。如果新样本到达后<u><strong>要重新学习</strong></u>全部数据,需要消耗大量时间和空间,因此批量的学习算法不能满足这种需求。</p>
<p>为了实现<strong>在线学习</strong>的需求,需要抛弃以前的学习结果,重新训练和学习,这对时间和空间的需求都很高。因此,迫切需要研究增量学习方法,可以<strong>渐进的进行知识更新</strong>,且能<strong>修正和加强以前的知识</strong>,使得更新后的知识能<strong>适应</strong>新增加的数据。</p>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p>增量学习是指一个学习系统能<strong><u>不断地从新样本中学习新的知识,并能保存大部分以前已经学习到的知识</u></strong>。增量学习非常<font color="#dd0000"><strong>类似于人类自身的学习模式</strong></font>。因为人在成长过程中,每天学习和接收新的事物,学习是逐步进行的,而且,对已经学习到的知识,<strong>人类一般是不会遗忘的</strong>。</p>
<p>与传统的数据分类技术相比，增量学习分类技术具有显著的优越性，这主要表现在两个方面：一方面由于其<strong>无需保存历史数据</strong>，从而减少存储空间的占用；另一方面，由于其在新的训练中<strong>充分<u>利用了历史的训练结果</u>，从而显著地减少了后续训练的时间</strong>。</p>
<p>增量学习技术是一种得到广泛应用的<strong><u>智能化数据挖掘与知识</u></strong>发现技术。其思想是当样本逐步积累时，学习精度也要随之提高。</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>对于满足以下条件的学习方法可以定义为增量学习方法：</p>
<ul>
<li>可以学习<strong>新的</strong>信息中的有用信息</li>
<li><strong>不需要访问</strong>已经用于训练分类器的<strong>原始数据</strong></li>
<li>但是，对已经学习的知识具有<strong>记忆</strong></li>
<li>在面对新数据中包含的新类别时，可以有效地进行处理</li>
</ul>
<h2 id="15-No-free-lunch理论"><a href="#15-No-free-lunch理论" class="headerlink" title="15 No free lunch理论"></a>15 No free lunch理论</h2><h2 id="16-解析-闭式解-vs-数值解"><a href="#16-解析-闭式解-vs-数值解" class="headerlink" title="16 解析\闭式解 vs 数值解"></a>16 解析\闭式解 vs 数值解</h2><h3 id="解析解-Analytical-solution-、-闭式解-Closed-form-solution"><a href="#解析解-Analytical-solution-、-闭式解-Closed-form-solution" class="headerlink" title="解析解(Analytical solution)、 闭式解(Closed-form solution)"></a>解析解(Analytical solution)、 闭式解(Closed-form solution)</h3><p>就是根据严格的<strong>公式推导</strong>，给出任意的自变量就可以求出其因变量，也就是<strong>求解公式/函数，它适用于所有这类方程的求解</strong>，然后可以利用这些公式计算相应的问题。</p>
<p>所谓的解析解是一种包含分式、三角函数、指数、对数甚至无限级数等<strong>基本<u>函数</u>的解的形式</strong>。用来求得解析解的方法称为解析法(Analytical techniques)，解析法即是常见的微积分技巧，例如分离变量法等。</p>
<p>因为解析解是一个<strong>封闭形式(Closed-form) 的<u>函数</u></strong>，因此<strong>对任一自变量，皆可将其<u>带入</u>解析函数</strong>求得正确的因变量。因此，解析解也被称为封闭解。</p>
<h3 id="数值解-Numerical-solution"><a href="#数值解-Numerical-solution" class="headerlink" title="数值解(Numerical solution)"></a>数值解(Numerical solution)</h3><p>是采用某种<strong>计算方法</strong>，如<strong>有限元法， 数值逼近法，插值法</strong>等得到的解。</p>
<p>别人<strong>只能利用数值计算的<u>结果</u></strong>，而<strong>因为没有求出函数解析式f(x)，不能<u>给出自变量</u>并求出其输出值</strong>。当无法藉由微积分技巧<strong>求得解析解</strong>时，这时便只能利用数值分析的方式来求得其数值解了。</p>
<p>在数值分析的过程中，首先会将原方程<strong>加以简化</strong>，以利于后来的数值分析。例如，会先将微分符号改为差分（微分的离散形式）符号等，然后再用传统的代数方法将原方程改写成另一种方便求解的形式。这时的求解步骤就是将一自变量带入，求得因变量的近似解，因此利用此方法所求得的因变量为一个个离散的数值，不像解析解为一连续的分布，而且因为经过上述简化的操作，其正确性也不如解析法可靠。</p>
<p>简而言之，解析解就是<strong>给出解的函数形式</strong>，从解的<strong>表达式中</strong>就可以算出任何对应值；数值解就是用<strong>数值方法求出的解</strong>，给出自变量, 求出某个特定方程的具体的解。在计算机应用中，这些<strong>特殊函数因为<u>大多有现成的数值法实现</u></strong>，它们通常被看作<strong>常见运算或常见函数</strong>；而实际上，在计算机的<strong><u>计算过程中</u></strong>，多数都是用<a href="https://baike.baidu.com/item/数值法" target="_blank" rel="noopener">数值法</a>计算的，</p>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Kenny_Ng.github.io/about" rel="external nofollow noreferrer">Kenny Ng</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://yoursite.com/Kenny_Ng.github.io/2020/07/25/ML-concepts-memorize/">http://yoursite.com/Kenny_Ng.github.io/2020/07/25/ML-concepts-memorize/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="/Kenny_Ng.github.io/about" target="_blank">Kenny Ng</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Kenny_Ng.github.io/tags/ML/">
                                    <span class="chip bg-color">ML</span>
                                </a>
                            
                                <a href="/Kenny_Ng.github.io/tags/math/">
                                    <span class="chip bg-color">math</span>
                                </a>
                            
                                <a href="/Kenny_Ng.github.io/tags/%E7%A7%AF%E7%B4%AF/">
                                    <span class="chip bg-color">积累</span>
                                </a>
                            
                                <a href="/Kenny_Ng.github.io/tags/%E7%BB%9F%E8%AE%A1%E6%A6%82%E7%8E%87/">
                                    <span class="chip bg-color">统计概率</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Kenny_Ng.github.io/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Kenny_Ng.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/Kenny_Ng.github.io/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/Kenny_Ng.github.io/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/Kenny_Ng.github.io/2020/08/08/NLP-DeepLearning-ai/">
                    <div class="card-image">
                        
                        <img src="https://miro.medium.com/max/2344/1*uc2HNS1m4CjG8Yb4AxGqbQ.png" class="responsive-img" alt="DeepLearning.ai出品NLP的Course1-NLP的Classification &amp; Vector Spaces">
                        
                        <span class="card-title">DeepLearning.ai出品NLP的Course1-NLP的Classification &amp; Vector Spaces</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Course1: NLP with Classification and Vector Spaces1. Task: text classification法1: Logsitc回归模型法2: 纯靠词频的Naive Bayes模型
Naiv
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Kenny Ng
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Kenny_Ng.github.io/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                    <a href="/Kenny_Ng.github.io/tags/math/">
                        <span class="chip bg-color">math</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Kenny_Ng.github.io/2020/07/25/AI4m-DeepLearning-ai/">
                    <div class="card-image">
                        
                        <img src="https://miro.medium.com/max/2344/1*uc2HNS1m4CjG8Yb4AxGqbQ.png" class="responsive-img" alt="DeepLearning.ai5月新课-AI4Medicince笔记">
                        
                        <span class="card-title">DeepLearning.ai5月新课-AI4Medicince笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            1. Course1-AI for Medical Diagnosis(诊断)I. ML进阶，可能遇到的更实际的进阶问题：3个

a. 训练数据分布-Class imbalance问题如 实际在收集要拿來做DL的医学数据時，常识是 正常的非
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-07-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Kenny Ng
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Kenny_Ng.github.io/tags/ML/">
                        <span class="chip bg-color">ML</span>
                    </a>
                    
                    <a href="/Kenny_Ng.github.io/tags/CV/">
                        <span class="chip bg-color">CV</span>
                    </a>
                    
                    <a href="/Kenny_Ng.github.io/tags/math/">
                        <span class="chip bg-color">math</span>
                    </a>
                    
                    <a href="/Kenny_Ng.github.io/tags/advancedML/">
                        <span class="chip bg-color">advancedML</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Kenny_Ng.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/Kenny_Ng.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Kenny_Ng.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Kenny_Ng.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Kenny_Ng.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Kenny_Ng.github.io/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="5200611191"
                   fixed='true'
                   autoplay='true'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.2'
                   list-folded='false'
        >
        </meting-js>
    </div>
</div>

<script src="/Kenny_Ng.github.io/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="/Kenny_Ng.github.io/about" target="_blank">Kenny Ng</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/KennyNg-19" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:wuyuhao2019@126.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=836316155" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 836316155" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/Kenny_Ng.github.io/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/Kenny_Ng.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/Kenny_Ng.github.io/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Kenny_Ng.github.io/libs/materialize/materialize.min.js"></script>
    <script src="/Kenny_Ng.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Kenny_Ng.github.io/libs/aos/aos.js"></script>
    <script src="/Kenny_Ng.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Kenny_Ng.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Kenny_Ng.github.io/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Kenny_Ng.github.io/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Kenny_Ng.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    

    
    <script src="/Kenny_Ng.github.io/libs/instantpage/instantpage.js" type="module"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</body>

</html>
